{"id": 1, "github_id": 1, "output": ["python citation.py --gpu 0 --dataset Citeseer --num-stacks 5 --lr 0"]}
{"id": 2, "github_id": 1, "output": ["import dgl\nimport torch\nimport torch.nn.functional as F\nfrom dgl.data import CiteseerGraphDataset\nfrom dgl.nn import GraphConv\n\nclass GCN(torch.nn.Module):\n    def __init__(self, in_feats, hidden_size, num_classes):\n        super(GCN, self).__init__()\n        self.conv1 = GraphConv(in_feats, hidden_size)\n        self.conv2 = GraphConv(hidden_size, num_classes)\n\n    def forward(self, g, inputs):\n        h = self.conv1(g, inputs)\n        h = F.elu(h)\n        h = self.conv2(g, h)\n        return h\n# Load graph data\n\ndataset = CiteseerGraphDataset()\ng = dataset[0]\n\n# Initialize model\n\nmodel = GCN(g.ndata['feat'].shape[1], 16, dataset.num_classes)\n\n# Training loop\n\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n\nfor epoch in range(50):\n    outputs = model(g, g.ndata['feat'])\n    loss = F.cross_entropy(outputs[g.train_mask], g.ndata['label'][g.train_mask])\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n    print('Epoch:', epoch, 'Loss:', loss.item())\n\n# Evaluation\n\nmodel.eval()\noutputs = model(g, g.ndata['feat'])\n_, predicted = outputs.max(dim=1)\ncorrect = (predicted == g.ndata['label']).sum().item()\nacc = correct / len(g.ndata['label'])\nprint('Accuracy:', acc)"]}
{"id": 3, "github_id": 1, "output": ["python train.py --gpu 1 --model gaan --dataset LA"]}
{"id": 7, "github_id": 1, "output": ["python3 main.py --batch-size 256 --log-interval 8"]}
{"id": 8, "github_id": 1, "output": ["import torch\nimport torch.nn.functional as F\nfrom ogb.nodeproppred import PygNodePropPredDataset, Evaluator\nfrom torch_geometric.nn import CorrectAndSmooth\nfrom torch_geometric.data import DataLoader\n\n# Load the data\ndataset = PygNodePropPredDataset(name='ogbn-arxiv')\ndata = dataset[0]\n\n# Define the model\nclass MLP(torch.nn.Module):\n    def __init__(self, hidden_channels, num_layers):\n        super(MLP, self).__init__()\n        self.hidden_channels = hidden_channels\n        self.num_layers = num_layers\n        self.lin1 = torch.nn.Linear(dataset.num_features, hidden_channels)\n        self.dropout = torch.nn.Dropout(0.7)\n        self.lins = torch.nn.ModuleList()\n        self.lins.append(self.lin1)\n        for _ in range(num_layers - 1):\n            self.lins.append(torch.nn.Linear(hidden_channels, hidden_channels))\n        self.lin2 = torch.nn.Linear(hidden_channels, dataset.num_classes)\n\n    def reset_parameters(self):\n        for lin in self.lins:\n            lin.reset_parameters()\n        self.lin2.reset_parameters()\n\n    def forward(self, x):\n        for lin in self.lins:\n            x = F.relu(lin(x))\n            x = self.dropout(x)\n        out = self.lin2(x)\n        return out.log_softmax(dim=-1)\n\nmodel = MLP(hidden_channels=64, num_layers=1)\nmodel.reset_parameters()\n\n# Train the model\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel, data = model.to(device), data.to(device)\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)\nevaluator = Evaluator(name='ogbn-arxiv')\n\nfor epoch in range(1, 701):\n    model.train()\n    optimizer.zero_grad()\n    out = model(data.x)\n    loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n    loss.backward()\n    optimizer.step()\n\n    model.eval()\n    with torch.no_grad():\n        pred = model(data.x).argmax(dim=-1, keepdim=True)\n\n    correct = pred.eq(data.y)\n    correct[data.train_mask | data.val_mask | data.test_mask] = True\n\n    correct_and_smooth = CorrectAndSmooth(num_correction_layers=1, correction_alpha=1.0, num_smoothing_layers=1, smoothing_alpha=0.8)\n    y_soft = correct_and_smooth.correct(data.y, pred)\n    pred = correct_and_smooth.smooth(y_soft)\n"]}
{"id": 11, "github_id": 1, "output": ["import torch\nimport requests\n\n# Make sure the dependencies are installed\n!pip install torch requests\n\n# Download and prepare the dataset\nDATASET_URL = 'https://github.com/kimiyoung/planetoid/raw/master/data'\ndata = []\nfor dataset in ['citeseer']:\n    tmp_data = Dataset(root='/tmp/', name=dataset, url=DATASET_URL)\n    data.append(tmp_data)\ntrain_dataset = data[0]\n\ndef _init_model(in_feats, hid_feats, out_feats):\n    model = DGI(in_feats, hid_feats, out_feats)\n    return model\n\n# Create the DGI model\nmodel = _init_model(train_dataset.num_features, 16, train_dataset.num_classes)\n\n# Define the device to train on\ndevice = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\nmodel = model.to(device)\n\n# Define the optimizer\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n\n# Train the model\nfor epoch in range(50):\n    loss = model(train_dataset)\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n    print('Epoch {:05d} | Loss {:.4f}'.format(epoch, loss))\n"]}
{"id": 18, "github_id": 1, "output": ["import torch.nn as nn\nimport dgl.nn as dglnn\nimport torch.nn.functional as F\n\nclass SAGE(nn.Module):\n    def __init__(self, in_feats, n_hidden, n_classes):\n        super().__init__()\n        self.layers = nn.ModuleList()\n        self.layers.append(dglnn.SAGEConv(in_feats, n_hidden, \"mean\"))\n        self.layers.append(dglnn.SAGEConv(n_hidden, n_hidden, \"mean\"))\n        self.layers.append(dglnn.SAGEConv(n_hidden, n_classes, \"mean\"))\n        self.dropout = nn.Dropout(0.5)\n\n    def forward(self, sg, x):\n        h = x\n        for l, layer in enumerate(self.layers):\n            h = layer(sg, h)\n            if l != len(self.layers) - 1:\n                h = F.relu(h)\n                h = self.dropout(h)\n        return h"]}
{"id": 21, "github_id": 1, "output": ["python3 train.py --dataset citeseer --gpu 1"]}
{"id": 23, "github_id": 1, "output": ["import utils\n\nargs = utils.init_args()\n\nvalid_sku_raw_ids = utils.get_valid_sku_set(args.item_info_data)\n\ng, sku_encoder, sku_decoder = utils.construct_graph(\n    args.action_data, args.session_interval_sec, valid_sku_raw_ids\n)\n\ntrain_g, test_g = utils.split_train_test_graph(g)\n\nsku_info_encoder, sku_info_decoder, sku_info = utils.encode_sku_fields(\n    args.item_info_data, sku_encoder, sku_decoder\n)\n\nnum_skus = len(sku_encoder)\nnum_brands = len(sku_info_encoder[\"brand\"])\nnum_shops = len(sku_info_encoder[\"shop\"])\nnum_cates = len(sku_info_encoder[\"cate\"])\n\nprint(\n    \"Num skus: {}, num brands: {}, num shops: {}, num cates: {}.format(\n        num_skus, num_brands, num_shops, num_cates\n    )\n)\n\nmodel = train(\n    args, train_g, sku_info, num_skus, num_brands, num_shops, num_cates\n)"]}
{"id": 24, "github_id": 1, "output": ["python main.py --score_func conve --opn ccorr --gpu 1 --data wn18rr --epochs 700"]}
{"id": 26, "github_id": 1, "output": ["python main.py --dataset wiki_cs --graph_encoder_layer 512 256 --drop_edge_p 0.2 0.3 --feat_mask_p 0.2 0.1 --lr 5e-4 --data_seed 94050 --gpu 6"]}
{"id": 30, "github_id": 1, "output": ["python main.py --gpu 7 --dataset amazon --lr 0.01"]}
{"id": 10, "github_id": 1, "output": ["import argparse\n\ndef arg_parse():\n    parser = argparse.ArgumentParser(description=\"DiffPool arguments\")\n    parser.add_argument(\"--dataset\", dest=\"dataset\", help=\"Input Dataset\")\n    parser.add_argument(\"--pool_ratio\", dest=\"pool_ratio\", type=float, help=\"pooling ratio\")\n    parser.add_argument(\"--num_pool\", dest=\"num_pool\", type=int, help=\"num_pooling layer\")\n    parser.add_argument(\"--no_link_pred\", dest=\"linkpred\", action=\"store_false\", help=\"switch of link prediction object\")\n    parser.add_argument(\"--cuda\", dest=\"cuda\", type=int, help=\"switch cuda\")\n    parser.add_argument(\"--lr\", dest=\"lr\", type=float, help=\"learning rate\")\n    parser.add_argument(\"--clip\", dest=\"clip\", type=float, help=\"gradient clipping\")\n    parser.add_argument(\"--batch-size\", dest=\"batch_size\", type=int, help=\"batch size\")\n    parser.add_argument(\"--epochs\", dest=\"epoch\", type=int, help=\"num-of-epoch\")\n    parser.add_argument(\"--train-ratio\", dest=\"train_ratio\", type=float, help=\"ratio of trainning dataset split\")\n    parser.add_argument(\"--test-ratio\", dest=\"test_ratio\", type=float, help=\"ratio of testing dataset split\")\n    parser.add_argument(\"--num_workers\", dest=\"n_worker\", type=int, help=\"number of workers when dataloading\")\n    parser.add_argument(\"--gc-per-block\", dest=\"gc_per_block\", type=int, help=\"number of graph conv layer per block\")\n    parser.add_argument(\"--bn\", dest=\"bn\", action=\"store_const\", const=True, default=True, help=\"switch for bn\")\n    parser.add_argument(\"--dropout\", dest=\"dropout\", type=float, help=\"dropout rate\")\n    parser.add_argument(\"--bias\", dest=\"bias\", action=\"store_const\", const=True, default=True, help=\"switch for bias\")\n    parser.add_argument(\"--save_dir\", dest=\"save_dir\", help=\"model saving directory: SAVE_DICT/DATASET\")\n    parser.add_argument(\"--load_epoch\", dest=\"load_epoch\", type=int, help=\"load trained model params from SAVE_DICT/DATASET/model-LOAD_EPOCH\")\n    parser.add_argument(\"--data_mode\", dest=\"data_mode\", help=\"data preprocessing mode: default, id, degree, or one-hot vector of degree number\", choices=[\"default\", \"id\", \"deg\", \"deg_num\"])\n    parser.set_defaults(dataset=\"ENZYMES\", pool_ratio=0.15, num_pool=1, cuda=1, lr=1e-3, clip=2.0, batch_size=20, epoch=4000, train_ratio=0.7, test_ratio=0.1, n_worker=1, gc_per_block=3,dropout=0.0, method=\"diffpool\", bn=True, bias=True, save_dir=\"./model_param\", load_epoch=-1, data_mode=\"default\")\n    return parser.parse_args()\n\nargs = arg_parse()\nargs.dataset = 'DD'\nargs.pool_ratio = 0.2\nargs.num_pool = 1"]}
{"id": 14, "github_id": 1, "output": ["python main.py --dataset gowalla --regs [1e-5] --embed_size 64 --layer_size [64,64,64] --lr 0.0001 --save_flag 1 --batch_size 512 --epoch 400 --verbose 1 --mess_dropout [0.1,0.1,0.1] --gpu 0"]}
{"id": 17, "github_id": 1, "output": ["python src/main_sparse.py --input data/your_dataset --gpu 6"]}
{"id": 27, "github_id": 1, "output": ["import dgl\n\nDATASET_NAME = 'coauthor_cs'\nDATASET_PATH = dgl.data.utils.get_download_dir() + '/{}'.format(DATASET_NAME)\n\n# load dataset\nfrom dgl.data.utils import load_graphs\ng, _ = load_graphs(DATASET_PATH)\n\n# dataset parameters\ndataset_params = {\n    'drop_edge_p': [0, 0.5],\n    'feat_mask_p': [0.5, 0.8],\n    'data_seed': 94050,\n}\n\n# model parameters\nmodel_params = {\n    'graph_encoder_layer': [512, 256, 128],\n    'predictor_hidden_size': 512,\n}\n\n# training parameters\ntrain_params = {\n    'epochs': 10000,\n    'lr': 0.00001,\n    'weight_decay': 0.00001,\n    'mm': 0.99,\n    'lr_warmup_epochs': 1000,\n    'weights_dir': '../weights',\n}\n\n# train model\nimport torch\n\n# initialize model with parameters\nmodel = BGRLModel(**model_params)\n\n# move model to GPU if available\nif torch.cuda.is_available():\n    model.to('cuda')\n\n# train model\nmodel.train(g, dataset_params, train_params)"]}
{"id": 28, "github_id": 1, "output": ["import argparse\nimport dgl\nfrom dgl.nn import GATConv\n\n# Parse command line arguments\nparser = argparse.ArgumentParser()\nparser.add_argument(\n    \"--dataset\",\n    type=str,\n    default=\"pubmed\",\n    help=\"Dataset name ('cora', 'citeseer', 'pubmed').\",\n)\nparser.add_argument(\n    \"--dt\",\n    type=str,\n    default=\"float\",\n    help=\"data type(float, bfloat16)\",\n)\nargs = parser.parse_args()\n\n# Load the dataset\nif args.dataset not in ['cora', 'citeseer', 'pubmed']:\n    raise ValueError(\"Unknown dataset: {}\".format(args.dataset))\ndata = getattr(dgl.data, args.dataset)()\n\n# Define the GAT model\nclass GAT(nn.Module):\n    def __init__(self):\n        super(GAT, self).__init__()\n        self.gat1 = GATConv(...)\n        self.gat2 = GATConv(...)\n\n    def forward(self, x):\n        x = F.relu(self.gat1(x))\n        x = self.gat2(x)\n        return x\n\n# Train the model\nmodel = GAT()\nfor epoch in range(number_of_epochs):\n    model.train()\n    \n    # ... Rest of the training loop\n"]}
{"id": 6, "github_id": 1, "output": ["python3 main.py --batch-size 128 --test-batch-size 128 --epochs 50"]}
{"id": 16, "github_id": 1, "output": ["import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport dgl\nfrom dgl import DGLGraph\nfrom dgl.data import AmazonCoBuy\n\n\n# Step 1: Prepare dataset\n\ndataset = AmazonCoBuy('category')\n\n# Step 2: Define the GATNE model\n\nclass GATNE(nn.Module):\n    def __init__(self, num_nodes, embedding_size, num_channels):\n        super().__init__()\n        self.num_nodes = num_nodes\n        self.embedding_size = embedding_size\n        self.num_channels = num_channels\n        self.embeddings = nn.Parameter(torch.Tensor(num_nodes, embedding_size))\n        self.gcn_layers = nn.ModuleList([nn.Linear(embedding_size, embedding_size) for _ in range(num_channels)])\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        nn.init.xavier_uniform_(self.embeddings)\n        for layer in self.gcn_layers:\n            nn.init.xavier_uniform_(layer.weight)\n\n    def forward(self, graph, node_ids):\n        node_embeddings = self.embeddings[node_ids]\n        return node_embeddings + sum(F.normalize(layer(graph.adjacency_matrix().to_dense(), node_embeddings)) \n                                     for layer in self.gcn_layers)\n\nnum_channels = 2\nembedding_size = 256\n\n# Step 3: Create the model\n\nmodel = GATNE(dataset[0].number_of_nodes(), embedding_size, num_channels)\n\n# Step 4: Define training function\n\ndef train(graph, model, node_ids):\n    optimizer = torch.optim.Adam(model.parameters())\n\n    for epoch in range(100):\n        optimizer.zero_grad()\n        embeddings = model(graph, node_ids)\n        # Replace `target` with your target tensor\n        loss = ((embeddings - target) ** 2).mean()\n        loss.backward()\n        optimizer.step()\n\n        print(f'Epoch {epoch}, Loss {loss.item()}')\n\ntrain(dataset[0], model, torch.arange(dataset[0].number_of_nodes()))"]}
{"id": 5, "github_id": 1, "output": ["python train.py --dataset citeseer --runs 300 --k 12"]}
{"id": 9, "github_id": 1, "output": ["python main.py --dataset ogbn-products --model linear --dropout 0.7 --epochs 1000 --lr 0.1"]}
{"id": 19, "github_id": 1, "output": ["import torch\nimport torch.nn as nn\nimport dgl\n\n\nclass DeepGenerativeModelOfGraphs(nn.Module):\n    def __init__(self, data_info, hidden_size=32, num_layers=1, bias=True):\n        super(DeepGenerativeModelOfGraphs, self).__init__()\n        self.data_info = data_info\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        self.bias = bias\n        self.linear = nn.Linear(self.data_info['input_size'], self.hidden_size, bias=self.bias)\n        self.layers = nn.ModuleList()\n        for i in range(num_layers):\n            self.layers.append(nn.Linear(self.hidden_size, self.hidden_size, bias=self.bias))\n\n    def forward(self, x):\n        x = self.linear(x)\n        for layer in self.layers:\n            x = layer(x)\n        return x"]}
{"id": 15, "github_id": 1, "output": ["python main.py --dataset gowalla --gpu 3 --embed_size 32"]}
{"id": 25, "github_id": 1, "output": ["python main.py --score_func conve --opn ccorr --gpu 1 --data wn18rr --gcn_drop 0.5 --seed 0"]}
{"id": 29, "github_id": 1, "output": ["python main.py --dataset amazon --model Care-gnn --learning_rate 0.005 --weight_decay 0.001"]}
