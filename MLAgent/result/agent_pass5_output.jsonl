{"id": 5, "github_id": 1, "output": ["python train.py --dataset citeseer --runs 300 --k 12", "python main.py --dataset Citeseer --gpu 0 --runs 300 --k 12", "import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom dgl.data import CiteseerGraphDataset\nfrom dgl.nn import GATConv\nfrom tqdm import tqdm\n\n# define the model\n# considering the original hyperparameters were set for Cora, you may need to tune them for best results on Citeseer.\nclass DAGNN(nn.Module):\n    def __init__(self, in_dim, hidden_dim, out_dim, num_layers):\n        super(DAGNN, self).__init__()\n        self.layers = nn.ModuleList()\n        self.layers.append(GATConv(in_dim, hidden_dim, num_heads=8, activation=F.relu))\n        for i in range(num_layers - 1):\n            self.layers.append(GATConv(hidden_dim * 8, hidden_dim, num_heads=8, activation=F.relu))\n        self.classify = nn.Linear(hidden_dim * 8, out_dim)\n\n    def forward(self, g):\n        h = g.ndata['feat']\n        for i, layer in enumerate(self.layers):\n            h = layer(g, h)\n        g.ndata['h'] = h\n        return self.classify(h.mean(1))\n\n# load the data\ndata = CiteseerGraphDataset()\ng = data[0]\n\ng = g.to(torch.device(\"cuda\"))\n\n# Soumith Chintala, PyTorch project lead, recommends using Xavier initialization in GNNs.\nn_reads, _ = g.multi_pull(0, \"feat\", \"sum\")\npull_mean, pull_std = n_reads.mean(), n_reads.std()\ninit_std = 1/pull_std.item()\n\n#initialize the parameters\nmodel = DAGNN(500, 64, 3, 10)  # replace 500, 64, 3, 10 with the dimensionality of the input features, the dimensionality of the hidden layers, the number of classes, and the number of layers\ntorch.nn.init.normal_(model.classify.weight.data, 0, init_std)\nmodel.classify.bias.data.fill_(0)\nmodel = model.to(torch.device(\"cuda\"))\n\noptimizer = optim.Adam(model.parameters(), lr=5e-3, weight_decay=5e-4)\nfor epoch in tqdm(range(300)):\n    model.train()\n    logits = model(g)\n    logp = F.log_softmax(logits, 1)\n    # you need the labels here for the loss\n    # loss = F.nll_loss(logp, labels)\n    # Commented out to not execute training in case running the entire notebook.\n    # optimizer.zero_grad()\n    # loss.backward()\n    # optimizer.step()\n", "python main.py --dataset Citeseer --runs 300 --k 12 --gpu 0", "python main.py --dataset Citeseer --gpu 0 --runs 300 --k 12"]}
{"id": 9, "github_id": 1, "output": ["python main.py --dataset ogbn-products --model linear --dropout 0.7 --epochs 1000 --lr 0.1", "from main import main\nmain(dataset='ogbn-products', model='linear', dropout=0.7, epochs=1000)", "python main.py --dataset ogbn-products --model linear --dropout 0.7 --epochs 1000", "python main.py --dataset ogbn-products --model linear --dropout 0.7 --epochs 1000 --lr 0.1", "python main.py --dataset ogbn-products --model linear --dropout 0.7 --epochs 1000 --lr 0.1"]}
{"id": 19, "github_id": 1, "output": ["import torch\nimport torch.nn as nn\nimport dgl\n\n\nclass DeepGenerativeModelOfGraphs(nn.Module):\n    def __init__(self, data_info, hidden_size=32, num_layers=1, bias=True):\n        super(DeepGenerativeModelOfGraphs, self).__init__()\n        self.data_info = data_info\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        self.bias = bias\n        self.linear = nn.Linear(self.data_info['input_size'], self.hidden_size, bias=self.bias)\n        self.layers = nn.ModuleList()\n        for i in range(num_layers):\n            self.layers.append(nn.Linear(self.hidden_size, self.hidden_size, bias=self.bias))\n\n    def forward(self, x):\n        x = self.linear(x)\n        for layer in self.layers:\n            x = layer(x)\n        return x", "import argparse\nfrom utils import setup\n\ndef main(opts):\n    # define your main function here\n    pass\n\nparser = argparse.ArgumentParser(description='DGMG')\n# configure\nparser.add_argument('--seed', type=int, default=9284, help='random seed')\n# dataset\nparser.add_argument('--dataset', choices=['cycles'], default='cycles', help='dataset to use')\nparser.add_argument('--path-to-dataset',type=str,default='cycles.p',help='load the dataset if it exists, generate it and save to the path otherwise',)\n\n# log\nparser.add_argument('--log-dir',default='./results',help='folder to save info like experiment configuration or model evaluation results',)\n\n# optimization\nparser.add_argument('--batch-size',type=int,default=10,help='batch size to use for training',)\nparser.add_argument('--clip-grad', action='store_true',default=True,help='gradient clipping is required to prevent gradient explosion',)\nparser.add_argument('--clip-bound',type=float,default=0.25,help='constraint of gradient norm for gradient clipping',)\n\nargs = parser.parse_args()\nopts = setup(args)\nmain(opts)", "python3 main.py", "import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport dgl\nimport dgl.function as fn\nimport dgl.nn as dglnn\n\n# Define the graph\nN = 100  # number of nodes\nDAMP = 0.85  # damping factor\nK = 10  # number of iterations\ng = dgl.graph(([i for i in range(N-1) for _ in range(DAMP)], [i+1 for _ in range(N-1) for i in range(DAMP)]))\ng.ndata['pv'] = torch.ones(N) / N\ng.ndata['deg'] = g.out_degrees(g.nodes()).float()\n\n# Define the message and reduce function\ndef pagerank_message_func(edges):\n    return {'pv' : edges.src['pv'] / edges.src['deg']}\n\ndef pagerank_reduce_func(nodes):\n    msgs = torch.sum(nodes.mailbox['pv'], dim=1)\n    pv = (1-DAMP) / N + DAMP*msgs\n    return {'pv' : pv}\n\n# Register the message and reduce function\ng.register_message_func(pagerank_message_func)\ng.register_reduce_func(pagerank_reduce_func)\n\n# Pagerank loop\nfor k in range(K):\n    # forward\n    g.send(g.edges())\n    g.recv(g.nodes())\n\nprint(g.ndata['pv'])", "import argparse\nfrom utils import setup\n\ndef main(opts):\n    pass\n\nparser = argparse.ArgumentParser(description=\"DGMG\")\n\n# dataset\nparser.add_argument(\n\t\"--dataset\", choices=[\"cycles\"], default=\"cycles\", help=\"dataset to use\"\n)\nparser.add_argument(\n\t\"--path-to-dataset\",\n\ttype=str,\n\tdefault=\"cycles.p\",\n\thelp=\"load the dataset if it exists, \"\n\t\"generate it and save to the path otherwise\",\n)\n\n# optimization\nparser.add_argument(\n\t\"--batch-size\",\n\ttype=int,\n\tdefault=10,\n\thelp=\"batch size to use for training\",\n)\nparser.add_argument(\n\t\"--clip-grad\",\n\taction=\"store_true\",\n\tdefault=True,\n\thelp=\"gradient clipping is required to prevent gradient explosion\",\n)\nparser.add_argument(\n\t\"--clip-bound\",\n\ttype=float,\n\tdefault=0.25,\n\thelp=\"constraint of gradient norm for gradient clipping\",\n)\n\nargs = parser.parse_args()\nopts = setup(args)\n\nmain(opts)"]}
{"id": 15, "github_id": 1, "output": ["python main.py --dataset gowalla --gpu 3 --embed_size 32", "python main.py --dataset=gowalla --gpu=3 --embed_size=32", "python main.py --dataset gowalla --embed_size 32 --layer_size [32,32,32] --gpu 3", "python main.py --dataset gowalla --regs [1e-5] --embed_size 32 --layer_size [32,32,32] --lr 0.0001 --save_flag 1 --batch_size 1024 --epoch 400 --verbose 1 --mess_dropout [0.1,0.1,0.1] --gpu 3", "python main.py --dataset gowalla --regs [1e-5] --embed_size 32 --layer_size [32,32,32] --lr 0.0001 --save_flag 1 --batch_size 1024 --epoch 400 --verbose 1 --mess_dropout [0.1,0.1,0.1] --gpu 3"]}
{"id": 25, "github_id": 1, "output": ["python main.py --score_func conve --opn ccorr --gpu 1 --data wn18rr --gcn_drop 0.5 --seed 0", "python main.py --score_func conve --opn ccorr --gpu 1 --data wn18rr --seed 0 --gcn_drop 0.5", "python main.py --score_func conve --opn ccorr --gpu 1 --data wn18rr --seed 0 --gcn_drop 0.5", "python main.py --score_func conve --opn ccorr --gpu 1 --data wn18rr --seed 0 --gcn_drop 0.5", "python main.py --score_func conve --opn ccorr --gpu 0 --data wn18rr --gcn_drop 0.5"]}
{"id": 29, "github_id": 1, "output": ["python main.py --dataset amazon --model Care-gnn --learning_rate 0.005 --weight_decay 0.001", "python main_sampling.py --dataset amazon --gpu 0 --learning-rate 0.005 --weight-decay 0.001", "python main.py --dataset=amazon --model=care-gnn --learning_rate=0.005 --weight_decay=0.001", "python main.py --gpu 0 --learning_rate 0.005 --weight_decay 0.001 --dataset amazon", "import dgl\nfrom dgl.data import FraudDataset\nfrom dgl.dataloading import GraphDataLoader\nfrom torch.utils.data.sampler import SubsetRandomSampler\nfrom model import CAREGNN\nimport torch \nimport torch.nn as nn\nfrom torch.optim import Adam\n\n# load dataset\namazon_data = FraudDataset('amazon')\nbipartite_graphs, labels = amazon_data[0]\nhomo_graph = amazon_data.homo_graph\nnum_feats = amazon_data.num_features\nnum_classes = amazon_data.num_classes\n\n# split dataset into train and validation set\nnum_nodes = homo_graph.num_nodes()\nnum_train = int(num_nodes * 0.8)\ntrain_nids = torch.arange(num_train)\nvalid_nids = torch.arange(num_train, num_nodes)\n\n# create dataloader for batch training\ntrain_sampler = SubsetRandomSampler(train_nids)\nvalid_sampler = SubsetRandomSampler(valid_nids)\ntrain_dataloader = GraphDataLoader(bipartite_graphs, sampler=train_sampler, batch_size=1024, drop_last=False)\nvalid_dataloader = GraphDataLoader(bipartite_graphs, sampler=valid_sampler, batch_size=1024, drop_last=False)\n\n# create model\nmodel = CAREGNN(in_features=num_feats, hidden_features=64, out_features=num_classes, num_res_gnn_blocks=2, tau=1.0)\nloss_fcn = nn.CrossEntropyLoss()\noptimizer = Adam(model.parameters(), lr=0.005, weight_decay=0.001)\n\n# move to gpu if available\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel = model.to(device)\n\n# training loop\nnum_epochs = 50\nfor epoch in range(num_epochs):\n    model.train()\n    for bg, labels in train_dataloader:\n        labels = labels.to(device)\n        prediction = model(bg, bg.ndata['feat'].float())\n        loss = loss_fcn(prediction, labels)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n}"]}
{"id": 33, "github_id": 2, "output": ["python run_classifier.py --task_name=MRPC --do_train=true --do_eval=true --data_dir=$GLUE_DIR/MRPC --vocab_file=$BERT_BASE_DIR/vocab.txt --bert_config_file=$BERT_BASE_DIR/bert_config.json --init_checkpoint=$BERT_BASE_DIR/bert_model.ckpt --max_seq_length=128 --train_batch_size=16 --learning_rate=2e-5 --num_train_epochs=3.0 --output_dir=/tmp/mrpc_output/", "python run_classifier.py \\\n  --task_name=MRPC \\\n  --do_train=true \\\n  --do_eval=true \\\n  --data_dir=$GLUE_DIR/MRPC \\\n  --vocab_file=$BERT_BASE_DIR/vocab.txt \\\n  --bert_config_file=$BERT_BASE_DIR/bert_config.json \\\n  --init_checkpoint=$BERT_BASE_DIR/bert_model.ckpt \\\n  --max_seq_length=128 \\\n  --train_batch_size=16 \\\n  --learning_rate=2e-5 \\\n  --num_train_epochs=3.0 \\\n  --output_dir=/tmp/mrpc_output/", "python run_classifier.py \\\n  --task_name=MRPC \\\n  --do_train=true \\\n  --do_eval=true \\\n  --data_dir=${GLUE_DIR}/MRPC \\\n  --vocab_file=./model/cased_L-12_H-768_A-12/vocab.txt \\\n  --bert_config_file=./model/cased_L-12_H-768_A-12/bert_config.json \\\n  --init_checkpoint=./model/cased_L-12_H-768_A-12/bert_model.ckpt \\\n  --max_seq_length=128 \\\n  --train_batch_size=16 \\\n  --learning_rate=2e-5 \\\n  --num_train_epochs=3.0 \\\n  --output_dir=/tmp/mrpc_output/", "python run_classifier.py \\\n--task_name=MRPC \\\n--do_train=true \\\n--do_eval=true \\\n--data_dir=$GLUE_DIR/MRPC \\\n--vocab_file=$BERT_BASE_DIR/vocab.txt \\\n--bert_config_file=$BERT_BASE_DIR/bert_config.json \\\n--init_checkpoint=$BERT_BASE_DIR/bert_model.ckpt \\\n--max_seq_length=128 \\\n--train_batch_size=16 \\\n--learning_rate=2e-5 \\\n--num_train_epochs=3.0 \\\n--output_dir=/tmp/mrpc_output/", "python run_classifier.py \\ \n  --task_name=MRPC \\ \n  --do_train=true \\ \n  --do_eval=true \\ \n  --data_dir=$GLUE_DIR/MRPC \\ \n  --vocab_file=$BERT_BASE_DIR/vocab.txt \\ \n  --bert_config_file=$BERT_BASE_DIR/bert_config.json \\ \n  --init_checkpoint=$BERT_BASE_DIR/bert_model.ckpt \\ \n  --max_seq_length=128 \\ \n  --train_batch_size=16 \\ \n  --learning_rate=2e-5 \\ \n  --num_train_epochs=3.0 \\ \n  --output_dir=/tmp/mrpc_output/"]}
{"id": 37, "github_id": 2, "output": ["python run_pretraining.py \\\n  --input_file=/tmp/tf_examples.tfrecord \\\n  --output_dir=/tmp/pretraining_output \\\n  --do_train=True \\\n  --do_eval=True \\\n  --bert_config_file=$BERT_BASE_DIR/bert_config.json \\\n  --init_checkpoint=$BERT_BASE_DIR/bert_model.ckpt \\\n  --train_batch_size=32 \\\n  --max_seq_length=512 \\\n  --max_predictions_per_seq=30 \\\n  --num_train_steps=10000 \\\n  --num_warmup_steps=10000 \\\n  --learning_rate=0.0001", "python run_pretraining.py \\\n  --input_file=/tmp/tf_examples.tfrecord \\\n  --output_dir=/tmp/pretraining_output \\\n  --do_train=True \\\n  --bert_config_file=$BERT_BASE_DIR/bert_config.json \\\n  --init_checkpoint=$BERT_BASE_DIR/bert_model.ckpt \\\n  --max_seq_length=512 \\\n  --max_predictions_per_seq=30 \\\n  --num_train_steps=10000 \\\n  --num_warmup_steps=10000 \\\n  --learning_rate=0.0001 \\\n  --train_batch_size=32", "python run_pretraining.py \\\n--input_file=/tmp/tf_examples.tfrecord \\\n--output_dir=/tmp/pretraining_output \\\n--do_train=True \\\n--do_eval=True \\\n--bert_config_file=$BERT_LARGE_DIR/bert_config.json \\\n--init_checkpoint=$BERT_LARGE_DIR/bert_model.ckpt \\\n--train_batch_size=32 \\\n--max_seq_length=512 \\\n--max_predictions_per_seq=30 \\\n--num_train_steps=10000 \\\n--num_warmup_steps=10000 \\\n--learning_rate=0.0001", "python run_pretraining.py \\\n  --input_file=<your_input_file> \\\n  --output_dir=<your_output_dir> \\\n  --do_train=True \\\n  --do_eval=True \\\n  --bert_config_file=$BERT_BASE_DIR/bert_config.json \\\n  --init_checkpoint=$BERT_BASE_DIR/bert_model.ckpt \\\n  --train_batch_size=32 \\\n  --max_seq_length=512 \\\n  --max_predictions_per_seq=30 \\\n  --num_train_steps=10000 \\\n  --num_warmup_steps=10000 \\\n  --learning_rate=0.0001", "python run_pretraining.py \n--input_file=gs://your-input-data-gcs-bucket/input-data.tfrecord \n--output_dir=gs://your-trained-model-gcs-bucket \n--do_train=True \n--do_eval=True \n--bert_config_file=$BERT_BASE_DIR/bert_config.json \n--init_checkpoint=$BERT_BASE_DIR/bert_model.ckpt \n--train_batch_size=32 \n--max_seq_length=512 \n--max_predictions_per_seq=30 \n--num_train_steps=10000 \n--num_warmup_steps=10000 \n--learning_rate=0.0001"]}
{"id": 41, "github_id": 2, "output": ["python run_pretraining.py --input_file=/tmp/tf_examples.tfrecord --output_dir=/tmp/pretraining_output --do_train=True --do_eval=True --bert_config_file=$BERT_BASE_DIR/bert_config.json --init_checkpoint=$BERT_BASE_DIR/bert_model.ckpt --train_batch_size=32 --max_seq_length=512 --max_predictions_per_seq=20 --num_train_steps=300000 --num_warmup_steps=300000 --learning_rate=0.0001", "python run_pretraining.py \n  --input_file=<input-file-path> \n  --output_dir=<output-directory-path> \n  --do_train=True \n  --do_eval=True \n  --bert_config_file=./model/cased_L-12_H-768_A-12/bert_config.json \n  --init_checkpoint=./model/cased_L-12_H-768_A-12/bert_model.ckpt \n  --train_batch_size=32 \n  --max_seq_length=512 \n  --max_predictions_per_seq=20 \n  --num_train_steps=300000 \n  --num_warmup_steps=300000 \n  --learning_rate=0.0001", "python run_pretraining.py \\\n  --input_file=/tmp/tf_examples.tfrecord \\\n  --output_dir=/tmp/pretraining_output \\\n  --do_train=True \\\n  --do_eval=True \\\n  --bert_config_file=$BERT_BASE_DIR/bert_config.json \\\n  --init_checkpoint=$BERT_BASE_DIR/bert_model.ckpt \\\n  --train_batch_size=32 \\\n  --max_seq_length=512 \\\n  --max_predictions_per_seq=20 \\\n  --num_train_steps=300000 \\\n  --num_warmup_steps=300000 \\\n  --learning_rate=0.0001", "python run_pretraining.py   --input_file=/tmp/tf_examples.tfrecord   --output_dir=/tmp/pretraining_output   --do_train=True   --do_eval=True   --bert_config_file=./model/cased_L-12_H-768_A-12/bert_config.json   --init_checkpoint=./model/cased_L-12_H-768_A-12/bert_model.ckpt   --train_batch_size=32   --max_seq_length=512   --max_predictions_per_seq=20   --num_train_steps=300000   --num_warmup_steps=300000   --learning_rate=0.0001", "import os\nfrom bert import modeling, optimization, tokenization\nfrom bert.run_pretraining import input_fn_builder, model_fn_builder\n\n# Configuration\nPRETRAINED_MODEL_PATH = './model/cased_L-12_H-768_A-12'\nOUTPUT_DIR = '/tmp/pretraining_output'\nINPUT_FILE = '/path/to/dataset/*.tfrecord' # Update with the path of your dataset\nBERT_CONFIG_FILE = os.path.join(PRETRAINED_MODEL_PATH, 'bert_config.json')\nVOCAB_FILE = os.path.join(PRETRAINED_MODEL_PATH, 'vocab.txt')\nINIT_CHECKPOINT = os.path.join(PRETRAINED_MODEL_PATH, 'bert_model.ckpt')\n\n# Hyperparameters\nTRAIN_BATCH_SIZE = 32\nMAX_SEQ_LENGTH = 512\nMAX_PREDICTIONS_PER_SEQ = 20\nMAX_STEPS = 300000\nWARMUP_STEPS = 30000\nLEARNING_RATE = 0.0001\n\n# Model configurations\nbert_config = modeling.BertConfig.from_json_file(BERT_CONFIG_FILE)\ninput_files = INPUT_FILE\ntokenizer = tokenization.FullTokenizer(vocab_file=VOCAB_FILE, do_lower_case=False)\n\n# create estimator\ntpu_cluster_resolver = None\nis_per_host = tf.contrib.tpu.InputPipelineConfig.PER_HOST_V2\nrun_config = tf.contrib.tpu.RunConfig(\n    cluster=tpu_cluster_resolver,\n    model_dir=OUTPUT_DIR,\n    save_checkpoints_steps=1000,\n    tpu_config=tf.contrib.tpu.TPUConfig(\n        iterations_per_loop=1000,\n        num_shards=8,\n        per_host_input_for_training=is_per_host))\n\nmodel_fn = model_fn_builder(\n    bert_config=bert_config,\n    init_checkpoint=INIT_CHECKPOINT,\n    learning_rate=LEARNING_RATE,\n    num_train_steps=MAX_STEPS,\n    num_warmup_steps=WARMUP_STEPS,\n    use_tpu=False,\n    use_one_hot_embeddings=False)\n\nestimator = tf.contrib.tpu.TPUEstimator(\n    use_tpu=False,\n    model_fn=model_fn,\n    config=run_config,\n    train_batch_size=TRAIN_BATCH_SIZE)\n\n# input function\ntrain_input_fn = input_fn_builder(\n    input_files=input_files,\n    max_seq_length=MAX_SEQ_LENGTH,\n    max_predictions_per_seq=MAX_PREDICTIONS_PER_SEQ,\n    is_training=True)\n\n# train model\nestimator.train(input_fn=train_input_fn, max_steps=MAX_STEPS)"]}
{"id": 45, "github_id": 2, "output": ["python run_pretraining.py \\\n  --input_file=./pretraining_data.tfrecord \\\n  --output_dir=./pretraining_output \\\n  --do_train=True \\\n  --do_eval=True \\\n  --bert_config_file=./model/wwm_uncased_L-24_H-1024_A-16/bert_config.json \\\n  --init_checkpoint=./model/wwm_uncased_L-24_H-1024_A-16/bert_model.ckpt \\\n  --max_seq_length=512 \\\n  --max_predictions_per_seq=20 \\\n  --train_batch_size=32 \\\n  --learning_rate=0.0005 \\\n  --num_train_steps=10000 \\\n  --num_warmup_steps=10000", "python run_pretraining.py \\\n  --input_file=/tmp/tf_examples.tfrecord \\\n  --output_dir=/tmp/pretraining_output \\\n  --do_train=True \\\n  --do_eval=True \\\n  --bert_config_file=./model/wwm_uncased_L-24_H-1024_A-16/bert_config.json \\\n  --init_checkpoint=./model/wwm_uncased_L-24_H-1024_A-16/bert_model.ckpt \\\n  --train_batch_size=32 \\\n  --max_seq_length=512 \\\n  --max_predictions_per_seq=20 \\\n  --num_train_steps=10000 \\\n  --num_warmup_steps=10000 \\\n  --learning_rate=0.0005", "python run_pretraining.py \\\n  --input_file=/tmp/tf_examples.tfrecord \\\n  --output_dir=/tmp/pretraining_output \\\n  --do_train=True \\\n  --do_eval=True \\\n  --bert_config_file=./model/wwm_uncased_L-24_H-1024_A-16/bert_config.json \\\n  --init_checkpoint=./model/wwm_uncased_L-24_H-1024_A-16/bert_model.ckpt \\\n  --train_batch_size=32 \\\n  --max_seq_length=512 \\\n  --max_predictions_per_seq=20 \\\n  --num_train_steps=10000 \\\n  --num_warmup_steps=1000 \\\n  --learning_rate=0.0005", "python run_pretraining.py --input_file=input_file.tfrecord --output_dir=/tmp/pretraining_output --do_train=True --do_eval=True --bert_config_file=./model/wwm_uncased_L-24_H-1024_A-16/bert_config.json --init_checkpoint=./model/wwm_uncased_L-24_H-1024_A-16/bert_model.ckpt --train_batch_size=32 --max_seq_length=512 --max_predictions_per_seq=20 --num_train_steps=10000 --num_warmup_steps=10000 --learning_rate=0.0005", "python run_pretraining.py \\\n--input_file=./pretraining_data.tfrecord \\\n--output_dir=./finetuned_wwm_model \\\n--do_train=True \\\n--do_eval=True \\\n--bert_config_file=./model/wwm_uncased_L-24_H-1024_A-16/bert_config.json \\\n--init_checkpoint=./model/wwm_uncased_L-24_H-1024_A-16/bert_model.ckpt \\\n--train_batch_size=32 \\\n--max_seq_length=512 \\\n--max_predictions_per_seq=20 \\\n--num_train_steps=10000 \\\n--num_warmup_steps=10000 \\\n--learning_rate=0.0005"]}
{"id": 49, "github_id": 2, "output": ["python create_pretraining_data.py \\\n  --input_file=./sample_text.txt \\\n  --output_file=/tmp/tf_examples.tfrecord \\\n  --vocab_file=$BERT_BASE_DIR/vocab.txt \\\n  --do_lower_case=True \\\n  --max_seq_length=256 \\\n  --max_predictions_per_seq=20 \\\n  --masked_lm_prob=0.15 \\\n  --random_seed=12345 \\\n  --dupe_factor=5\n\n# Use this to run the pre-training\npython run_pretraining.py \\\n  --input_file=/tmp/tf_examples.tfrecord \\\n  --output_dir=/tmp/pretraining_output \\\n  --do_train=True \\\n  --do_eval=True \\\n  --bert_config_file=$BERT_BASE_DIR/bert_config.json \\\n  --init_checkpoint=$BERT_BASE_DIR/bert_model.ckpt \\\n  --train_batch_size=32 \\\n  --max_seq_length=256 \\\n  --max_predictions_per_seq=20 \\\n  --num_train_steps=300000 \\\n  --num_warmup_steps=300000 \\\n  --learning_rate=0.0001", "python create_pretraining_data.py \\\n  --input_file=./sample_text.txt \\\n  --output_file=/tmp/tf_examples.tfrecord \\\n  --vocab_file=$BERT_BASE_DIR/vocab.txt \\\n  --do_lower_case=True \\\n  --max_seq_length=256 \\\n  --max_predictions_per_seq=20 \\\n  --masked_lm_prob=0.15 \\\n  --random_seed=12345 \\\n  --dupe_factor=5\n\npython run_pretraining.py \\\n  --input_file=/tmp/tf_examples.tfrecord \\\n  --output_dir=/tmp/pretraining_output \\\n  --do_train=True \\\n  --do_eval=True \\\n  --bert_config_file=$BERT_BASE_DIR/bert_config.json \\\n  --init_checkpoint=$BERT_BASE_DIR/bert_model.ckpt \\\n  --train_batch_size=32 \\\n  --max_seq_length=256 \\\n  --max_predictions_per_seq=20 \\\n  --num_train_steps=300000 \\\n  --num_warmup_steps=300000 \\\n  --learning_rate=0.0001", "python run_pretraining.py \\\n  --input_file=/tmp/tf_examples.tfrecord \\\n  --output_dir=/tmp/pretraining_output \\\n  --do_train=True \\\n  --do_eval=True \\\n  --bert_config_file=$BERT_BASE_DIR/bert_config.json \\\n  --init_checkpoint=$BERT_BASE_DIR/bert_model.ckpt \\\n  --train_batch_size=32 \\\n  --max_seq_length=256 \\\n  --max_predictions_per_seq=20 \\\n  --num_train_steps=300000 \\\n  --num_warmup_steps=300000 \\\n  --learning_rate=1e-4", "python run_pretraining.py \\\n  --input_file=/path/to/your/train.tfrecord \\\n  --output_dir=/path/to/your/output_dir \\\n  --do_train=True \\\n  --do_eval=True \\\n  --bert_config_file=/path/to/uncased_L-12_H-768_A-12/bert_config.json \\\n  --init_checkpoint=/path/to/uncased_L-12_H-768_A-12/bert_model.ckpt \\\n  --train_batch_size=32 \\\n  --max_seq_length=256 \\\n  --max_predictions_per_seq=20 \\\n  --num_train_steps=300000 \\\n  --num_warmup_steps=300000 \\\n  --learning_rate=0.0001", "python run_pretraining.py \\\n  --input_file=/tmp/tf_examples.tfrecord \\\n  --output_dir=/tmp/pretraining_output \\\n  --do_train=True \\\n  --do_eval=True \\\n  --bert_config_file=$BERT_BASE_DIR/bert_config.json \\\n  --init_checkpoint=$BERT_BASE_DIR/bert_model.ckpt \\\n  --train_batch_size=32 \\\n  --max_seq_length=256 \\\n  --max_predictions_per_seq=20 \\\n  --num_train_steps=300000 \\\n  --num_warmup_steps=300000 \\\n  --learning_rate=1e-4"]}
{"id": 53, "github_id": 2, "output": ["python run_pretraining.py \\\n  --input_file=/path/to/input_data.tf \\\n  --output_dir=./model/uncased_L-12_H-768_A-12_pretraining_output \\\n  --do_train=True \\\n  --do_eval=True \\\n  --bert_config_file=./model/uncased_L-12_H-768_A-12/bert_config.json \\\n  --init_checkpoint=./model/uncased_L-12_H-768_A-12/bert_model.ckpt \\\n  --train_batch_size=32 \\\n  --max_seq_length=512 \\\n  --max_predictions_per_seq=30 \\\n  --num_train_steps=300000 \\\n  --num_warmup_steps=300000 \\\n  --learning_rate=0.0001", "python run_pretraining.py \\\n  --input_file=/tmp/tf_examples.tfrecord \\\n  --output_dir=./model/uncased_L-12_H-768_A-12 \\\n  --do_train=True \\\n  --do_eval=True \\\n  --bert_config_file=./model/uncased_L-12_H-768_A-12/bert_config.json \\\n  --init_checkpoint=./model/uncased_L-12_H-768_A-12/bert_model.ckpt \\\n  --train_batch_size=32 \\\n  --max_seq_length=512 \\\n  --max_predictions_per_seq=30 \\\n  --num_train_steps=300000 \\\n  --num_warmup_steps=300000 \\\n  --learning_rate=0.0001", "python create_pretraining_data.py \\\n  --input_file=./sample_text.txt \\\n  --output_file=./output/tf_examples.tfrecord \\\n  --vocab_file=./model/uncased_L-12_H-768_A-12/vocab.txt \\\n  --do_lower_case=True \\\n  --max_seq_length=512 \\\n  --max_predictions_per_seq=30 \\\n  --masked_lm_prob=0.15 \\\n  --random_seed=12345 \\\n  --dupe_factor=5\nclear\n\npython run_pretraining.py \\\n  --input_file=./output/tf_examples.tfrecord \\\n  --output_dir=./output/pretraining_output \\\n  --do_train=True \\\n  --do_eval=True \\\n  --bert_config_file=./model/uncased_L-12_H-768_A-12/bert_config.json \\\n  --init_checkpoint=./model/uncased_L-12_H-768_A-12/bert_model.ckpt \\\n  --train_batch_size=32 \\\n  --max_seq_length=512 \\\n  --max_predictions_per_seq=30 \\\n  --num_train_steps=300000 \\\n  --num_warmup_steps=300000 \\\n  --learning_rate=0.0001", "python run_pretraining.py \\\n  --input_file=./data.tfrecord \\\n  --output_dir=./pretraining_output \\\n  --do_train=True \\\n  --do_eval=True \\\n  --bert_config_file=./model/uncased_L-12_H-768_A-12/bert_config.json \\\n  --init_checkpoint=./model/uncased_L-12_H-768_A-12/bert_model.ckpt \\\n  --train_batch_size=32 \\\n  --max_seq_length=512 \\\n  --max_predictions_per_seq=30 \\\n  --num_train_steps=300000 \\\n  --num_warmup_steps=300000 \\\n  --learning_rate=0.0001", "python run_pretraining.py \\\n  --input_file=./sample_text.txt \\\n  --output_dir=./pretraining_output \\\n  --do_train=True \\\n  --do_eval=True \\\n  --bert_config_file=./model/bert_config.json \\\n  --init_checkpoint=./model/bert_model.ckpt \\\n  --train_batch_size=32 \\\n  --max_seq_length=512 \\\n  --max_predictions_per_seq=30 \\\n  --num_train_steps=300000 \\\n  --num_warmup_steps=300000 \\\n  --learning_rate=0.0001"]}
{"id": 57, "github_id": 2, "output": ["python run_pretraining.py \\\n  --input_file=/tmp/tf_examples.tfrecord \\\n  --output_dir=/tmp/pretraining_output \\\n  --do_train=True \\\n  --do_eval=True \\\n  --bert_config_file=./model/wwm_uncased_L-24_H-1024_A-16/bert_config.json \\\n  --init_checkpoint=./model/wwm_uncased_L-24_H-1024_A-16/bert_model.ckpt \\\n  --train_batch_size=32 \\\n  --max_seq_length=512 \\\n  --max_predictions_per_seq=20 \\\n  --num_train_steps=300000 \\\n  --num_warmup_steps=300000 \\\n  --learning_rate=0.0001", "python run_pretraining.py \\\n  --input_file=/tmp/tf_examples.tfrecord \\\n  --output_dir=/tmp/pretraining_output \\\n  --do_train=True \\\n  --do_eval=True \\\n  --bert_config_file=./model/wwm_uncased_L-24_H-1024_A-16/bert_config.json \\\n  --init_checkpoint=./model/wwm_uncased_L-24_H-1024_A-16/bert_model.ckpt \\\n  --train_batch_size=32 \\\n  --max_seq_length=512 \\\n  --max_predictions_per_seq=20 \\\n  --num_train_steps=300000 \\\n  --num_warmup_steps=3000 \\\n  --learning_rate=0.0001", "python run_pretraining.py   --input_file=/tmp/tf_examples.tfrecord   --output_dir=/tmp/pretraining_output   --do_train=True   --do_eval=True   --bert_config_file=$BERT_BASE_DIR/bert_config.json   --init_checkpoint=$BERT_BASE_DIR/bert_model.ckpt   --train_batch_size=32   --max_seq_length=512   --max_predictions_per_seq=20   --num_train_steps=300000   --num_warmup_steps=300000   --learning_rate=0.0001", "python run_pretraining.py \\\n  --input_file=/path/to/your/dataset \\\n  --output_dir=/path/for/saving/your/model/checkpoints \\\n  --do_train=True \\\n  --do_eval=True \\\n  --bert_config_file=./model/wwm_uncased_L-24_H-1024_A-16/bert_config.json \\\n  --init_checkpoint=./model/wwm_uncased_L-24_H-1024_A-16/bert_model.ckpt \\\n  --train_batch_size=32 \\\n  --max_seq_length=512 \\\n  --max_predictions_per_seq=20 \\\n  --num_train_steps=300000 \\\n  --num_warmup_steps=300000 \\\n  --learning_rate=0.0001", "python run_pretraining.py \\\n  --input_file=/tmp/tf_examples.tfrecord \\\n  --output_dir=/tmp/pretraining_output \\\n  --do_train=True \\\n  --do_eval=True \\\n  --bert_config_file=./model/wwm_uncased_L-24_H-1024_A-16/bert_config.json \\\n  --init_checkpoint=./model/wwm_uncased_L-24_H-1024_A-16/bert_model.ckpt \\\n  --train_batch_size=32 \\\n  --max_seq_length=512 \\\n  --max_predictions_per_seq=20 \\\n  --num_train_steps=300000 \\\n  --num_warmup_steps=300000 \\\n  --learning_rate=0.0001"]}
{"id": 61, "github_id": 3, "output": ["class ImageCaptioning:\n\n    def __init__(self, image_encoder, text_decoder, prompt=None, max_txt_len=40):\n        self.image_encoder = image_encoder\n        self.text_decoder = text_decoder\n        self.prompt = prompt\n        self.max_txt_len = max_txt_len\n\n    def forward_encoder(self, samples):\n        return self.image_encoder(samples)\n\n    def forward_decoder(self, samples, image_embeds):\n        return self.text_decoder(samples, image_embeds)\n\n    def forward(self, samples):\n        image_embeds = self.forward_encoder(samples)\n        return self.forward_decoder(samples, image_embeds)\n\n    def generate(self, samples, use_nucleus_sampling=False, num_beams=3, max_length=30, min_length=10, top_p=0.9, repetition_penalty=1.0, num_captions=1):\n        image_embeds = self.forward_encoder(samples)\n        return self.text_decoder.generate(image_embeds, use_nucleus_sampling, num_beams, max_length, min_length, top_p, repetition_penalty, num_captions)\n\n    @classmethod\n    def from_config(cls, cfg):\n        image_encoder = cfg['image_encoder']\n        text_decoder = cfg['text_decoder']\n        return cls(image_encoder, text_decoder, cfg.get('prompt'), cfg.get('max_txt_len'))", "from lavis.models import load_model_and_preprocess\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# loads BLIP caption base model, with finetuned checkpoints on MSCOCO captioning dataset.\n# this also loads the associated image processors\nmodel, vis_processors, _ = load_model_and_preprocess(name=\"blip_caption\", model_type=\"base_coco\", is_eval=True, device=device)\n# preprocess the image\n# vis_processors stores image transforms for \"train\" and \"eval\" (validation / testing / inference)\nimage = vis_processors[\"eval\"](raw_image).unsqueeze(0).to(device)\n# generate caption\nmodel.generate({\"image\": image})\n# ['a large fountain spewing water into the air']", "model, vis_processors, _ = load_model_and_preprocess(name=\"blip_caption\", model_type=\"base_coco\", is_eval=True, device=device)\n# preprocess the image\n# vis_processors stores image transforms for \"train\" and \"eval\" (validation / testing / inference)\nimage = vis_processors[\"eval\"](raw_image).unsqueeze(0).to(device)\n# generate caption\nmodel.generate({\"image\": image})", "from lavis.models import load_model_and_preprocess\n\n# setup device to use\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# loads BLIP caption base model, with finetuned checkpoints on MSCOCO captioning dataset.\n# this also loads the associated image processors\nmodel, vis_processors, _ = load_model_and_preprocess(name=\"blip_caption\", model_type=\"base_coco\", is_eval=True, device=device)\n# preprocess the image\n# vis_processors stores image transforms for \"train\" and \"eval\" (validation / testing / inference)\nimage = vis_processors[\"eval\"](raw_image).unsqueeze(0).to(device)\n# generate caption\nmodel.generate({\"image\": image})", "import torch\nfrom lavis.models import load_model_and_preprocess\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n# loads BLIP caption base model, with finetuned checkpoints on MSCOCO captioning dataset.\n# this also loads the associated image processors\nmodel, vis_processors, _ = load_model_and_preprocess(name='blip_caption', model_type='base_coco', is_eval=True, device=device)\n# preprocess the image\n# vis_processors stores image transforms for 'train' and 'eval' (validation / testing / inference)\nimage = vis_processors['eval'](raw_image).unsqueeze(0).to(device)\n# generate caption\nmodel.generate({'image': image})"]}
{"id": 65, "github_id": 3, "output": ["import torch\nfrom lavis.models import load_model_and_preprocess\nfrom PIL import Image\n\n# setup device to use\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# load the image\nraw_image = Image.open('path_to_your_image.jpg').convert(\"RGB\")\n\n# load model and preprocessors\nmodel, vis_processors, txt_processors = load_model_and_preprocess(name=\"albef_vqa\", model_type=\"vqav2\", is_eval=True, device=device)\n\n# preprocess the image\nimage = vis_processors[\"eval\"](raw_image).unsqueeze(0).to(device)\n\n# ask a question\nquestion = \"What is in front of the cat?\"\nquestion = txt_processors[\"eval\"](question)\n\n# make inference\nanswer = model.predict_answers(samples={\"image\": image, \"text_input\": question}, inference_method=\"generate\")\nprint('Answer:', answer)", "from lavis.models import load_model_and_preprocess\n\n# load model and preprocessors\nmodel, vis_processors, txt_processors =\nload_model_and_preprocess(name='albef_vqa', model_type='vqav2', is_eval=True, device=device)\n\n# specify your question\nquestion = 'What is in front of the cat?'\n\n# preprocess the image and question\nimage = vis_processors['eval'](raw_image).unsqueeze(0).to(device)\nquestion = txt_processors['eval'](question)\n\n# predict the answer\nmodel.predict_answers(samples={\"image\": image, \"text_input\": question}, inference_method='generate')", "from lavis.models import load_model_and_preprocess\nmodel, vis_processors, txt_processors = load_model_and_preprocess(name=\"albef_vqa\", model_type=\"vqav2\", is_eval=True, device=device)\n# ask a question about what lies in front of the cat\nquestion = \"What is in front of the cat?\"\nimage = vis_processors[\"eval\"](raw_image).unsqueeze(0).to(device)\nquestion = txt_processors[\"eval\"](question)\nmodel.predict_answers(samples={\"image\": image, \"text_input\": question}, inference_method=\"generate\")", "from lavis.models import load_model_and_preprocess\n\n# load the albaf vqa model\nmodel, vis_processors, txt_processors = load_model_and_preprocess(name=\"albef_vqa\", model_type=\"vqav2\", is_eval=True, device=device)\n\n# prepare your imageinput, let's say it is stored in the variable 'image_input'\n\n# preprocess the image\nimage = vis_processors[\"eval\"](image_input).unsqueeze(0).to(device)\n\n# prepare your question\nquestion = \"What is in front of the cat?\"\nquestion = txt_processors[\"eval\"](question)\n\n# make prediction\nmodel.predict_answers(samples={\"image\": image, \"text_input\": [question]}, inference_method=\"generate\")", "from lavis.models import load_model_and_preprocess\n\n# load model and preprocessors\nmodel, vis_processors, txt_processors = load_model_and_preprocess(name=\"albef_vqa\", model_type=\"vqav2\", is_eval=True, device=device)\n\n# prepare image and question\nimage = vis_processors[\"eval\"](raw_image).unsqueeze(0).to(device)\nquestion = \"What is in front of the cat?\"\nquestion = txt_processors[\"eval\"](question)\n\n# get answer from model\nanswer = model.predict_answers(samples={\"image\": image, \"text_input\": question}, inference_method=\"generate\")\nanswer"]}
{"id": 69, "github_id": 3, "output": ["This repo can not do this task(request > 5)", "This repo cannot do the task(request > 5)", "This repo cannot do the task(request > 5)", "This repo cannot do the task(request > 5)", "This repo cannot do the task(request > 5)"]}
{"id": 73, "github_id": 3, "output": ["from PIL import Image\nimport torch\nfrom lavis.models import load_model_and_preprocess\n\n# setup device to use\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# load sample image\nraw_image = Image.open('./data/image.jpg').convert(\"RGB\")\n\n# loads BLIP caption base model, with finetuned checkpoints on MSCOCO captioning dataset.\n# this also loads the associated image processors\nmodel, vis_processors, _ = load_model_and_preprocess(name=\"blip_caption\", model_type=\"base_coco\", is_eval=True, device=device)\n# preprocess the image\nimage = vis_processors[\"eval\"](raw_image).unsqueeze(0).to(device)\n# generate caption\ncaption = model.generate({\"image\": image})\n\ncaption", "import torch\nfrom PIL import Image\nimport requests\nfrom transformers import Blip2Processor, Blip2ForConditionalGeneration\n\n# Load pretrained model and processor\ndvice = 'cuda' if torch.cuda.is_available() else 'cpu'\nprocessor = Blip2Processor.from_pretrained('Salesforce/blip2-opt-2.7b', torch_dtype=torch.float16)\nmodel = Blip2ForConditionalGeneration.from_pretrained('Salesforce/blip2-opt-2.7b', torch_dtype=torch.float16).to(device)\n\n# Load target image\nimg_path = './data/image.jpg' # adjust the path if necessary\nimage = Image.open(img_path)\n\n# Define your textual prompt\nprompt = 'Is there a dog and a cat behind it?'\n\n# Preprocess inputs\ninputs = processor(images=image, text=prompt, return_tensors='pt')\ntext_input = inputs['input_ids'].to(device)\nimage_input = inputs['pixel_values'].to(device)\n\n# Generate prediction\nout = model.generate(**inputs)\n\n# Print result\nresult = processor.batch_decode(out, skip_special_tokens=True)[0]\nprint(result)\n", "from PIL import Image\nimport matplotlib.pyplot as plt\nfrom lavis.models import load_model_and_preprocess\n\n# Load the image\ndog_cat_image_path = './data/image.jpg'\ndog_cat_image = Image.open(dog_cat_image_path)\n\ndef show_image(img):\n    plt.imshow(img)\n    plt.axis('off')\n    plt.show()\n\n# Load a pretrained model for visual question answering\nmodel, vis_processors, txt_processors = load_model_and_preprocess(name=\"blip_vqa\", model_type=\"vqav2\", is_eval=True, device=torch.device('cuda'))\n\n# Prepare the image and question\ndog_cat_image = vis_processors[\"eval\"](dog_cat_image).unsqueeze(0).to(torch.device('cuda'))\nquestion = 'Is there a cat behind the dog in the image?'\nquestion = txt_processors[\"eval\"](question)\n\n# Perform inference with the model\nanswer = model.predict_answers(samples={\"image\": dog_cat_image, \"text_input\": question}, inference_method=\"generate\")\nprint('Answer:', answer[0])", "import torch\nimport torch.nn.functional as F\nfrom PIL import Image\nfrom torchvision import transforms\nfrom torchvision.models import resnet50\n\n# Load the pre-trained ResNet model\nmodel = resnet50(pretrained=True)\nmodel.eval()\n\n# Load the image\nimage = Image.open('./data/image.jpg')\n\n# Define the image transforms\ntransform = transforms.Compose([\n    transforms.Resize(256),\n    transforms.CenterCrop(224),\n    transforms.ToTensor(),\n    transforms.Normalize(\n        mean=[0.485, 0.456, 0.406],\n        std=[0.229, 0.224, 0.225]\n    ),\n])\n\n# Transform the image and add an extra batch dimension\nimage = transform(image).unsqueeze(0)\n\n# Make a forward pass through the model\noutput = model(image)\n\n# Get the top 5 predictions\n_, preds = torch.topk(output, 5)\n\n# Define the labels\nlabels = [\n    'n02085620',  # Chihuahua\n    'n02085782',  # Japanese spaniel\n    'n02085936',  # Maltese dog\n    'n02086079',  # Pekinese\n    'n02086240',  # Shih-Tzu\n    # ...\n    'n02123045',  # tabby\n    'n02123159',  # tiger cat\n    'n02123394',  # Persian cat\n    'n02123597',  # Siamese cat\n    'n02124075',  # Egyptian cat\n    # ...\n]\n\n# Check if there's a dog and a cat in the image\nis_dog = any(pred in labels[:5] for pred in preds[0])\nis_cat = any(pred in labels[-5:] for pred in preds[0])\n\nprint(f'Is there a dog? {is_dog}')\nprint(f'Is there a cat? {is_cat}')", "from PIL import Image\nimport torch\nfrom lavis.models import load_model_and_preprocess\n\n# setup device to use\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# load sample image\nraw_image = Image.open('./data/image.jpg').convert('RGB')\n\n# loads CLIP image-text retrieval model, this also loads the associated image processors\nmodel, vis_processors, _ = load_model_and_preprocess(name='clip', model_type='RN50', is_eval=True, device=device)\n\n# preprocess the image\nimage = vis_processors['eval'](raw_image).unsqueeze(0).to(device)\n\n# generate question\nquestion = 'Is there a cat behind the dog?'\nquestion = _['eval'](question)\n\n# get answer\nanswer = model.predict_answers(samples={'image': image, 'text_input': question}, inference_method='generate')\nprint(answer)"]}
{"id": 77, "github_id": 3, "output": ["import torch\nfrom PIL import Image\nfrom lavis.models import load_model_and_preprocess\n\n# setup device to use\ndevice = torch.device('cuda') if torch.cuda.is_available() else 'cpu'\n\n# load image\nraw_image = Image.open('test.jpg').convert('RGB')\n\n# load model and preprocessors\nmodel, vis_processors, _ = load_model_and_preprocess(\n    name='blip2_t5',\n    model_type='caption_coco_flant5xl',\n    is_eval=True,\n    device=device,\n)\n\n# prepare the image\nimage = vis_processors['eval'](raw_image).unsqueeze(0).to(device)\n\n# generate image description\ndescription = model.generate({'image': image})\nprint(description)\n", "\nimport torch\nfrom PIL import Image\nfrom lavis.models import load_model_and_preprocess\n\n# setup device to use\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# load the image\ntest_image_path = 'test.jpg'\nraw_image = Image.open(test_image_path).convert(\"RGB\")\n\n# Load the BLIP2_T5 model with the specified type (caption_coco_flant5xl)\nmodel, vis_processors, _ = load_model_and_preprocess(name=\"blip2_t5\", model_type=\"caption_coco_flant5xl\", is_eval=True, device=device)\n\n# Preprocess the image\nimage = vis_processors[\"eval\"](raw_image).unsqueeze(0).to(device)\n\n# Generate caption\nmodel.generate({\"image\": image})\n", "import torch\nfrom PIL import Image\nfrom lavis.models import load_model_and_preprocess\n\n# setup device to use\ndevice = torch.device('cuda') if torch.cuda.is_available() else 'cpu'\n\n# load the image\nraw_image = Image.open('test.jpg').convert('RGB')\n\n# load the pretrained blip2_t5 model with its preprocessors (transforms)\nmodel, vis_processors, _ = load_model_and_preprocess(name='blip2_t5', model_type='caption_coco_flant5xl', is_eval=True, device=device)\n\n# prepare the image\nimage = vis_processors['eval'](raw_image).unsqueeze(0).to(device)\n\n# generate caption\ncaption = model.generate({'image': image})\nprint('Generated caption:', caption)", "from PIL import Image\nimport torch\n\nfrom lavis.models import load_model_and_preprocess\n\n# setup device to use\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# load the image\nraw_image = Image.open('test.jpg').convert(\"RGB\")\n\n# loads blip2_t5 model, with finetuned checkpoints on MSCOCO captioning dataset.\n# this also loads the associated image processors\nmodel, vis_processors, _ = load_model_and_preprocess(\n    name=\"blip2_t5\", model_type=\"caption_coco_flant5xl\", is_eval=True, device=device\n)\n# preprocess the image\n# vis_processors stores image transforms for \"train\" and \"eval\" (validation / testing / inference)\nimage = vis_processors[\"eval\"](raw_image).unsqueeze(0).to(device)\n\n# generate caption to recognize the content of the image\ncaption_generated = model.generate({\"image\": image})\nprint(\"The content of the image is likely:\", caption_generated[0])\n", "from PIL import Image\nimport torch\nfrom lavis.models import load_model_and_preprocess\n\n# set device\ndevice = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n\n# load the model\nmodel, vis_processors, _ = load_model_and_preprocess(name='blip2_t5', model_type='caption_coco_flant5xl', is_eval=True, device=device)\n\n# load your image\nraw_image = Image.open('test.jpg').convert('RGB')\n\n# prepare the image\nimage = vis_processors['eval'](raw_image).unsqueeze(0).to(device)\n\n# generate the model's response\nresult = model.generate({'image': image})\n\nprint(result)"]}
{"id": 81, "github_id": 3, "output": ["from lavis.models import load_model_and_preprocess\nfrom PIL import Image\n\n# setup device to use\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# load sample images\nimage1 = Image.open(\"picture.jpg\").convert(\"RGB\")\nimage2 = Image.open(\"a mansion.jpg\").convert(\"RGB\")\n\n# load model and preprocessors\nmodel, vis_processors, _ = load_model_and_preprocess(name=\"albef_classification\", model_type=\"ve\", is_eval=True, device=device)\n\n# preprocess the images\nimage1 = vis_processors[\"eval\"](image1).unsqueeze(0).to(device)\nimage2 = vis_processors[\"eval\"](image2).unsqueeze(0).to(device)\n\n# extract features\nfeatures1 = model.extract_features({\"image\": image1}, mode=\"image\")\nfeatures2 = model.extract_features({\"image\": image2}, mode=\"image\")\n\n# print the shape of the extracted features\nprint(features1.image_embeds.shape)\nprint(features2.image_embeds.shape)", "from lavis.models import load_model_and_preprocess\nimport torch\nfrom PIL import Image\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# Load model and preprocessors\nmodel, preprocessors, _ = load_model_and_preprocess('albef_feature_extractor', 've', is_eval=True, device=device)\n\n# Load images\nimage1 = Image.open('picture.jpg').convert('RGB')\nimage2 = Image.open('a mansion.jpg').convert('RGB')\n\n# Preprocess images\nimage1 = preprocessors['eval'](image1).unsqueeze(0).to(device)\nimage2 = preprocessors['eval'](image2).unsqueeze(0).to(device)\n\n# Extract features\nfeatures1 = model.extract_features({'image': image1}, mode='image')\nfeatures2 = model.extract_features({'image': image2}, mode='image')\n\n# Print out the features\nprint('Features for picture.jpg:')\nprint(features1.image_embeds)\nprint('Features for a mansion.jpg:')\nprint(features2.image_embeds)", "import torch\nfrom PIL import Image\nfrom lavis.models import load_model_and_preprocess\n\n# setup device\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# load sample images\nraw_image1 = Image.open('picture.jpg').convert('RGB')\nraw_image2 = Image.open('mansion.jpg').convert('RGB')\n\n# loading the feature extractor variant of the ALBEF model\nmodel, vis_processors, _ = load_model_and_preprocess(name='albef_feature_extractor', model_type='ve', is_eval=True, device=device)\n\n# preprocess the images\nimage1 = vis_processors['eval'](raw_image1).unsqueeze(0).to(device)\nimage2 = vis_processors['eval'](raw_image2).unsqueeze(0).to(device)\n\n# extract features\nfeatures1 = model.extract_features({'image': image1}, mode='image')\nfeatures2 = model.extract_features({'image': image2}, mode='image')\n\n# print the features\nprint('Features image 1:', features1.image_embeds)\nprint('Features image 2:', features2.image_embeds)", "import torch\nfrom PIL import Image\nfrom lavis.models import load_model_and_preprocess\n\n# setup device to use\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# load model and associated preprocessors\nmodel, vis_processors, txt_processors = load_model_and_preprocess(name='albef_classification', model_type='ve', is_eval=True, device=device)\n\n# load images\nimage1 = Image.open('picture.jpg').convert('RGB')\nimage2 = Image.open('a mansion.jpg').convert('RGB')\n\n# preprocess images\nimage1 = vis_processors['eval'](image1).unsqueeze(0).to(device)\nimage2 = vis_processors['eval'](image2).unsqueeze(0).to(device)\n\n# extract features\nfeatures1 = model.extract_features({'image': image1}, mode='image').image_embeds\nfeatures2 = model.extract_features({'image': image2}, mode='image').image_embeds\n\nprint('Features for image1:', features1)\nprint('Features for image2:', features2)", "from lavis.models import load_model_and_preprocess\nfrom PIL import Image\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# load the model\nmodel, vis_processors, _ = load_model_and_preprocess(name=\"albef_feature_extractor\", model_type=\"ve\", is_eval=True, device=device)\n\n# load images\nimage_paths = [\"picture.jpg\", \"a mansion.jpg\"]\nimages = [Image.open(path).convert(\"RGB\") for path in image_paths]\n\n# preprocess the images\npreprocessed_images = [vis_processors[\"eval\"](img).unsqueeze(0).to(device) for img in images]\n\n# extract features\nfor img in preprocessed_images:\n    features = model.extract_features({\"image\": img}, mode=\"image\")\n    print(features.image_embeds)"]}
{"id": 85, "github_id": 3, "output": ["from PIL import Image\nfrom lavis.models import load_model_and_preprocess\n\n# setup device to use\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# load sample image\nraw_image = Image.open(\"./image/example.jpg\").convert(\"RGB\")\n\n# loads BLIP caption base model, with finetuned checkpoints on MSCOCO captioning dataset.\n# this also loads the associated image processors\nmodel, vis_processors, _ = load_model_and_preprocess(name=\"blip_caption\", model_type=\"base_coco\", is_eval=True, device=device)\n# preprocess the image\nimage = vis_processors[\"eval\"](raw_image).unsqueeze(0).to(device)\n\n# Extract Features\nfeatures = model.extract_features({\"image\": image}, mode='image')\n\n# Now \"features\" contains feature embeddings extracted from the image by blip_caption model\nprint(features.image_embeds.shape)  # For high-dimensional features\nprint(features.image_embeds_proj.shape)  # For low-dimensional projected features", "import torch\nfrom PIL import Image\nfrom lavis.models import load_model_and_preprocess\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# load sample image\nraw_image = Image.open(\"./image/example.jpg\").convert(\"RGB\")\n\n# load the blip_caption base model for feature extraction, with associated image processors\nmodel, vis_processors, _ = load_model_and_preprocess(\n    name=\"blip_caption\", model_type=\"base_coco\", feature_extractor=True, is_eval=True, device=device\n)\n\n# preprocess the image using the provided image processors. \n# The processors perform necessary image transformations before feeding into the model.\n# The 'eval' processors are used during inference\nimage = vis_processors[\"eval\"](raw_image).unsqueeze(0).to(device)\n\n# extract features\nfeatures = model.extract_features({\"image\": image})\n\n# features is a dictionary containing different types of features. Let's print them out.\nprint(features.keys())\n\n# to use the features, you can do the following:\n# features['multimodal_embeds'] is the joint embedding of image and text (in this case, this is the image embedding)\nmultimodal_embeds = features['multimodal_embeds'].cpu().detach().numpy()\nprint(multimodal_embeds.shape)  # should print: (1, 768)\n\n# features['image_embeds'] is the output from the vision backbone (the final convolutional layer)\nimage_embeds = features['image_embeds'].cpu().detach().numpy()\nprint(image_embeds.shape)  # should print: (1, 2048, 7, 7)", "from PIL import Image\nfrom lavis.models import load_model_and_preprocess\n\n# Load image\ndef load_image(image_path):\n    return Image.open(image_path).convert(\"RGB\")\n\n# Load models and preprocess image\ndef extract_features(image):\n    model, vis_processors, _ = load_model_and_preprocess(\n        name=\"blip_feature_extractor\", model_type=\"base\", is_eval=True\n    )\n    image = vis_processors[\"eval\"](image).unsqueeze(0)\n\n    # Extract features\n    features = model.extract_features({\"image\": image})\n    return features\n\n# Implementation\ndef main():\n    image_path = './image/example.jpg'\n    image = load_image(image_path)\n    features = extract_features(image)\n    print(features)\n\n# Run code\nmain()\n", "import torch\nfrom PIL import Image\nfrom lavis.models import load_model_and_preprocess\n\n# setup device to use\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# load sample image\nraw_image = Image.open('./image/example.jpg').convert('RGB')\n\n# load the base feature extractor variant of the BLIP model and its associated preprocessors\nmodel, vis_processors, _ = load_model_and_preprocess(name='blip_feature_extractor', model_type='base', is_eval=True, device=device)\n\n# preprocess the image\nimage = vis_processors['eval'](raw_image).unsqueeze(0).to(device)\n\n# extract features\nfeatures = model.extract_features({'image': image})\n\n# features.multimodal_embeds stores the multimodal features\n# features.image_embeds stores the image features\n# features.image_embeds_proj stores the low-dimensional projected image features\n\nprint(features.multimodal_embeds)\nprint(features.image_embeds)\nprint(features.image_embeds_proj)", "from PIL import Image\nfrom lavis.models import load_model_and_preprocess\n\n# setup device to use\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# load sample image\nraw_image = Image.open(\"./image/example.jpg\").convert(\"RGB\")\n# loads BLIP caption base model, with finetuned checkpoints on MSCOCO captioning dataset.\n# this also loads the associated image processors\nmodel, vis_processors, _ = load_model_and_preprocess(name=\"blip_caption\", model_type=\"base_coco\", is_eval=True, device=device)\n# preprocess the image\n# vis_processors stores image transforms for \"train\" and \"eval\" (validation / testing / inference)\nimage = vis_processors[\"eval\"](raw_image).unsqueeze(0).to(device)\n# generate feature\nfeatures = model.extract_features(\"image\": image))\n"]}
{"id": 89, "github_id": 3, "output": ["import torch\nfrom lavis.models import load_model_and_preprocess\nfrom lavis.datasets import DatasetsBuilder\nfrom lavis.runners.runner_eval import EvaluationRunner\n\n# Set the device to 'cuda' if you have a CUDA compatible GPU\ndevice = 'cuda'\n\n# Specify the path to your dataset directory\ndataset_dir = 'path/to/your/dataset/directory' # e.g., '/home/username/dataset/'\n\n# Load the blip2 flant5xl model along with preprocessors\nmodel, vis_processors, _ = load_model_and_preprocess(\n    name='blip2_t5',\n    model_type='caption_coco_flant5xl',\n    is_eval=True,\n    device=device\n)\n\n# Build coco caption dataset\ndataset_builder = DatasetsBuilder('coco_caption')\ndataset = dataset_builder(dataset_dir,\n                          ann_file='annotations/captions_val2017.json',\n                          img_dir='val2017',\n                          split='val',\n                          transforms=vis_processors['eval']).build()\n\n# Run evaluation\nrunner = EvaluationRunner(model, dataset, device=device)\nmetrics, score, outputs, _ = runner()\n\n# Report metrics\nprint(f'Evaluated score: {score}')\nfor metric, value in metrics.items():\n    print(f'{metric}: {value}')\n", "import torch\nfrom lavis.models import load_model_and_preprocess\n# setup device to use\ndevice = torch.device(\"cuda\") if torch.cuda.is_available() else \"cpu\"\n# loads BLIP-2 pre-trained model\nmodel, vis_processors, _ = load_model_and_preprocess(name=\"blip2_t5\", model_type=\"pretrain_flant5xxl\", is_eval=True, device=device)\n\n# Load your dataset here\n# ...\n\n# Then you can use the model to evaluate each data\n# you may need to properly preprocess your data using vis_processors", "import torch\nfrom lavis.models import load_model_and_preprocess\n# Load the model\nmodel, vis_processors, _ = load_model_and_preprocess(name=\"blip2\", model_type=\"caption_coco_flant5xl\", is_eval=True)\n# Prepare the dataset\nfrom lavis.datasets.builders import CaptionBuilder\nbuilder = CaptionBuilder(name=\"coco\", split=\"test\")\n# Prepare data loader\nfrom torch.utils.data import DataLoader\ndata_loader = DataLoader(builder, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS)\n# Run the evaluation\nmodel.eval()\nfor batch in data_loader:\n    images = batch['image'].to(device)\n    captions = batch['caption'].to(device)\n    with torch.no_grad():\n        outputs = model(images)\n        # TODO: compute metric based on your needs\n}\n# Note: device should be your computing device, the BATCH_SIZE and NUM_WORKERS need to be set according to your machine capability.", "from lavis.models import load_model_and_preprocess, load_config\nfrom lavis.datasets.datasets.vqa_datasets import CocoImageCaptionTor\nfrom torch.utils.data import DataLoader\nfrom lavis.dataloaders import create_dataloader\n\n# Load model and preprocessor\nmodel, processors, _ = load_model_and_preprocess(name=\"blip2_t5\", model_type=\"caption_coco_flant5xl\", is_eval=True, device=device)\n\n# Load model and preprocessor\nvocab = processors['text'].tokenizer.get_vocab()\nconfig = load_config('lavis/configs/models/blip2_models/blip2_t5_caption_coco_flant5xl.yaml')\n\n# Create dataset\ncap_coco_dset = CocoImageCaptionTor(root='path_to_coco_images', vocab=vocab, transforms=processors['eval'], split='test')\n\n# Create dataloader\ndata_loader = create_dataloader(cap_coco_dset, config.dataloader)\n\n# Evaluation loop\nmodel.eval()\nfor batch in data_loader:\n    outputs = model(batch)\n    # Add your evaluation code here\n\n# Print out evaluation results.", "import torch\nfrom lavis.models import load_model_and_preprocess\n\n# setup device to use\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# loads BLIP-2 'flant5xl' model for image caption task. \n# This also loads the associated image processors\nmodel, processors, _ = load_model_and_preprocess(name='blip2_caption', model_type='flant5xl', is_eval=True, device=device)\n\n\n# Assume you have your eval dataloader ready:\n# dataloader = ...\n\n# You can then perform evaluation on the 'cap_coco' dataset as below:\nmodel.eval()\nfor batch in dataloader:\n    # move batch data to device\n    batch = {k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in batch.items()}\n    # forward pass through the model\n    output = model(**batch)\n    # compute metrics from output and batch['target']\n    # metrics = compute_metrics(output, batch['target'])\n\n\n# Compute the average metrics for all batches as the final evaluation results.\n"]}
{"id": 93, "github_id": 4, "output": ["from deepfloyd_if.modules import IFStageII\nfrom deepfloyd_if.modules.t5 import T5Embedder\nfrom deepfloyd_if.pipelines import dream\n\n# Initialize the models\nif_II = IFStageII('IF-II-L-v1.0', device='cuda:0')\nt5 = T5Embedder(device=\"cpu\")\n\n# Set Prompt\ndog_prompts = ['A golden retriever playing with a ball', 'A dalmatian running in a field', 'A bulldog sitting in the park', 'A poodle with a funny haircut']\n\n# Generate visuals for each dog's prompt\nfor prompt in dog_prompts:\n    result = dream(\n        t5=t5, \n        if_II=if_II,\n        prompt=[prompt],\n        seed=42,\n        if_II_kwargs={\n            'guidance_scale': 5.0,\n            'sample_timestep_respacing': 'smart50',\n        },\n    )\n    if_II.show(result['II'], 1, 20)", "from deepfloyd_if.modules import IFStageII, T5Embedder\nfrom deepfloyd_if.pipelines import dream\n\ndevice = 'cuda:0'\nif_II = IFStageII('IF-II-L-v1.0', device=device)\nt5 = T5Embedder(device=\"cpu\")\n\nprompt = 'dog'\ncount = 4\n\nresult = dream(\n    t5=t5, if_II=if_II,\n    prompt=[prompt]*count,\n    seed=42,\n    if_II_kwargs={\n        \"guidance_scale\": 5.0,\n        \"sample_timestep_respacing\": \"smart50\",\n    },\n)\n\nif_II.show(result['II'], size=14)", "from deepfloyd_if.modules import IFStageII\nfrom deepfloyd_if.modules.t5 import T5Embedder\nfrom deepfloyd_if.pipelines import dream\n\n# Loading the IF-II module\nif_II = IFStageII('IF-II-L-v1.0', device='cuda:0')\n\n# Loading the T5 text embedder\nT5embedder = T5Embedder(device='cuda:0')\n\n# Prompts for the images\nprompts = ['A puppy', 'An adult dog', 'A dog running', 'A dog playing with a ball']\n\n# Generating the images\nresults = dream(t5=T5embedder, if_II=if_II, prompt=prompts, \n            seed=1, batch_size=4,\n            if_II_kwargs={'guidance_scale': 5.0, 'sample_timestep_respacing': '25'})\n\n# The resulting images are in results['II']", "from deepfloyd_if.modules import IFStageI, IFStageII\nfrom deepfloyd_if.modules.t5 import T5Embedder\nfrom deepfloyd_if.pipelines import dream\n\n# Loading the models into VRAM\n# Please substitute 'cuda:0' with the device you're using.\ndevice = 'cuda:0'\nif_II = IFStageII('IF-II-L-v1.0', device=device)\nt5 = T5Embedder(device=\"cpu\")\n\n# Dream\nprompt = 'dog'\ncount = 4\nresult = dream(\n    t5=t5, if_II=if_II,\n    prompt=[prompt]*count,\n    seed=42,\n    if_II_kwargs={\n        \"guidance_scale\": 5.0,\n        \"sample_timestep_respacing\": \"smart50\",\n    }\n)\n\n# Display result\nif_II.show(result['II'], size=14)", "from deepfloyd_if.modules import IFStageII\nfrom diffusers import DiffusionPipeline\nfrom diffusers.utils import pt_to_pil\nimport torch\n\n\n# stage 2\ndevice = 'cuda:0'\nif_II = IFStageII('IF-II-L-v1.0', device=device)\n\nprompt = ['a bulldog', 'a german shepherd', 'a labrador retriever', 'a beagle']\n\n# text embeds\n# Assuming stage_1 is the model from first stage, and it has been defined before.\nprompt_embeds, negative_embeds = stage_1.encode_prompt(prompt)\n\nresults = []\nfor i in range(4):\n    generator = torch.manual_seed(i)\n    image = if_II(\n    prompt_embeds=prompt_embeds[i], \n    negative_prompt_embeds=negative_embeds[i], \n    generator=generator, \n    guidance_scale=5.0, \n    output_type=\"pt\"\n    ).images\n    pt_to_pil(image)[0].save(f\"./if_stage_II_dog{i}.png\")\n    results.append(image)\n\n# 'results' contains the generated images\n"]}
{"id": 97, "github_id": 4, "output": ["from deepfloyd_if.modules import IFStageII\nfrom deepfloyd_if.modules.t5 import T5Embedder\nfrom deepfloyd_if.pipelines import dream\n\n# Load the models\ndevice = 'cuda:0'\nt5 = T5Embedder(device=\"cpu\")\nif_II = IFStageII('IF-II-L-v1.0', device=device)\n\n# Define the prompt\ndog_prompt = 'color photo of a beagle dog'\ncount = 4\n\n# Generate images\nresult = dream(\n    t5=t5, if_II=if_II,\n    prompt=[dog_prompt]*count,\n    seed=42,\n    if_II_kwargs={\n        \"guidance_scale\": 4.0,\n        \"sample_timestep_respacing\": \"smart100\",\n    },\n)\n\n# Show the generated images\nif_II.show(result['II'], size=14)", "from deepfloyd_if.modules import IFStageII\nfrom deepfloyd_if.modules.t5 import T5Embedder\nfrom deepfloyd_if.pipelines import dream\n\n# Load the models\ndevice = 'cuda:0'\nif_II = IFStageII('IF-II-L-v1.0', device=device)\nt5 = T5Embedder(device=\"cpu\")\n\nprompt = 'A picture of a dog'\ncount = 4\n\n# Generate the images\nresult = dream(\n    t5=t5, \n    if_II=if_II, \n    prompt=[prompt]*count,\n    seed=42,\n    if_II_kwargs={\n        \"guidance_scale\": 4.0,\n        \"sample_timestep_respacing\": \"smart100\",\n    }\n)\n\n# Show the result\nif_II.show(result['II'], size=14)", "from deepfloyd_if.modules import IFStageII\nfrom deepfloyd_if.modules.t5 import T5Embedder\ndevice = 'cuda:0'\nif_II = IFStageII('IF-II-L-v1.0', device=device)\nt5 = T5Embedder(device=\"cpu\")\n\nprompt = ['a dog']*4\n\nfrom deepfloyd_if.pipelines import dream\nresult = dream(\n    t5=t5, if_II=if_II,\n    prompt=prompt,\n    seed=42,\n    if_II_kwargs={\n        \"guidance_scale\": 4.0,\n        \"sample_timestep_respacing\": \"smart100\",\n    },\n)", "from deepfloyd_if.modules import IFStageII\nfrom deepfloyd_if.modules.t5 import T5Embedder\nfrom deepfloyd_if.pipelines import dream\n\n# Initialize the models\nif_II = IFStageII('IF-II-L-v1.0', device='cuda:0')\nt5 = T5Embedder(device='cuda:0')\n\n# Generate images\nprompt = ['a dog'] * 4  # prompt for four dogs\nresult = dream(\n    t5=t5, if_II=if_II,\n    prompt=prompt,\n    seed=42,\n    if_II_kwargs={\n        'guidance_scale': 4.0,\n        'sample_timestep_respacing': 'smart100',\n    },\n)\n\n# Display the generated images\nif_II.show(result['II'], 2, 2)\n", "from deepfloyd_if.modules import IFStageII\nfrom deepfloyd_if.modules.t5 import T5Embedder\n\n\ndevice = 'cuda:0'\n\nif_II = IFStageII('IF-II-L-v1.0', device=device)\nt5 = T5Embedder(device=\"cpu\")\n\n\nfrom deepfloyd_if.pipelines import dream\n\nprompt = ['a cute puppy', 'a sleepy dog', 'a happy dog playing in the park', 'a dog doing tricks']\n\nresult = dream(\n    t5=t5, if_II=if_II,\n    prompt=prompt,\n    seed=42,\n    if_II_kwargs={\n        \"guidance_scale\": 4.0,\n        \"sample_timestep_respacing\": \"smart100\",\n    },\n)"]}
{"id": 101, "github_id": 4, "output": ["import PIL.Image\n\nimage_path = 'image.jpg'\n\nimage = PIL.Image.open(image_path)\n\nstyle_prompt = \"animated cartoon style\"\n\nresult = style_transfer(\n    if_I,\n    support_pil_img=image,\n    style_prompt=style_prompt,\n)\n\nresult.show()", "from deepfloyd_if.modules import IFStageI\nfrom deepfloyd_if.modules.t5 import T5Embedder\nfrom deepfloyd_if.pipelines import style_transfer\nfrom PIL import Image\n\n# Load the image\nraw_pil_image = Image.open('image.jpg')\n\n# Load the models\ndevice = 'cuda:0'\nif_I = IFStageI('IF-I-M-v1.0', device=device)\nt5 = T5Embedder(device=\"cpu\")\n\n# Style Transfer\nresult = style_transfer(\n    t5=t5, if_I=if_I,\n    support_pil_img=raw_pil_image,\n    style_prompt=[\n        'in style of animated cartoon',\n    ],\n    seed=42,\n    if_I_kwargs={\n        \"guidance_scale\": 10.0,\n        \"sample_timestep_respacing\": \"10,10,10,10,10,0,0,0,0,0\",\n        'support_noise_less_qsample_steps': 5,\n    },\n)\nif_I.show(result['II'], 1, 20)", "from PIL import Image\n\n# Load the image\nsupport_pil_img = Image.open('image.jpg')\n\n# Define the style prompt\nstyle_prompt = 'animated cartoon'\n\n# Define the IF-I-M-v1.0 model\n# Please note, you need to have this model pretrained or loaded before this script.\nt5 = IF_I_M_Model  # Assuming that IF_I_M_Model is your pre-loaded or pretrained model\n\n# Apply style transfer\nstyle_transfer(\n    t5,\n    if_I='yes',\n    if_II=???  # We need more context to fill this argument\n    support_pil_img=support_pil_img,\n    style_prompt=style_prompt,\n    progress=True,\n)", "import PIL.Image\n\n# Load the IF-I-M model\nfrom models import IFIMv1\n\nt5 = IFIMv1()\n\n# Load the image\nimg = PIL.Image.open('image.jpg')\n\n# Define the style prompt\nstyle_prompt = 'animated cartoon'\n\n# Define the seeds\nseeds = [42]\n\n# Perform the style transfer\nresult = style_transfer(\n    t5,\n    if_I=True,\n    if_II=True,\n    if_III=True,\n    support_pil_img=img,\n    style_prompt=style_prompt,\n    seed=seeds,\n    if_I_kwargs={\"num_steps\": 500, \"save_every\": 50},\n    if_II_kwargs={\"num_steps\": 500, \"save_every\": 50},\n    if_III_kwargs={\"num_steps\": 1000, \"save_every\": 50},\n    progress=True,\n    return_tensors=False,\n    disable_watermark=False\n)", "from deepfloyd_if.modules import IFStageI, IFStageII\nfrom deepfloyd_if.modules.t5 import T5Embedder\nfrom deepfloyd_if.pipelines import style_transfer\nfrom PIL import Image\n\n# Load the image\nraw_pil_image = Image.open('image.jpg')\n\n# Initialize the modules\ndevice = 'cuda:0'\nif_I = IFStageI('IF-I-M-v1.0', device=device)\nt5 = T5Embedder(device=\"cpu\")\n\nresult = style_transfer(\n    t5=t5,\n    if_I=if_I,\n    support_pil_img=raw_pil_image,\n    style_prompt=[\n        'in style of classic animated cartoon',\n    ],\n    if_I_kwargs={\n        \"guidance_scale\": 10.0,\n        \"sample_timestep_respacing\": \"10,10,10,10,10,0,0,0,0,0\",\n        'support_noise_less_qsample_steps': 5,\n    }\n)\n\n# Save the result\nresult['I'][0].save('image_cartoon_style.jpg')"]}
{"id": 105, "github_id": 4, "output": ["\n```python\nfrom deepfloyd_if.modules import IFStageI\nfrom deepfloyd_if.modules.t5 import T5Embedder\nfrom deepfloyd_if.pipelines import style_transfer\n\n# Loading the models into VRAM\ndevice = 'cuda:0'\nif_I = IFStageI('IF-I-XL-v1.0', device=device)\nt5 = T5Embedder(device=\"cpu\")\n\n# Loading the image\nfrom PIL import Image\nraw_pil_image = Image.open('image.jpg')\n\n# Starting the style transfer\nresult = style_transfer(\n    t5=t5, if_I=if_I,\n    support_pil_img = raw_pil_image,\n    style_prompt = ['in the style of an ink wash painting'],\n    seed=42,\n    if_I_kwargs = {\n        \"guidance_scale\": 10.0,\n        \"sample_timestep_respacing\": \"10,10,10,10,10,0,0,0,0,0\",\n        'support_noise_less_qsample_steps': 5,\n    }\n)\n\n# Displaying the result\nif_I.show(result['I'], 1, 20)\n```\n", "from deepfloyd_if.modules import IFStageI\nfrom deepfloyd_if.modules.t5 import T5Embedder\nfrom deepfloyd_if.pipelines import style_transfer\nfrom PIL import Image\n\n# Define the location of your image\nimage_location = 'image.jpg'\n\n# Load the image\nraw_pil_image = Image.open(image_location)\n\n# Load the models\nif_I = IFStageI('IF-I-XL-v1.0', device='cuda:0')\nif_II = IFStageII('IF-II-L-v1.0', device='cuda:0')\nt5 = T5Embedder(device=\"cpu\")\n\n# Define the style prompt\nstyle_prompt = ['in the style of ink wash painting']\n\n# Perform the style transfer\nresult = style_transfer(\n    t5=t5, if_I=if_I, if_II=if_II,\n    support_pil_img=raw_pil_image,\n    style_prompt=style_prompt,\n    if_I_kwargs={\n        \"guidance_scale\": 10.0,\n        \"sample_timestep_respacing\": \"10,10,10,10,10,0,0,0,0,0\",\n        'support_noise_less_qsample_steps': 5,\n    },\n    if_II_kwargs={\n        \"guidance_scale\": 4.0,\n        \"sample_timestep_respacing\": 'smart50',\n        \"support_noise_less_qsample_steps\": 5,\n    },\n)\n\n# Display the final result\nif_I.show(result['II'], 1, 20)", "from deepfloyd_if.modules import IFStageI\nfrom deepfloyd_if.modules.t5 import T5Embedder\nfrom deepfloyd_if.pipelines import style_transfer\n\n# Loading the models into VRAM\ndevice = 'cuda:0'\nif_I = IFStageI('IF-I-XL-v1.0', device=device)\nt5 = T5Embedder(device=\"cpu\")\n\n# Read the image file\nfrom PIL import Image\nraw_pil_image = Image.open('image.jpg')\n\n# Style Transfer\nresult = style_transfer(\n    t5=t5, if_I=if_I, \n    support_pil_img=raw_pil_image, \n    style_prompt=['in style of ink wash'],\n    seed=42,\n    if_I_kwargs={\n        \"guidance_scale\": 10.0,\n        \"sample_timestep_respacing\": \"10,10,10,10,10,10,10,10,0,0\",\n        'support_noise_less_qsample_steps': 5,\n    }\n)\nif_I.show(result['I'], 2, 3)", "from deepfloyd_if.modules import IFStageI\nfrom deepfloyd_if.modules.t5 import T5Embedder\nfrom deepfloyd_if.pipelines import style_transfer\n\n# Load the models into VRAM\ndevice = 'cuda:0'\nif_I = IFStageI('IF-I-XL-v1.0', device=device)\nt5 = T5Embedder(device=\"cpu\")\n\n# Specify the support image and the style prompt\nsupport_image = 'path_to_your_image.jpg' # Add your image path\nstyle_prompt = ['in style of ink wash']\n\n# Call the style_transfer function to apply the transformation\nresult = style_transfer(\n    t5=t5, if_I=if_I,\n    support_pil_img=support_image,\n    style_prompt=style_prompt,\n    seed=42,\n    if_I_kwargs={\n        \"guidance_scale\": 10.0,\n        \"sample_timestep_respacing\": \"10,10,10,10,10,0,0,0,0,0\",\n        'support_noise_less_qsample_steps': 5,\n    }\n)\n\n# Save the output image\nif_I.show(result['I'], 2, 6)", "from deepfloyd_if.modules import IFStageI, IFStageII, StableStageIII, T5Embedder\nfrom deepfloyd_if.pipelines import style_transfer\n\n# Allocating device for computation\ndevice = 'cuda:0'\n\n# Loading necessary models\nif_I = IFStageI('IF-I-XL-v1.0', device=device)\nt5 = T5Embedder(device='cpu')\n\n# Defining base image and transformation style\nraw_pil_image = 'your/image/path/image.jpg'\nstyle_prompt = 'in the style of ink wash'\n\n# Implementing style transfer\nresult = style_transfer(\n    t5=t5, if_I=if_I,\n    support_pil_img=raw_pil_image,\n    style_prompt=[style_prompt],\n    seed=42,\n    if_I_kwargs={\n        'guidance_scale': 10.0,\n        'sample_timestep_respacing': '10,10,10,10,10,10,10,10,0,0',\n        'support_noise_less_qsample_steps': 5,\n    },\n)\n\n# Display transformed image\nif_I.show(result['I'], 1, 20)"]}
{"id": 109, "github_id": 4, "output": ["from deepfloyd_if.modules import IFStageI\nimport os\nfrom torchvision.io import read_image\nfrom torchvision.transforms import ToPILImage\n\n# Initialize the \uff29\ud835\udd23\ud835\udd30\ud835\udd20\ud835\udd2a\ud835\udd22\ud835\udd1f\ud835\udd21\ud835\udd22\ud835\udd2fI Module on the GPU\nif_I = IFStageI('IF-I-L-v1.0', device='cuda:0')\n# Load image from desktop\nimg_path = os.getenv('HOME') + '/Desktop/example.jpg'\nimg_tensor = read_image(img_path)\n\n# Transform the tensor to a PIL Image\nimg_pil = ToPILImage()(img_tensor)\n\n# Style Transfer\nsupport_input = if_I.transfer_support_input(img_pil)\nprompt = ['in style of an ink wash painting']\n\nresult = if_I(support_input=support_input,prompt=prompt)\n# Save the result\ncode = 'result[0][2][0].show()'", "from deepfloyd_if.modules import IFStageI\nfrom deepfloyd_if.pipelines import style_transfer\nfrom PIL import Image\n\ndevice = 'cuda:0'\nif_I = IFStageI('IF-I-M-v1.0', device=device)\n\nt5 = T5Embedder(device=\"cpu\")\n\n# Load your image\nimage_path = 'example.jpg'\nraw_pil_image = Image.open(image_path)\n\nresult = style_transfer(\n    t5=t5, if_I=if_I,\n    support_pil_img=raw_pil_image,\n    style_prompt=[\n        'in style of ink wash painting',\n    ],\n    seed=42,\n    if_I_kwargs={\n        \"guidance_scale\": 10.0,\n        \"sample_timestep_respacing\": \"10,10,10,10,10,0,0,0,0,0\",\n        'support_noise_less_qsample_steps': 5,\n    },\n)\n\nif_I.show(result['I'], 1, 20)", "from deepfloyd_if.modules import IFStageI\nfrom deepfloyd_if.modules.t5 import T5Embedder\nfrom deepfloyd_if.pipelines import style_transfer\nfrom PIL import Image\n\n# Load the model\nmodel = IFStageI('IF-I-M-v1.0', device='cuda:0')\nembedder = T5Embedder(device='cuda:0')\n\n# Load the image\ndigital_image = Image.open('example.jpg')\n\n# Specify the style transformation\ntarget_style = 'in style of ink wash painting'\n\n# Perform the transformation\nresult = style_transfer(\n    t5=embedder, if_I=model,\n    support_pil_img=digital_image,\n    style_prompt=[target_style],\n    seed=42,\n    if_I_kwargs={\n        'guidance_scale': 10.0,\n        'sample_timestep_respacing': '10,10,10,10,10,0,0,0,0,0',\n        'support_noise_less_qsample_steps': 5,\n    }\n)\n\n# Save the resulting image\nresult['I'][0].save('ink_wash_masterpiece.jpg')\n", "import PIL.Image\nfrom datetime import datetime\n\n# Load your image\nimage = PIL.Image.open('example.jpg')\n\n# Define style prompt as 'ink wash'\nstyle_prompt = ['ink wash']\n\n# Specify the IF model version\nif_I = 'IF-I-M-v1.0'\n\n# Use the style_transfer function\ndata_generating = style_transfer(\n    t5,\n    if_I,\n    if_II,\n    if_III=None,\n    support_pil_img=image,\n    style_prompt=style_prompt,\n    prompt=None,\n    negative_prompt=None,\n    seed=None,\n    if_I_kwargs=None,\n    if_II_kwargs=None,\n    if_III_kwargs=None,\n    progress=True,\n    return_tensors=False,\n    disable_watermark=False,\n)\n\n# Now 'data_generating' contains your styled image\nprint('Your styled image is ready.')", "import torch\nfrom torchvision.transforms import functional as TF\nfrom PIL import Image\n\n# Load the IF model\nmodel = torch.hub.load('github/IF', 'IF-I-M-v1.0')\n\n# Open the image\nimage = Image.open('example.jpg')\n\n# Perform the style transfer\nstyled_image = model.style_transfer(\n    image, 'ink_wash',\n    support_pil_img=True,\n    progress=True,\n    disable_watermark=True\n)\n\n# Save the output image\nstyled_image.save('output.jpg')"]}
{"id": 113, "github_id": 4, "output": ["from deepfloyd_if.pipelines import super_resolution\nfrom deepfloyd_if.modules import IFStageI, IFStageII, StableStageIII\nfrom deepfloyd_if.modules.t5 import T5Embedder\nfrom PIL import Image\n\n# Load the image\nraw_pil_image = Image.open('example.jpg')\n\n# Load the models into VRAM\ndevice = 'cuda:0'\nif_I = IFStageI('IF-I-L-v1.0', device=device)\nif_II = IFStageII('IF-II-L-v1.0', device=device)\nif_III = StableStageIII('stable-diffusion-x4-upscaler', device=device)\nt5 = T5Embedder(device=\"cpu\")\n\n# Middle Resolution\nmiddle_res = super_resolution(\n    t5,\n    if_III=if_II,\n    prompt=[''],\n    support_pil_img=raw_pil_image,\n    img_scale=4.,\n    img_size=64,\n    if_III_kwargs={\n        'sample_timestep_respacing': 'smart100',\n        'aug_level': 0.5,\n        'guidance_scale': 6.0,\n    },\n)\n\n# High Resolution\nhigh_res = super_resolution(\n    t5,\n    if_III=if_III,\n    prompt=[''],\n    support_pil_img=middle_res['III'][0],\n    img_scale=4.,\n    img_size=256,\n    if_III_kwargs={\n        'guidance_scale': 7.0,\n        'noise_level': 30,\n        'sample_timestep_respacing': '75',\n    },\n)", "from deepfloyd_if.modules import IFStageII, StableStageIII\nfrom deepfloyd_if.modules.t5 import T5Embedder\nfrom deepfloyd_if.pipelines import super_resolution\nfrom PIL import Image\n\ndevice = 'cuda:0'\nif_II = IFStageII('IF-II-L-v1.0', device=device)\nif_III = StableStageIII('stable-diffusion-x4-upscaler', device=device)\nt5 = T5Embedder(device=\"cpu\")\n\n# Load the image\nraw_pil_image = Image.open('example.jpg')\n\n# Perform Super Resolution\nresult = super_resolution(\n    t5=t5, if_II=if_II, if_III=if_III,\n    prompt=['dog on a desk, detailed picture, 4k dslr, best quality'],\n    support_pil_img=raw_pil_image,\n    img_scale=4.,\n    img_size=256,\n    if_II_kwargs={\n        \"guidance_scale\": 7.0,\n        \"sample_timestep_respacing\": '100',\n    },\n    if_III_kwargs={\n        \"guidance_scale\": 7.0,\n        \"noise_level\": 30,\n        \"sample_timestep_respacing\": \"75\",\n    },\n)\n\n# Save the result image\nresult['III'][0].save(\"super_resolved_image.jpg\")", "import torch\nfrom deepfloyd_if.modules import IFStageI, IFStageII, StableStageIII\nfrom deepfloyd_if.modules.t5 import T5Embedder\nfrom deepfloyd_if.pipelines import super_resolution\nfrom PIL import Image\n\n# Load the models into CPU\nif_I = IFStageI('IF-I-XL-v1.0', device='cuda:0')\nif_II = IFStageII('IF-II-L-v1.0', device='cuda:0')\nif_III = StableStageIII('stable-diffusion-x4-upscaler', device='cuda:0')\nt5 = T5Embedder(device=\"cpu\")\n\n# Load the image\ndog_image = Image.open('example.jpg')\n\n# Super Resolution\nmiddle_res = super_resolution(\n    t5,\n    if_III=if_II,\n    prompt=['Dog on a Desk'],\n    support_pil_img=dog_image,\n    img_scale=4.,\n    img_size=64,\n    if_III_kwargs={\n        'sample_timestep_respacing': 'smart100',\n        'aug_level': 0.5,\n        'guidance_scale': 7.0,\n    },\n)\n\nhigh_res = super_resolution(\n    t5,\n    if_III=if_III,\n    prompt=[''],\n    support_pil_img=middle_res['III'][0],\n    img_scale=4.,\n    img_size=256,\n    if_III_kwargs={\n        'guidance_scale': 9.0,\n        'noise_level': 30,\n        'sample_timestep_respacing': '75',\n    },\n)\n\n# Saving the result\nhigh_res['III'][0].save('./super_res_dog_image.jpg')", "from deepfloyd_if.modules import IFStageI, IFStageII, StableStageIII\nfrom deepfloyd_if.modules.t5 import T5Embedder\nfrom deepfloyd_if.pipelines import super_resolution\nfrom PIL import Image\n\ndevice = 'cuda:0'\nif_I = IFStageI('IF-I-L-v1.0', device=device)\nif_II = IFStageII('IF-II-L-v1.0', device=device)\nif_III = StableStageIII('stable-diffusion-x4-upscaler', device=device)\nt5 = T5Embedder(device=\"cpu\")\n\nraw_pil_image = Image.open('example.jpg')\n\nmiddle_res = super_resolution(\n    t5,\n    if_III=if_II,\n    prompt=['dog on a desk, detailed picture, 4k dslr, best quality'],\n    support_pil_img=raw_pil_image,\n    img_scale=4.,\n    img_size=64,\n    if_III_kwargs={\n        'sample_timestep_respacing': 'smart100',\n        'aug_level': 0.5,\n        'guidance_scale': 7.0,\n    },\n)\n\nhigh_res = super_resolution(\n    t5,\n    if_III=if_III,\n    prompt=[''],\n    support_pil_img=middle_res['III'][0],\n    img_scale=4.,\n    img_size=256,\n    if_III_kwargs={\n        \"guidance_scale\": 7.0,\n        \"noise_level\": 30,\n        \"sample_timestep_respacing\": \"75\",\n    },\n)\n\nhigh_res['III'][0].save('super_res_image.jpg')", "from deepfloyd_if.modules import IFStageI, IFStageII, StableStageIII\nfrom deepfloyd_if.modules.t5 import T5Embedder\nfrom deepfloyd_if.pipelines import super_resolution\nfrom PIL import Image\n\ndevice = 'cuda:0'\nraw_pil_image = Image.open('example.jpg')\n\nif_I = IFStageI('IF-I-XL-v1.0', device=device)\nif_II = IFStageII('IF-II-L-v1.0', device=device)\nif_III = StableStageIII('stable-diffusion-x4-upscaler', device=device)\nt5 = T5Embedder(device=\"cpu\")\n\nmiddle_res = super_resolution(\n    t5,\n    if_III=if_II,\n    prompt=['detail enhancement of the picture'],\n    support_pil_img=raw_pil_image,\n    img_scale=4.,\n    img_size=64,\n    if_III_kwargs={\n        'sample_timestep_respacing': 'smart100',\n        'aug_level': 0.5,\n        'guidance_scale': 6.0,\n    },\n)\nhigh_res = super_resolution(\n    t5,\n    if_III=if_III,\n    prompt=[''],\n    support_pil_img=middle_res['III'][0],\n    img_scale=4.,\n    img_size=256,\n    if_III_kwargs={\n        \"guidance_scale\": 7.0,\n        \"noise_level\": 30,\n        \"sample_timestep_respacing\": \"75\",\n    },\n)\n\n# To save the super resolution image\nhigh_res['III'][0].save('./super_resolution_image.png')"]}
{"id": 117, "github_id": 4, "output": ["from deepfloyd_if.modules import IFStageII\nfrom deepfloyd_if.pipelines import super_resolution\nfrom PIL import Image\n\nt5 = T5Embedder(device=\"cpu\")\nif_II = IFStageII('IF-II-M-v1.0', device='cuda:0')\n\nraw_pil_image = Image.open('example.jpg')\n\nresult = super_resolution(\n    t5,\n    if_II=if_II,\n    prompt=['woman with a blue headscarf and a blue sweater'],\n    support_pil_img=raw_pil_image,\n    img_scale=4.,\n    img_size=64,\n    if_II_kwargs={\n        'sample_timestep_respacing': 'smart100',\n        'aug_level': 0.5,\n        'guidance_scale': 7.0,\n    },\n)\n\nif_II.show(result['II'], 1, 20)", "\nfrom deepfloyd_if.modules import IFStageII\nfrom deepfloyd_if.pipelines import super_resolution\nfrom PIL import Image\n\n# Load the models into VRAM\ndevice = 'cuda:0'\nif_II = IFStageII('IF-II-M-v1.0', device=device)\n\n# Load the image\nraw_pil_image = Image.open(\"example.jpg\")\n\n# Use super-resolution\nmiddle_res = super_resolution(\n    if_III=if_II,\n    prompt=[\"a woman in a blue headscarf and a blue sweater\"],\n    support_pil_img=raw_pil_image,\n    img_scale=4.,\n    img_size=64,\n    if_III_kwargs={\n        'sample_timestep_respacing': 'smart100',\n        'aug_level': 0.5,\n        'guidance_scale': 7.0,\n    },\n)\n\n# Display the result\nmiddle_res['III'][0].show()\n", "from deepfloyd_if.modules import IFStageII\nfrom deepfloyd_if.pipelines import super_resolution\nfrom PIL import Image\n\nt5 = T5Embedder(device=\"cpu\")\nif_II = IFStageII('IF-II-L-v1.0', device=device)\n\n# Load your image\nraw_pil_image = Image.open('example.jpg')\n\n# Define your prompt\nprompt = ['woman with a blue headscarf and a blue sweater, detailed picture, 4k dslr, best quality']\n\n# Implement super resolution\nmiddle_res = super_resolution(\n    t5,\n    if_III=if_II,  # Here we are loading IF_II as if it's if_III because the super_resolution function expects if_III as an argument\n    prompt=prompt,\n    support_pil_img=raw_pil_image,\n    img_scale=4.,\n    img_size=64,\n    if_III_kwargs={\n        'sample_timestep_respacing': 'smart100',\n        'aug_level': 0.5,\n        'guidance_scale': 7.0,\n    },\n)", "from deepfloyd_if.pipelines import super_resolution\nfrom deepfloyd_if.modules import IFStageII\nfrom deepfloyd_if.modules.t5 import T5Embedder\nfrom PIL import Image\n\nt5 = T5Embedder(device=\"cpu\")\nif_II = IFStageII('IF-II-M-v1.0', device='cuda:0')\n\nraw_pil_image = Image.open('example.jpg') # Load your image\n\nresult = super_resolution(\n    t5,\n    if_III=if_II,\n    prompt=['woman with a blue headscarf and a blue sweater, detailed picture, 4k dslr, best quality'],\n    support_pil_img=raw_pil_image,\n    img_scale=4.,\n    img_size=64,\n    if_III_kwargs={\n        'sample_timestep_respacing': 'smart100',\n        'aug_level': 0.5,\n        'guidance_scale': 7.0,\n    },\n)\n\nnew_image = result['III'][0] # This is your new high-resolution image\nnew_image.save('high_res_example.jpg') # Save the new image\n", "from deepfloyd_if.modules import IFStageII\nfrom deepfloyd_if.pipelines import super_resolution\nfrom PIL import Image\n\nt5 = T5Embedder(device=\"cpu\")\n\n# Load the image\nraw_pil_image = Image.open('example.jpg')\n\n# Load the model\nif_II = IFStageII('IF-II-M-v1.0', device='cuda:0')\n\n# Use Super Resolution\nresult = super_resolution(\n    t5,\n    if_III=if_II,\n    prompt=['woman with a blue headscarf and a blue sweater'],\n    support_pil_img=raw_pil_image,\n    img_scale=4.,\n    img_size=64,\n    if_III_kwargs={\n        'sample_timestep_respacing': 'smart100',\n        'aug_level': 0.5,\n        'guidance_scale': 7.0,\n    }\n)\nresult['III'][0].save('example_super_res.jpg')"]}
{"id": 121, "github_id": 5, "output": ["python train.py --name edge2face_512 --dataroot datasets/face/ --dataset_mode face --input_nc 15 --loadSize 512 --num_D 3 --gpu_ids 0,1,2,3,4,5,6,7 --n_gpus_gen 6 --n_frames_total 12", "python train.py --name edge2face_512 --dataroot datasets/face/ --dataset_mode face --input_nc 15 --loadSize 512 --num_D 3 --gpu_ids 0,1,2,3,4,5,6,7 --n_gpus_gen 6 --n_frames_total 12", "python train.py --name edge2face_512 --dataroot datasets/face/ --dataset_mode face --input_nc 15 --loadSize 512 --num_D 3 --gpu_ids 0,1,2,3,4,5,6,7 --n_gpus_gen 6 --n_frames_total 12", "python train.py --name edge2face_512 --dataroot datasets/face/ --dataset_mode face --input_nc 15 --loadSize 512 --num_D 3 --gpu_ids 0,1,2,3,4,5,6,7 --n_gpus_gen 6 --n_frames_total 12", "python train.py --name edge2face_512 --dataroot datasets/face/ --dataset_mode face --input_nc 15 --loadSize 512 --num_D 3 --gpu_ids 0,1,2,3,4,5,6,7 --n_gpus_gen 6 --n_frames_total 12"]}
{"id": 125, "github_id": 5, "output": ["python test.py --name pose2body_256p --dataroot datasets/pose --dataset_mode pose --input_nc 6 --loadSize 256 --use_instance --fg --n_downsample_G 2 --num_D 1 --max_frames_per_gpu 6 --n_frames_total 6 --no_first_img", "python test.py --name pose2body_256p --dataroot datasets/pose --dataset_mode pose --input_nc 6 --resize_or_crop ScaleHeight_and_scaledCrop --loadSize 384 --fineSize 256 --gpu_ids 0 --max_frames_per_gpu 3 --n_frames_total 12 --max_t_step 4", "python test.py --name pose2body_256p --dataroot datasets/pose --dataset_mode pose --input_nc 6 --num_D 2 --resize_or_crop ScaleHeight_and_scaledCrop --loadSize 384 --fineSize 256 --gpu_ids 0 --batchSize 1 --max_frames_per_gpu 1 --no_first_img --n_frames_total 12 --max_t_step 4", "python test.py --name pose2body_256p --dataroot datasets/pose --dataset_mode pose --input_nc 6 --num_D 2 --resize_or_crop ScaleHeight_and_scaledCrop --loadSize 384 --fineSize 256 --gpu_ids 0 --batchSize 1 --max_frames_per_gpu 3 --no_first_img --n_frames_total 12 --max_t_step 4", "python test.py --name pose2body_256p --dataroot datasets/pose/ --dataset_mode pose --input_nc 6 --loadSize 256 --gpu_ids 0 --max_frames_per_gpu 3 --n_frames_total 12 --max_t_step 4"]}
{"id": 129, "github_id": 5, "output": ["python train.py --name edge2face_256 --dataroot datasets/face/ --dataset_mode face --input_nc 15 --loadSize 256 --num_D 3 --gpu_ids 0,1,2,3,4,5,6,7 --n_gpus_gen 6 --n_frames_total 12", "python train.py --name edge2face_256 --dataroot datasets/face/ --dataset_mode face --input_nc 15 --loadSize 256 --num_D 3 --gpu_ids 0,1,2,3,4,5,6,7 --n_gpus_gen 6 --n_frames_total 12", "python train.py --name edge2face_256 --dataroot datasets/face/ --dataset_mode face --input_nc 15 --loadSize 256 --num_D 3 --gpu_ids 0,1,2,3,4,5,6,7 --n_gpus_gen 6 --n_frames_total 12 ", "python train.py --name edge2face_256 --dataroot datasets/face/ --dataset_mode face --input_nc 15 --loadSize 256 --num_D 3 --gpu_ids 0,1,2,3,4,5,6,7 --n_gpus_gen 6 --n_frames_total 12", "\npython data/face_landmark_detection.py train\npython train.py --name edge2face_256 --dataroot datasets/face/ --dataset_mode face --input_nc 15 --loadSize 256 --num_D 3 --gpu_ids 0,1,2,3,4,5,6,7 --n_gpus_gen 6 --n_frames_total 12"]}
{"id": 133, "github_id": 5, "output": ["python train.py --name edge2face_256 --dataroot datasets/face/ --dataset_mode face --input_nc 15 --loadSize 256 --num_D 3 --max_frames_per_gpu 4 --use_single_G", "python train.py --name edge2face_256 --dataroot datasets/face/ --dataset_mode face --input_nc 15 --loadSize 256 --num_D 3 --gpu_ids 0,1,2,3 --n_gpus_gen 4 --n_frames_total 12 --max_frames_per_gpu 4", "python train.py --name edge2face_256 --dataroot datasets/face/ --dataset_mode face --input_nc 15 --loadSize 256 --num_D 3 --gpu_ids 0,1,2,3 --max_frames_per_gpu 4 --n_frames_total 12", "python train.py --name edge2face_256 --dataroot datasets/face/ --dataset_mode face --input_nc 15 --loadSize 256 --num_D 3 --gpu_ids 0,1,2,3 --max_frames_per_gpu 4 --n_frames_total 12", "python train.py --name edge2face_256 --dataroot datasets/face/ --dataset_mode face --input_nc 15 --loadSize 256 --num_D 3 --gpu_ids 0,1,2,3 --max_frames_per_gpu 4 --n_frames_total 12"]}
{"id": 137, "github_id": 5, "output": ["python train.py --name pose2body_1024p --dataroot datasets/pose --dataset_mode pose --input_nc 6 --num_D 2 --resize_or_crop ScaleHeight_and_scaledCrop --loadSize 384 --fineSize 256 --gpu_ids 0 --batchSize 8 --max_frames_per_gpu 3 --no_first_img --n_frames_total 12 --max_t_step 4", "python train.py --name pose2body_1024p_g1 --dataroot datasets/pose --dataset_mode pose --input_nc 6 --n_scales_spatial 3 --num_D 4 --ngf 64 --ndf 32 --resize_or_crop randomScaleHeight_and_scaledCrop --loadSize 1536 --fineSize 1024 --no_first_img --n_frames_total 12 --max_t_step 4 --add_face_disc --niter_fix_global 3 --niter 5 --niter_decay 5 --lr 0.00005 --load_pretrain checkpoints/pose2body_512p_g1", "python train.py --name pose2body_1024p --dataroot datasets/pose --dataset_mode pose --input_nc 6 --resize_or_crop ScaleHeight_and_scaledCrop --loadSize 1024 --fineSize 256 --gpu_ids 0,1,2,3,4,5,6,7 --batchSize 1 --max_frames_per_gpu 3 --no_first_img --n_frames_total 12 --max_t_step 4", "python train.py --name pose2body_1024p_g1 --dataroot datasets/pose --dataset_mode pose --input_nc 6 --n_scales_spatial 3 --num_D 4 --ngf 64 --ndf 32 --resize_or_crop randomScaleHeight_and_scaledCrop --loadSize 1536 --fineSize 1024 --no_first_img --n_frames_total 12 --max_t_step 4 --add_face_disc --niter_fix_global 3 --niter 5 --niter_decay 5 --lr 0.00005 --load_pretrain checkpoints/pose2body_512p_g1", "python train.py --name pose2body_1024p --dataroot datasets/pose --dataset_mode pose --input_nc 6 --num_D 2 --resize_or_crop ScaleHeight_and_scaledCrop --loadSize 1280 --fineSize 1024 --gpu_ids 0 --batchSize 1 --max_frames_per_gpu 3 --no_first_img -n_frames_total 12 --max_t_step 4"]}
{"id": 141, "github_id": 5, "output": ["python train.py --name pose2body_256p --dataroot datasets/pose --dataset_mode pose --input_nc 6 --num_D 2 --resize_or_crop ScaleHeight_and_scaledCrop --loadSize 384 --fineSize 256 --gpu_ids 0 --batchSize 1 --max_frames_per_gpu 2 --no_first_img --n_frames_total 12 --max_t_step 4", "python train.py --name pose2body_256p --dataroot datasets/pose --dataset_mode pose --input_nc 6 --num_D 2 --resize_or_crop ScaleHeight_and_scaledCrop --loadSize 384 --fineSize 256 --gpu_ids 0 --batchSize 1 --max_frames_per_gpu 3 --no_first_img --n_frames_total 12 --max_t_step 4", "python train.py --name pose2body_256p --dataroot datasets/pose --dataset_mode pose --input_nc 6 --num_D 2 --resize_or_crop ScaleHeight_and_scaledCrop --loadSize 384 --fineSize 256 --gpu_ids 0 --batchSize 1 --max_frames_per_gpu 3 --no_first_img --n_frames_total 12 --max_t_step 4", "python train.py --name pose2body_256p --dataroot datasets/pose --dataset_mode pose --input_nc 6 --num_D 2 --resize_or_crop ScaleHeight_and_scaledCrop --loadSize 384 --fineSize 256 --gpu_ids 0 --batchSize 1 --max_frames_per_gpu 1 --no_first_img --n_frames_total 12 --max_t_step 4", "python train.py --name pose2body_256p_g1 --dataroot datasets/pose --dataset_mode pose --input_nc 6 --num_D 2 --resize_or_crop scaleHeight --loadSize 384 --fineSize 256 --gpu_ids 0 --n_gpus_gen 1 --max_frames_per_gpu 2 --batchSize 1 --n_frames_total 6 --max_t_step 4"]}
{"id": 145, "github_id": 5, "output": ["python train.py --name label2city_512 --label_nc 35 --loadSize 512 --num_D 3 --gpu_ids 0,1,2,3,4,5,6,7 --n_gpus_gen 4 --use_instance --no_fg --niter_step 2 --niter_fix_global 10 --load_pretrain checkpoints/label2city_512", "python train.py --name label2city_512_no_fg --label_nc 35 --loadSize 512 --n_scales_spatial 3 --no_fg --use_single_G", "# Training code for 'label2city' model on Cityscapes dataset excluding foreground objects \n# and at a resolution of 512 pixels.\npython train.py --name label2city_512_no_fg --label_nc 35 --loadSize 512 --n_scales_spatial 2 --num_D 3 --gpu_ids 0,1,2,3,4,5,6,7 --n_gpus_gen 4 --use_instance --fg_label 0 --no_first_img --niter_step 2 --niter_fix_global 10 --load_pretrain checkpoints/label2city_512", "python train.py --name label2city_512_no_fg --label_nc 35 --loadSize 512 --use_instance --fg --no_fg", "python train.py --name label2city_512_no_fg --label_nc 35 --loadSize 512 --n_scales_spatial 3 --use_instance --no_fg --gpu_ids 0,1,2,3,4,5,6,7 --n_gpus_gen 6 --n_frames_total 6"]}
{"id": 149, "github_id": 5, "output": ["First, download the FlowNet2 checkpoint file by running `python scripts/download_models_flownet2.py`.\n\nNext, you'll want to train a model at 512 x 256 resolution first. You can do this by running the following command:\n\n```bash\npython train.py --name label2city_512 --label_nc 35 --gpu_ids 0,1,2,3,4,5,6,7 --n_gpus_gen 6 --n_frames_total 6 --use_instance --fg\n```\n\nAfter you've trained a model at 512 x 256 resolution, you can train a model at 1024 x 512 resolution, which is your target resolution size. Run the following command to accomplish this:\n\n```bash\npython train.py --name label2city_1024 --label_nc 35 --loadSize 1024 --n_scales_spatial 2 --num_D 3 --gpu_ids 0,1,2,3,4,5,6,7 --n_gpus_gen 4 --use_instance --fg --niter_step 2 --niter_fix_global 10 --load_pretrain checkpoints/label2city_512\n```\n\nIf you have TensorFlow installed, you can see TensorBoard logs in `./checkpoints/label2city_1024/logs` by adding `--tf_log` to the training scripts.\n\nPlease note that these commands assume you have 8 GPUs available. If you have a different number of GPUs, you'll need to adjust the `gpu_ids` and `n_gpus_gen` parameters accordingly.", "python train.py --name label2city_1024 --label_nc 35 --loadSize 1024 --n_scales_spatial 2 --num_D 3 --gpu_ids 0,1,2,3,4,5,6,7 --n_gpus_gen 4 --use_instance --fg --niter_step 2 --niter_fix_global 10 --load_pretrain checkpoints/label2city_512", "python train.py --name label2city_1024 --label_nc 35 --loadSize 1024 --n_scales_spatial 2 --num_D 3 --gpu_ids 0,1,2,3,4,5,6,7 --n_gpus_gen 4 --use_instance --fg --niter_step 2 --niter_fix_global 10 --load_pretrain checkpoints/label2city_512", "After you have cloned the vid2vid repository, follow these steps:\n\n1. Be sure you have downloaded the Cityscapes dataset. This dataset can be downloaded from the official Cityscapes website (https://www.cityscapes-dataset.com/) after registration. Once downloaded, add the images to your 'datasets' folder in the same way that the example images are provided.\n\n2. Run the following command to download the FlowNet2 checkpoint files:\n```python\npython scripts/download_models_flownet2.py\n```\n\n3. Next, use the provided script to train your model at the 1024 x 512 resolution. Be sure that you have already trained at the 512 x 256 resolution as the model\u2019s training adopts a coarse-to-fine approach:\n```bash\n#!./scripts/street/train_1024.sh\npython train.py --name label2city_1024 --label_nc 35 --loadSize 1024 --n_scales_spatial 2 --num_D 3 --gpu_ids 0,1,2,3,4,5,6,7 --n_gpus_gen 4 --use_instance --fg --niter_step 2 --niter_fix_global 10 --load_pretrain checkpoints/label2city_512\n```\n\n4. If you have TensorFlow installed, you can see TensorBoard logs in `./checkpoints/label2city_1024/logs` by adding `--tf_log` to the training scripts.\n\nAdditional note: If you want to train the images at full resolution (2k x 1k), this requires 8 GPUs with at least 24GB memory. Checkout the 'train_2048.sh' script.\n", "Git clone the repository -\n\n```\ngit clone https://github.com/NVIDIA/vid2vid\n```\n\nGo into the project directory - \n\n```\ncd vid2vid\n```\n\nNext, we are going to install the necessary dependencies:\n\nDominate - `pip install dominate`\n\nRequests - `pip install requests`\n\nInstall dlib for face datasets - `pip install dlib`\n\nNext, download the FlowNet2 checkpoint file by running:\n\n```\npython scripts/download_models_flownet2.py\n```\n\nNow, download the pre-trained Cityscapes model by running:\n\n```\npython scripts/street/download_models.py\n```\n\nNext, for the training phase at a 1024 x 512 resolution (must train 512 x 256 first) (`bash ./scripts/street/train_1024.sh`) :\n\n```\n#./scripts/street/train_1024.sh\n\npython train.py --name label2city_1024 --label_nc 35 --loadSize 1024 --n_scales_spatial 2 --num_D 3 --gpu_ids 0,1,2,3,4,5,6,7 --n_gpus_gen 4 --use_instance --fg --niter_step 2 --niter_fix_global 10 --load_pretrain checkpoints/label2city_512\n```\n\nOnce your TensorFlow is installed, you can see TensorBoard logs in `./checkpoints/label2city_1024/logs` by adding `--tf_log` to the training scripts."]}
{"id": 151, "github_id": 6, "output": ["import torch\nimport esm\n\nmodel = esm.pretrained.esmfold_v1()\nmodel = model.eval().cuda()\n\nsequence = \"MKTVRQERLKSIVRILERSKEPV\"\n\nwith torch.no_grad():\n    output = model.infer_pdb(sequence)\n\nwith open(\"result.pdb\", \"w\") as f:\n    f.write(output)\n\nimport biotite.structure.io as bsio\nstruct = bsio.load_structure(\"result.pdb\", extra_fields=[\"b_factor\"])\nprint(struct.b_factor.mean())", "import torch\nimport esm\n\n# Load ESMFold model\nmodel = esm.pretrained.esmfold_v1()\nmodel = model.eval().cuda()\n\n# Sequence for structure prediction\ndata = 'MKTVRQERLKSIVRILERSKEPV'\n\n# Predict structure\nwith torch.no_grad():\n    results = model.predict(data)\n\n# Save results\ntorch.save(results, 'predicted_structure.pt')", "import torch\nimport esm\n\n# Load ESMFold model\nmodel = esm.pretrained.esmfold_v1()\nmodel = model.eval().cuda()\n\n# Prepare your protein sequence\nsequence = \"MKTVRQERLKSIVRILERSKEPV\"\n\n# Use the ESMFold model to predict the structure\nwith torch.no_grad():\n    output = model.infer_pdb(sequence)\n\n# Save the output in a PDB file\nwith open(\"result.pdb\", \"w\") as f:\n    f.write(output)", "import esm\nimport torch\n\nmodel = esm.pretrained.esmfold_v1()\nmodel = model.eval().cuda()\n\nsequence = \"MKTVRQERLKSIVRILERSKEPV\"\n\nwith torch.no_grad():\n    output = model.infer_pdb(sequence)\n\nwith open(\"result.pdb\", \"w\") as f:\n    f.write(output)", "import torch\nimport esm\n\n# Load ESMFold model\nmodel = esm.pretrained.esmfold_v1()\nmodel = model.eval().cuda()\n\n# sequence to predict structure for\nsequence = \"MKTVRQERLKSIVRILERSKEPV\"\n\nwith torch.no_grad():\n    output = model.infer_pdb(sequence)\n\nwith open(\"result.pdb\", \"w\") as f:\n    f.write(output)"]}
{"id": 156, "github_id": 6, "output": ["import torch\nimport esm\n\n# Load ESM-2 model\nmodel, alphabet = esm.pretrained.esm2_t48_15B_UR50D()\nbatch_converter = alphabet.get_batch_converter()\nmodel.eval()  # disables dropout for deterministic results\n\n# Prepare data\nfasta_file = \"rna.fasta\"\ndata = []\nwith open(fasta_file, 'r') as file:\n    for line in file:\n        if line.startswith(\">\"):\n            continue\n        data.append((\"fasta_seq\", line.strip()))\n\nbatch_labels, batch_strs, batch_tokens = batch_converter(data)\nbatch_lens = (batch_tokens != alphabet.padding_idx).sum(1)\n\n# Extract per-residue representations (on CPU)\nwith torch.no_grad():\n    results = model(batch_tokens, repr_layers=[59], return_contacts=False)\n\ntoken_representations = results[\"representations\"][59]\n\n# Save tensor to file\ntorch.save(token_representations, 'dna_36_embeddings.emb')", "import torch\nimport esm\n\n# Load ESM-2 model\nmodel, alphabet = esm.pretrained.esm2_t48_15B_UR50D()\nbatch_converter = alphabet.get_batch_converter()\nmodel.eval()  # disables dropout for deterministic results\n\n# Prepare data (rna.fasta)\ndata = [('rna', 'sequence in fasta file')]\nbatch_labels, batch_strs, batch_tokens = batch_converter(data)\nbatch_lens = (batch_tokens != alphabet.padding_idx).sum(1)\n\n# Extract per-residue representations (on CPU)\nwith torch.no_grad():\n    results = model(batch_tokens, repr_layers=[60])\ntoken_representations = results['representations'][60]\n\n# Save embeddings to file\ntorch.save(token_representations, 'dna_36_embeddings.emb')", "import torch\nimport esm\n\n# Load ESM-2 model\nmodel, alphabet = esm.pretrained.esm2_t48_15B_UR50D()\nbatch_converter = alphabet.get_batch_converter()\nmodel.eval()  # disables dropout for deterministic results\n\nrna_file = open('rna.fasta', 'r')\nrna_seq = rna_file.read()\n\n# Prepare data (assumes the sequence is a protein sequence)\ndata = [\n    (\"rna1\", rna_seq),\n]\n\nbatch_labels, batch_strs, batch_tokens = batch_converter(data)\n\n# Extract per-residue representations (on CPU)\nwith torch.no_grad():\n    results = model(batch_tokens, repr_layers=[0, 33, 60])\n\n# Generate per-sequence representations via averaging\n# NOTE: token 0 is always a beginning-of-sequence token, so the first residue is token 1.\nsequence_representation = results['representations'][60][0, 1:-1].mean(0)\n\n# Save embeddings to a file\nwith open('dna_36_embeddings.emb', 'w') as f:\n    f.write(str(sequence_representation.tolist()))", "import torch\nimport esm\n\n# Load ESM-2 model\nmodel, alphabet = esm.pretrained.esm2_t48_15B_UR50D()\nbatch_converter = alphabet.get_batch_converter()\nmodel.eval()  # disables dropout for deterministic results\n\n# Prepare data for rna.fasta\nwith open(\"rna.fasta\") as f:\n    lines = f.readlines()\n    data = []\n    sequence = ''\n    for line in lines:\n        if line.startswith('>'):\n            if sequence:\n                data.append((sequence_name, sequence))\n            sequence_name = line[1:].strip()\n            sequence = ''\n        else:\n            sequence += line.strip()\n    if sequence:\n        # if file didn't end with '>', add last read sequence.\n        data.append((sequence_name, sequence))\n\nbatch_labels, batch_strs, batch_tokens = batch_converter(data)\nbatch_lens = (batch_tokens != alphabet.padding_idx).sum(1)\n\n# Extract per-residue representations (on CPU)\nwith torch.no_grad():\n    results = model(batch_tokens, repr_layers=[60])\n    torch.save(results['representations'][60], 'dna_36_embeddings.emb')", "import torch\nimport esm\n\n# Load ESM-2 model\nmodel, alphabet = esm.pretrained.esm2_t48_15B_UR50D()\nbatch_converter = alphabet.get_batch_converter()\nmodel.eval()\n\n# Load data from FASTA file\ndata = []\nwith open('rna.fasta', 'r') as file:\n    for line in file:\n        if line.startswith('>'):\n            name = line[1:].strip()\n        else:\n            seq = line.strip()\n            data.append((name, seq))\n\n# Convert batch data into tensors\nbatch_labels, batch_strs, batch_tokens = batch_converter(data)\n\n# Use model to extract per-residue representations\nwith torch.no_grad():\n    results = model(batch_tokens, repr_layers=[60])\n    representations = results['representations'][60]\n\n# Save the resulting embedding to a file\ntorch.save(representations, 'dna_36_embeddings.emb')"]}
{"id": 168, "github_id": 6, "output": ["import esm.inverse_folding\n\n# Load model and alphabet\nmodel, alphabet = esm.pretrained.esm_if1_gvp4_t16_142M_UR50()\nmodel = model.eval()\n\n# Load structure\nstructure = esm.inverse_folding.util.load_structure('Fam20A.pdb', 'B')\n\n# Extract coordinates\ncoords, seq = esm.inverse_folding.util.extract_coords_from_structure(structure)\n\n# Sample sequences\ndesigns = [model.sample(coords, temperature=8) for _ in range(5)]\n\n# Print designs\nfor i, design in enumerate(designs):\n    print(f\"Design {i + 1}: {design}\")", "import esm.inverse_folding\n\n# Load the pretrained model and its corresponding alphabet\nmodel, alphabet = esm.pretrained.esm_if1_gvp4_t16_142M_UR50()\nmodel = model.eval()\n\n# Define the pdb file path and chain\nfpath = 'Fam20A.pdb'\nchain_id = 'B' \n\n# Load a single chain from PDB file and extract the backbone coordinates of the N, CA, C atoms as model input\nstructure = esm.inverse_folding.util.load_structure(fpath, chain_id)\ncoords, seq = esm.inverse_folding.util.extract_coords_from_structure(structure)\n\n# Sample 5 sequences for a given set of backbone coordinates for chain B, at a temperature of 8\nsampled_sequences = []\nfor _ in range(5):\n    sampled_seq = model.sample(coords, temperature=8)\n    sampled_sequences.append(sampled_seq)\n    \nprint(sampled_sequences)", "import esm.inverse_folding\n\n# Load model\nmodel, alphabet = esm.pretrained.esm_if1_gvp4_t16_142M_UR50()\nmodel = model.eval()\n\n# Load structure\nfpath = 'Fam20A.pdb' # The path to the pdb file\nchain_id = 'B' # The chain to extract from\nstructure = esm.inverse_folding.util.load_structure(fpath, chain_id)\n\n# Extract backbone coordinates and sequence\ncoords, seq = esm.inverse_folding.util.extract_coords_from_structure(structure)\n\n# Design sequences\nnum_samples = 5 # Number of sequences to design\ntemperature = 8 # Temperature for designs\nsampled_seqs = model.sample(coords, temperature=temperature, num_samples=num_samples)\n\n# Print sampled sequences\nfor i, sampled_seq in enumerate(sampled_seqs):\n    print(f'Sampled sequence {i+1}: {sampled_seq}')", "import esm.pretrained\nimport esm.inverse_folding\n\nmodel, alphabet = esm.pretrained.esm_if1_gvp4_t16_142M_UR50()\nmodel = model.eval()\nstructure = esm.inverse_folding.util.load_structure('Fam20A.pdb', 'B')\ncoords, seq = esm.inverse_folding.util.extract_coords_from_structure(structure)\nsamples = []\n\nfor _ in range(5):\n    sampled_seq = model.sample(coords, temperature=8)\n    samples.append(sampled_seq)\n\nprint(samples)", "import esm\n\n# Load the inverse folding model and alphabet\nmodel, alphabet = esm.pretrained.esm_if1_gvp4_t16_142M_UR50()\n\n# Set the model in eval mode for accurate results\nmodel = model.eval()\n\n# Define the path to the structure file\nstruct_file_path = 'Fam20A.pdb'\n\n# Define the target chain\nchain_id = 'B'\n\n# Load the structure and extract the coordinates\nstructure = esm.inverse_folding.util.load_structure(struct_file_path, chain_id)\ncoords, _ = esm.inverse_folding.util.extract_coords_from_structure(structure)\n\n# Sample 5 sequence designs for the given structure at a temperature of 8\nsequences = [model.sample(coords, temperature=8) for _ in range(5)]\n\n# Print the sampled sequences\nfor i, seq in enumerate(sequences):\n    print(f'Sequence {i+1}: {seq}')\n"]}
{"id": 160, "github_id": 6, "output": ["import torch\nimport esm\n\n# Load ESM-2 model\nmodel, alphabet = esm.pretrained.esm2_t48_15B_UR50D()\nbatch_converter = alphabet.get_batch_converter()\nmodel.eval()  # disables dropout for deterministic results\n\n# Prepare data (reading your fasta file)\nwith open('dna_virus.fasta') as f:\n    sequences = f.read().splitlines()\n\ndata = [(f'protein{i}', seq) for i, seq in enumerate(sequences)]\n\nbatch_labels, batch_strs, batch_tokens = batch_converter(data)\n\n# Extract per-residue representations (on CPU)\nwith torch.no_grad():\n    results = model(batch_tokens, repr_layers=[42])  # only request 42nd layer\n    torch.save(results, 'output.embeddings')", "import torch\nimport esm\n\n# Load ESM-2 model\nmodel, alphabet = esm.pretrained.esm2_t48_15B_UR50D()\nbatch_converter = alphabet.get_batch_converter()\nmodel.eval()  # disables dropout for deterministic results\n\n# Prepare data\nwith open('dna_virus.fasta', 'r') as f:\n    data = [(line.strip()[1:], next(f).strip()) for line in f if line.startswith('>')]\nbatch_labels, batch_strs, batch_tokens = batch_converter(data)\n\n# Extract 42-layer embedding\nwith torch.no_grad():\n    results = model(batch_tokens, repr_layers=[42])\n    token_representations = results['representations'][42]\n\n# Save embeddings\ntorch.save(token_representations, 'output.embeddings')", "import torch\nimport esm\n# Load ESM-2 model\nmodel, alphabet = esm.pretrained.esm2_t48_15B_UR50D()\nbatch_converter = alphabet.get_batch_converter()\nmodel.eval()  # disables dropout for deterministic results\n# Prepare data\nfastafile = 'dna_virus.fasta'\nwith open(fastafile)as f:\n    sequences = {line.strip().split()[0]: line.strip().split()[1] for line in f}\ndata = [(k, v) for k, v in sequences.items()]\nbatch_labels, batch_strs, batch_tokens = batch_converter(data)\n# Extract per-residue representations (on CPU)\nwith torch.no_grad():\n    results = model(batch_tokens, repr_layers=[42])\nembeddings = results['representations'][42]\n# Save representations\ntorch.save(embeddings, 'output.embeddings')", "import torch\nimport esm\n\n# Load ESM-2 model\nmodel, alphabet = esm.pretrained.esm2_t48_15B_UR50D()\nbatch_converter = alphabet.get_batch_converter()\nmodel.eval()\n\n# Prepare data (Replace with your own FASTA file)\ndata = [\n    (\"sequence_id\", \"MKTVRQERLKSIVRILERSKEPVSGAQLAEELSVSRQVIVQDIAYLRSLGYNIVATPRGYVLAGG\"),\n]\nbatch_labels, batch_strs, batch_tokens = batch_converter(data)\n\n# Extract per-residue representations (on CPU)\nwith torch.no_grad():\n    results = model(batch_tokens, repr_layers=[42])\n\n# Save the output\ntorch.save(results, 'output.embeddings')\n", "import torch\nimport esm\n\n# Load ESM-2 model\nmodel, alphabet = esm.pretrained.esm2_t48_15B_UR50D()\nbatch_converter = alphabet.get_batch_converter()\nmodel.eval()  # disables dropout for deterministic results\n\n# open fasta file and read sequences\ndata = []\nwith open('dna_virus.fasta') as f:\n    for line in f:\n        if not line.startswith('>'):   # Skip any header lines\n            data.append(('', line.strip()))  # unnamed sequences\n\n# Prepare data (first 2 sequences from ESMStructuralSplitDataset superfamily / 4)\nbatch_labels, batch_strs, batch_tokens = batch_converter(data)\n\n# Extract per-residue representations (on CPU)\nwith torch.no_grad():\n    results = model(batch_tokens, repr_layers=[42])\n    torch.save(results['representations'][42], 'output.embeddings')"]}
{"id": 164, "github_id": 6, "output": ["import esm.inverse_folding\n\n# Load the pretrained model and its corresponding alphabet\nmodel, alphabet = esm.pretrained.esm_if1_gvp4_t16_142M_UR50()\nmodel = model.eval()\n\n# Load a single chain from PDB and extract the backbone coordinates\nstructure = esm.inverse_folding.util.load_structure('5YH2.pdb', 'B')\ncoords, seq = esm.inverse_folding.util.extract_coords_from_structure(structure)\n\n# Sample 3 sequence designs for the given structure at a temperature of 1\nsampled_sequences = model.sample(coords, temperature=1, num_samples=3)\n\n# Save the sampled sequences in fasta format to the specified output file\nwith open('model_structure.fasta', 'w') as f:\n    for i, sequence in enumerate(sampled_sequences):\n        f.write(f'>Sample_{i+1}\\n{sequence}\\n')", "import torch\nimport esm\n\n# Load ESMFold model\nmodel = esm.pretrained.esmfold_v1()\nmodel = model.eval().cuda()\n\n# Override the chunk size if you want to, this can help to reduce memory\n# model.set_chunk_size(128)\n\n# The structure to extract the design, mentioning the chain\nstructure = \"5YH2.pdb\"\nchain = \"B\"\n\nwith torch.no_grad():\n    designs = model.design(structure, chain=chain, num_samples=3, temperature=1)\n\n# Writing to fasta file\nwith open(\"model_structure.fasta\", \"w\") as file:\n    for i, design in enumerate(designs):\n        file.write(f\">Design_{i+1}\\n{design}\\n\")\n", "import torch\nimport esm\n\nmodel = esm.pretrained.esmfold_v1()\nmodel = model.eval().cuda()\n\nsequence = \"5YH2.pdb:chain B\"\n\nwith torch.no_grad():\n    output = model.infer_designs(sequence, num_samples=3)\n\nwith open(\"model_structure.fasta\", \"w\") as f:\n    for i, seq in enumerate(output):\n        f.write(f\">design_{i+1}\\n{seq}\\n\")", "import esm.inverse_folding\n\n# Load the model\nmodel, alphabet = esm.pretrained.esm_if1_gvp4_t16_142M_UR50()\nmodel = model.eval()\n\n# Load the structure\nstructure = esm.inverse_folding.util.load_structure('5YH2.pdb', 'B')\n# Extract coordinates\ncoords, seq = esm.inverse_folding.util.extract_coords_from_structure(structure)\n\n# Sample sequences\nfor i in range(3):\n    sampled_seq = model.sample(coords, temperature=1)\n    fasta_header = f'>sample_{i+1}'\n    fasta_seq = ''.join([alphabet.itos[i] for i in sampled_seq])\n    with open('model_structure.fasta', 'a') as f:\n        f.write(fasta_header + '\\n' + fasta_seq + '\\n')", "import esm.inverse_folding\n\n# Load the model and its corresponding alphabet\nmodel, alphabet = esm.pretrained.esm_if1_gvp4_t16_142M_UR50()\nmodel = model.eval()\n\n# Load the structure and extract the backbone coordinates\nstructure = esm.inverse_folding.util.load_structure('5YH2.pdb', 'B')\ncoords, seq = esm.inverse_folding.util.extract_coords_from_structure(structure)\n\n# Sample sequences for the given structure\nsampled_sequences = esm.inverse_folding.multichain_util.sample_sequence_in_complex(\n    model, coords, 'B', temperature=1, num_samples=3\n)\n\n# Save the sampled sequences to a fasta file\nwith open('model_structure.fasta', 'w') as f:\n    for i, seq in enumerate(sampled_sequences):\n        f.write(f'>sample_{i+1}\n{seq}\n')\n"]}
{"id": 172, "github_id": 6, "output": ["import esm.inverse_folding\n\nmodel, alphabet = esm.pretrained.esm_if1_gvp4_t16_142M_UR50()\nmodel = model.eval()\n\nfpath = 'examples/inverse_folding/data/5YH2.pdb'\n\nstructure = esm.inverse_folding.util.load_structure(fpath, 'B')\ncoords, seq = esm.inverse_folding.util.extract_coords_from_structure(structure)\n\nfor i in range(3):\n    sampled_seq = model.sample(coords, temperature=3)\n    print(f'Sampled sequence {i+1}:', sampled_seq)", "import esm.inverse_folding\n\n# Load the pretrained model and its corresponding alphabet\nmodel, alphabet = esm.pretrained.esm_if1_gvp4_t16_142M_UR50()\nmodel = model.eval()\n\n# Load the structure\nstructure = esm.inverse_folding.util.load_structure('examples/inverse_folding/data/5YH2.pdb', 'B')\ncoords, seq = esm.inverse_folding.util.extract_coords_from_structure(structure)\n\n# Sample 3 sequence designs\nfor _ in range(3):\n    sampled_seq = model.sample(coords, temperature=3)\n    \n    # The sampled sequence would be a tensor, convert it back to string using the alphabet\n    sampled_seq_str = ''.join(alphabet.get_aa(seq) for seq in sampled_seq.cpu().numpy())\n    print(sampled_seq_str)", "from esm.pretrained import esm_if1_gvp4_t16_142M_UR50\nimport esm.inverse_folding\n\n# Load the model\nmodel, alphabet = esm_if1_gvp4_t16_142M_UR50()\nmodel = model.eval()\n\n# Load the structure\nstructure = esm.inverse_folding.util.load_structure('examples/inverse_folding/data/5YH2.pdb', 'B')\n\n# Extract backbone coordinates\ncoords, seq = esm.inverse_folding.util.extract_coords_from_structure(structure)\n\n# Sampling 3 sequences for the given structure at temperature 3\nfor i in range(3):\n    sampled_sequence = model.sample(coords, temperature=3)\n    print(f'Sampled sequence {i+1}:', sampled_sequence)", "import esm\nimport esm.inverse_folding\n\nmodel, alphabet = esm.pretrained.esm_if1_gvp4_t16_142M_UR50()\nmodel = model.eval()\n\nstructure = esm.inverse_folding.util.load_structure('examples/inverse_folding/data/5YH2.pdb', 'B')\ncoords, seq = esm.inverse_folding.util.extract_coords_from_structure(structure)\n\nsampled_seq = model.sample(coords, temperature=3)\n\nfor idx, seq in enumerate(sampled_seq):\n    print(f'Sampled sequence {idx + 1} : {seq}')", "import esm.inverse_folding\nmodel, alphabet = esm.pretrained.esm_if1_gvp4_t16_142M_UR50()\nmodel = model.eval()\nstructure = esm.inverse_folding.util.load_structure('examples/inverse_folding/data/5YH2.pdb', 'B')\ncoords, seq = esm.inverse_folding.util.extract_coords_from_structure(structure)\nfor i in range(3):\n    sampled_seq = model.sample(coords, temperature=3)\n    print(f'Sampled Sequence {i+1}: {sampled_seq}')"]}
{"id": 176, "github_id": 6, "output": ["python sample_sequences.py data/5YH2.pdb --chain A --temperature 5 --num-samples 3 --outpath model_structure.fasta", "import esm.inverse_folding\nstructure = esm.inverse_folding.util.load_structure('5YH2.pdb', 'A')\ncoords, seq = esm.inverse_folding.util.extract_coords_from_structure(structure)\nmodel, alphabet = esm.pretrained.esm_if1_gvp4_t16_142M_UR50()\nmodel = model.eval()\nfor i in range(3):\n    sampled_seq = model.sample(coords, temperature=5)\n    with open('model_structure.fasta', 'a') as f:\n        f.write(f'>Structure_{i+1}\\n{sampled_seq}\\n')", "import esm.inverse_folding\n\n# model and alphabet loading\nmodel, alphabet = esm.pretrained.esm_if1_gvp4_t16_142M_UR50()\nmodel = model.eval()\n\n# load model structure\nstructure = esm.inverse_folding.util.load_structure('5YH2.pdb', 'A')\ncoords, seq = esm.inverse_folding.util.extract_coords_from_structure(structure)\n\n# sample sequences \nsampled_seq = model.sample(coords, temperature=5, num_samples=3)\n\n# Save to fasta file\nwith open('model_structure.fasta', 'w') as f:\n    for i, s in enumerate(sampled_seq):\n        f.write(f'>sequence_{i+1}\\n')\n        f.write(f'{s}\\n')\n", "import esm.inverse_folding\n\n# Load the model\nmodel = esm.pretrained.esm_if1_gvp4_t16_142M_UR50()\nmodel = model.eval()\n\n# Load the structure and ensure only backbone of chain A is used as model input\nstructure = esm.inverse_folding.multichain_util.load_structure_and_select_chain('5YH2.pdb', 'A')\ncoords, seq = esm.inverse_folding.multichain_util.extract_coords_from_structure(structure)\n\n# Sample 3 sequence designs for the given structure\nsampled_sequences = esm.inverse_folding.multichain_util.sample_n_sequence_designs_for_complex(model, coords, chain='C', temperature=5, num_samples=3)\n\n# Save the sampled sequences into fasta file\nwith open('model_structure.fasta', 'w') as f:\n    for i, seq in enumerate(sampled_sequences):\n        f.write(f'>structure_{i+1}\\n{seq}\\n')", "import esm.inverse_folding\n\nmodel, alphabet = esm.pretrained.esm_if1_gvp4_t16_142M_UR50()\nmodel = model.eval()\n\nstructure = esm.inverse_folding.util.load_structure('5YH2.pdb', 'A')\ncoords, seq = esm.inverse_folding.util.extract_coords_from_structure(structure)\n\nfor i in range(3):\n  sampled_seq = model.sample(coords, temperature=5)\n  with open('model_structure.fasta', 'a') as f:\n    f.write('>Structure_{}\\n'.format(i+1))\n    f.write(''.join([alphabet.int2sym[aa_idx] for aa_idx in sampled_seq]) + '\\n')"]}
{"id": 180, "github_id": 6, "output": ["import esm.inverse_folding\nmodel, alphabet = esm.pretrained.esm_if1_gvp4_t16_142M_UR50()\nmodel = model.eval()\nstructure = esm.inverse_folding.util.load_structure('examples/inverse_folding/data/5YH2.pdb', 'B')\ncoords, seq = esm.inverse_folding.util.extract_coords_from_structure(structure)\nsampled_seq = model.sample(coords, temperature=1)\nprint(sampled_seq)", "import esm.inverse_folding\nimport esm.pretrained\n\n# specify file and chain\nfpath = 'examples/inverse_folding/data/5YH2.pdb'\nchain_id = 'B'\n\n# specify number of samples and temperature\nnum_samples = 3\nT = 1\n\n# load model and alphabet\nmodel, alphabet = esm.pretrained.esm_if1_gvp4_t16_142M_UR50()\nmodel = model.eval()\n\n# load structure and extract coordinates\nstructure = esm.inverse_folding.util.load_structure(fpath, chain_id)\ncoords, seq = esm.inverse_folding.util.extract_coords_from_structure(structure)\n\n# sample sequences\ndesigns = [model.sample(coords, temperature=T) for _ in range(num_samples)]\n\n# output designs\nprint('Sampled sequence designs:')\nfor i, design in enumerate(designs):\n    print(f'Design {i+1}:')\n    print(design)\n", "import esm\n\n# Load the pretrained inverse folding model and the alphabet\nmodel, alphabet = esm.pretrained.esm_if1_gvp4_t16_142M_UR50()\nmodel = model.eval()\n\n# Load the structure and the chain backbone coordinates\nstructure = esm.inverse_folding.util.load_structure('examples/inverse_folding/data/5YH2.pdb', 'B')\ncoords, seq = esm.inverse_folding.util.extract_coords_from_structure(structure)\n\n# Sample 3 sequence designs for the given structure\nnum_samples = 3\ntemperature = 1\nsampled_sequences = [model.sample(coords, temperature=temperature) for _ in range(num_samples)]\n\n# Print the sampled sequences\nfor i, sampled_seq in enumerate(sampled_sequences):\n    print(f'Sampled sequence {i+1}: {sampled_seq}')", "import torch\nimport esm\n\n# Load ESMFold model\nmodel = esm.pretrained.esmfold_v1()\nmodel = model.eval().cuda()\n\n# Set the pdb file and chain\npdb_file = 'examples/inverse_folding/data/5YH2.pdb' \nchain = 'B'\n\n# Set the temperature for sampling\ntemperature = 1\n\n# Set the number of sequence designs you want to sample\nnum_samples = 3\n\n# Now run the sequence sampling with all your parameters\nwith torch.no_grad():\n    output = model.sample_sequences(pdb_file, chain, temperature, num_samples)\n\n# Print your sampled sequences\nprint(output)", "import esm\n\n# Load model\nmodel, alphabet = esm.pretrained.esm_if1_gvp4_t16_142M_UR50()\nmodel = model.eval()\n\n# Load structure\nimport esm.inverse_folding\nstructure = esm.inverse_folding.util.load_structure('examples/inverse_folding/data/5YH2.pdb', 'B')\ncoords, seq = esm.inverse_folding.util.extract_coords_from_structure(structure)\n\n# Sample sequence designs\nsampled_seq = model.sample(coords, temperature=1)\n\n# Print the sampled sequences\nprint('Sampled Sequence Designs:')\nfor i, s in enumerate(sampled_seq):\n    print(f'Sample {i+1}: {s}')"]}
{"id": 186, "github_id": 7, "output": ["\nimport torch\nfrom PIL import Image\nimport open_clip\n\n# Load ViT-B-32 model and pretrain on laion400m_e31\nmodel, _, preprocess = open_clip.create_model_and_transforms('ViT-B-32', pretrained='laion400m_e31')\ntokenizer = open_clip.get_tokenizer('ViT-B-32')\n\n# Your list of image paths here \nimage_paths = ['/path/to/image1', '/path/to/image2', '/path/to/image3'] \n\nimage_embeddings = []\nfor image_path in image_paths:\n    # Open and pre-process image\n    image = preprocess(Image.open(image_path)).unsqueeze(0)\n\n    # Compute the image embedding\n    with torch.no_grad(), torch.cuda.amp.autocast():\n        image_embedding = model.encode_image(image)\n        image_embeddings.append(image_embedding)\n    image_embeddings = torch.cat(image_embeddings)\n\n# Now you can use 'image_embeddings' for your further tasks.\n# Such as clustering, similarity computation etc.\n", "import torch\nfrom PIL import Image\nimport open_clip\n\nmodel, _, preprocess = open_clip.create_model_and_transforms('ViT-B-32', pretrained='laion400m_e31')\ntokenizer = open_clip.get_tokenizer('ViT-B-32')\n\nimage = preprocess(Image.open(\"your_image.png\")).unsqueeze(0)\ntext = tokenizer([\"a description of your image\"])\n\nwith torch.no_grad(), torch.cuda.amp.autocast():\n    image_features = model.encode_image(image)\n    text_features = model.encode_text(text)\n    image_features /= image_features.norm(dim=-1, keepdim=True)\n    text_features /= text_features.norm(dim=-1, keepdim=True)\n\n    text_probs = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n\nprint(\"Label probs:\", text_probs)", "import torch\nfrom PIL import Image\nimport open_clip\n\nmodel, _, preprocess = open_clip.create_model_and_transforms('ViT-B-32', pretrained='laion400m_e31')\ntokenizer = open_clip.get_tokenizer('ViT-B-32')\n\nimage = preprocess(Image.open(\"your_image.png\")).unsqueeze(0)\ntext = tokenizer([\"a diagram\", \"a dog\", \"a cat\"])\n\nwith torch.no_grad(), torch.cuda.amp.autocast():\n    image_features = model.encode_image(image)\n    text_features = model.encode_text(text)\n    image_features /= image_features.norm(dim=-1, keepdim=True)\n    text_features /= text_features.norm(dim=-1, keepdim=True)\n\n    text_probs = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n\nprint(\"Label probs:\", text_probs)  # prints: [[1., 0., 0.]]", "import torch\nfrom PIL import Image\nimport open_clip\n\nmodel, _, preprocess = open_clip.create_model_and_transforms('ViT-B-32', pretrained='laion400m_e31')\ntokenizer = open_clip.get_tokenizer('ViT-B-32')\n\nimage_path_list = ['image1.png', 'image2.png', 'image3.png']  # Fill this with paths to your images\n\ntext_list = ['description for image1', 'description for image2', 'description for image3']  # Fill this with corresponding descriptions\n\nfor idx, image_path in enumerate(image_path_list):\n    image = preprocess(Image.open(image_path)).unsqueeze(0)\n    text = tokenizer([text_list[idx]])\n\n    with torch.no_grad(), torch.cuda.amp.autocast():\n        image_features = model.encode_image(image)\n        text_features = model.encode_text(text)\n        image_features /= image_features.norm(dim=-1, keepdim=True)\n        text_features /= text_features.norm(dim=-1, keepdim=True)\n        similarity = (image_features @ text_features.T).item()\n        print(f'Similarity between description and image {idx+1}: {similarity}')", "import torch\nfrom PIL import Image\nimport open_clip\n\nmodel, _, preprocess = open_clip.create_model_and_transforms('ViT-B-32', pretrained='laion400m_e31')\ntokenizer = open_clip.get_tokenizer('ViT-B-32')\n\n# replace 'your_image.jpg' with your image file\nimage = preprocess(Image.open('your_image.jpg')).unsqueeze(0)\ntext = tokenizer(['your text'])\n\nwith torch.no_grad(), torch.cuda.amp.autocast():\n    image_features = model.encode_image(image)\n    text_features = model.encode_text(text)\n    image_features /= image_features.norm(dim=-1, keepdim=True)\n    text_features /= text_features.norm(dim=-1, keepdim=True)\n\ntext_probs = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n\n# The probabilities for each text\nprint('Label probs:', text_probs)"]}
{"id": 190, "github_id": 7, "output": ["import torch\nfrom PIL import Image\nimport open_clip\n\nimages_paths = ['./usr/image/plane.jpg', './usr/image/cat.jpg']\n\ndescriptions = ['A plane is flying in the blue sky', 'A grey and white cat']\n\n# Create model, tokenizer, and preprocess\nmodel, tokenizer, preprocess = open_clip.create_model_and_transforms('convnext_large_d', pretrained='laion2b_s26b_b102k_augreg')\n\n# Process each image\nfor image_path, description in zip(images_paths, descriptions):\n  # Open and preprocess image\n  image = preprocess(Image.open(image_path)).unsqueeze(0)\n\n  text = tokenizer([description])\n\n  # Generate image and text features\n  with torch.no_grad(), torch.cuda.amp.autocast():\n    image_features = model.encode_image(image)\n    text_features = model.encode_text(text)\n    image_features /= image_features.norm(dim=-1, keepdim=True)\n    text_features /= text_features.norm(dim=-1, keepdim=True)\n\n  # Calculate similarity\n  similarity = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n  print(f'Similarity between image {image_path} and description '{description}': {similarity.item() * 100}%')\n", "import torch\nfrom PIL import Image\nimport open_clip\n\n# Load the model\nmodel, _, preprocess = open_clip.create_model_and_transforms('convnext_large_d', pretrained='laion2b_s26b_b102k_augreg')\ntokenizer = open_clip.get_tokenizer('convnext_large_d')\n\n# Define the images and their descriptions\ndescriptions = ['A plane is flying in the blue sky', 'A grey and white cat']\nimage_paths = ['./usr/image/plane.jpg', './usr/image/cat.jpg']\n\nfor i, image_path in enumerate(image_paths):\n    # Load and preprocess the image\n    image = preprocess(Image.open(image_path)).unsqueeze(0)\n\n    # Encode the description\n    text = tokenizer([descriptions[i]])\n\n    with torch.no_grad(), torch.cuda.amp.autocast():\n        # Encode the image and text\n        image_features = model.encode_image(image)\n        text_features = model.encode_text(text)\n\n        # Normalize the features\n        image_features /= image_features.norm(dim=-1, keepdim=True)\n        text_features /= text_features.norm(dim=-1, keepdim=True)\n\n        # Calculate the probabilities\n        text_probs = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n\n    print(\"Label probs for image '{}':\".format(image_path), text_probs)", "import torch\nfrom PIL import Image\nimport open_clip\n\n# Create the model and transforms\nmodel, _, preprocess = open_clip.create_model_and_transforms('convnext_large_d', pretrained='laion2b_s26b_b102k_augreg')\ntokenizer = open_clip.get_tokenizer('convnext_large_d')\n\n# List of image-to-text descriptions\ndescription_dict = {\n  'plane': 'A plane is flying in the blue sky',\n  'cat': 'A grey and white cat'\n}\n\n# Preprocess each image and create a tensor from it\nimage_tensors = {}\nfor image_name, description in description_dict.items():\n  image = preprocess(Image.open(f\"./usr/image/{image_name}.png\")).unsqueeze(0)\n  image_tensors[image_name] = image\n\n# Use the OpenCLIP model to predict the label for each image\nfor image_name, image_tensor in image_tensors.items():\n  # Prepare the text\n  text = tokenizer([description_dict[image_name]])\n\n  # Compute the features for the image and the text\n  with torch.no_grad(), torch.cuda.amp.autocast():\n      image_features = model.encode_image(image_tensor)\n      text_features = model.encode_text(text)\n\n      # Normalize the features\n      image_features /= image_features.norm(dim=-1, keepdim=True)\n      text_features /= text_features.norm(dim=-1, keepdim=True)\n\n  # Compute the probabilities for each label\n  text_probs = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n  print(f\"Label probs for {image_name}:\", text_probs)  # this will print the probabilities of each label for the image", "import torch\nfrom PIL import Image\nimport open_clip\n\n# \u0110reate model and preprocess\nmodel, _, preprocess = open_clip.create_model_and_transforms(\n    'convnext_large_d', pretrained='laion2b_s26b_b102k_augreg')\ntokenizer = open_clip.get_tokenizer('convnext_large_d')\n\n# Load Images\nimage1 = preprocess(Image.open('/usr/image/plane.jpg')).unsqueeze(0)\nimage2 = preprocess(Image.open('/usr/image/cat.jpg')).unsqueeze(0)\n\n# Process Texts\ntext1 = tokenizer(['A plane is flying in the blue sky'])\ntext2 = tokenizer(['A grey and white cat'])\n\nwith torch.no_grad(), torch.cuda.amp.autocast():\n  # Encode image and text features\n  image_features1 = model.encode_image(image1)\n  text_features1 = model.encode_text(text1)\n  image_features2 = model.encode_image(image2)\n  text_features2 = model.encode_text(text2)\n\n  # Normalize\n  image_features1 /= image_features1.norm(dim=-1, keepdim=True)\n  text_features1 /= text_features1.norm(dim=-1, keepdim=True)\n  image_features2 /= image_features2.norm(dim=-1, keepdim=True)\n  text_features2 /= text_features2.norm(dim=-1, keepdim=True)\n\n  # Calculate probability\n  text_probs1 = (100.0 * image_features1 @ text_features1.T).softmax(dim=-1)\n  text_probs2 = (100.0 * image_features2 @ text_features2.T).softmax(dim=-1)\n\n# output text probabilities\nprint('Probability for plane:', text_probs1, '\\nProbability for cat:', text_probs2)", "import torch\nfrom PIL import Image\nimport open_clip\n\nmodel, _, preprocess = open_clip.create_model_and_transforms('convnext_large_d', pretrained='laion2b_s26b_b102k_augreg')\ntokenizer = open_clip.get_tokenizer('convnext_large_d')\n\nimage_paths = ['./usr/image/plane.jpg', './usr/image/cat.jpg']\ntexts = ['A plane is flying in the blue sky', 'A grey and white cat']\n\nfor image_path, text in zip(image_paths, texts):\n    image = preprocess(Image.open(image_path)).unsqueeze(0)\n    text = tokenizer([text])\n\n    with torch.no_grad(), torch.cuda.amp.autocast():\n        image_features = model.encode_image(image)\n        text_features = model.encode_text(text)\n        image_features /= image_features.norm(dim=-1, keepdim=True)\n        text_features /= text_features.norm(dim=-1, keepdim=True)\n\n        # Calculate the match between the image and the text\n        similarity = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n\n    print(f'The match between {image_path} and the given text is {similarity.item()}%')"]}
{"id": 194, "github_id": 7, "output": ["import torch\nfrom PIL import Image\nimport open_clip\n\n#Creating the model and transforms\nmodel, _, preprocess = open_clip.create_model_and_transforms('EVA02-E-14', pretrained='laion2b_s4b_b115k')\ntokenizer = open_clip.get_tokenizer('EVA02-E-14')\n\n#Path to your directory\nimage_directory = './usr/image'\n\nimage_labels = ['plane', 'cat']\nimage_descriptions = ['A plane is flying in the blue sky', 'A grey and white cat']\n\nfor label, description in zip(image_labels, image_descriptions):\n  image_path = f'{image_directory}/{label}.png'  # Assuming images are png files\n  image = preprocess(Image.open(image_path)).unsqueeze(0)\n  text = tokenizer([description])\n\n  with torch.no_grad(), torch.cuda.amp.autocast():\n    image_features = model.encode_image(image)\n    text_features = model.encode_text(text)\n    image_features /= image_features.norm(dim=-1, keepdim=True)\n    text_features /= text_features.norm(dim=-1, keepdim=True)\n\n    # Comparing the image with the description\n    similarity_score = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n\n  print(f'Similarity score between image {label} and description: {similarity_score}')", "import torch\nfrom PIL import Image\nimport open_clip\n\nmodel, _, preprocess = open_clip.create_model_and_transforms('EVA02-E-14', pretrained='laion2b_s4b_b115k')\ntokenizer = open_clip.get_tokenizer('EVA02-E-14')\n\nimage_dir = './usr/image'\nimage_files = os.listdir(image_dir)\n\nimage_descriptions = {\n  'plane': 'A plane is flying in the blue sky',\n  'cat': 'A grey and white cat'\n}\n\nimage2text_probs = {}\n\nfor image_file in image_files:\n  image_id = image_file.split('.')[0]\n  if image_id not in image_descriptions:\n    continue\n\n  image = preprocess(Image.open(os.path.join(image_dir, image_file))).unsqueeze(0)\n  text = tokenizer([image_descriptions[image_id]])\n\n  with torch.no_grad(), torch.cuda.amp.autocast():\n    image_features = model.encode_image(image)\n    text_features = model.encode_text(text)\n    image_features /= image_features.norm(dim=-1, keepdim=True)\n    text_features /= text_features.norm(dim=-1, keepdim=True)\n\n    text_probs = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n    image2text_probs[image_file] = text_probs.item()\n\n# Sort the images based on their text matching probabilities in descending order\nsorted_images = sorted(image2text_probs.items(), key=lambda item: item[1], reverse=True)\n\n# Print sorted images\nfor image_file, text_prob in sorted_images:\n  print(f'{image_file}: {text_prob}')", "import torch\nfrom PIL import Image\nimport open_clip\n\n# Create model, tokenizer, and transform\nmodel, _, preprocess = open_clip.create_model_and_transforms('EVA02-E-14', pretrained='laion2b_s4b_b115k')\ntokenizer = open_clip.get_tokenizer('EVA02-E-14')\n\n# Define descriptions\nimage_descriptions = [('plane', 'A plane is flying in the blue sky'), ('cat', 'A grey and white cat')]\n\n# Sort images based on their descriptions\nfor image_name, description in image_descriptions:\n    image_path = f'./usr/image/{image_name}.jpg'\n    image = preprocess(Image.open(image_path)).unsqueeze(0)\n    text = tokenizer([description])\n\n    with torch.no_grad(), torch.cuda.amp.autocast():\n        # Extract image and text features\n        image_features = model.encode_image(image)\n        text_features = model.encode_text(text)\n\n        # Normalize features\n        image_features /= image_features.norm(dim=-1, keepdim=True)\n        text_features /= text_features.norm(dim=-1, keepdim=True)\n\n        # Calculate similarity between image and description\n        # The higher the similarity, the better the match\n        similarity = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n        print(f'Similarity between {image_name} and its description: {similarity.item()}')\n", "import torch\nfrom PIL import Image\nimport open_clip\n\ntokenizer = open_clip.get_tokenizer('EVA02-E-14')\nmodel, _, preprocess = open_clip.create_model_and_transforms('EVA02-E-14', pretrained='laion2b_s4b_b115k')\n\nimages = ['./usr/image/plane.jpg', './usr/image/cat.jpg']\ntexts = ['A plane is flying in the blue sky', 'A grey and white cat']\n\n# Preprocess the images and texts\nprocessed_images = [preprocess(Image.open(image)).unsqueeze(0) for image in images]\nprocessed_texts = tokenizer(texts)\n\n# Use model to encode images and texts\nwith torch.no_grad(), torch.cuda.amp.autocast():\n    image_features = [model.encode_image(image) for image in processed_images]\n    text_features = model.encode_text(processed_texts)\n\n# Normalize\nimage_features = [feat / feat.norm(dim=-1, keepdim=True) for feat in image_features]\ntext_features /= text_features.norm(dim=-1, keepdim=True)\n\n# Compute similarity and sort\nsimilarity = [torch.matmul(feat, text_features.T) for feat in image_features]\nsorted_indices = [torch.argsort(sim, descending=True) for sim in similarity]\n\n# Print results\nfor i, indices in enumerate(sorted_indices):\n    print(f'Image {images[i]} is most similar to: {texts[indices[0]]}')", "import torch\nfrom PIL import Image\nimport open_clip\n\n# Initialize the pretrained model and tokenizer\nmodel, _, preprocess = open_clip.create_model_and_transforms('EVA02-E-14', pretrained='laion2b_s4b_b115k')\ntokenizer = open_clip.get_tokenizer('EVA02-E-14')\n\n# Define the image labels and descriptions\ndescriptions = ['A plane is flying in the blue sky', 'A grey and white cat']\nlabels = ['plane', 'cat']\n\n# Convert descriptions to tokens\nencoded_descriptions = tokenizer(descriptions)\n\n# Sort images based on their descriptions\nsorted_images = sorted(\n    './usr/image',\n    key=lambda img_path: (\n        preprocess(Image.open(img_path)).unsqueeze(0),\n        model.encode_text(encoded_descriptions),\n        model.encode_image(img_path)\n    )\n)\n\n# Print the sorted images\nprint('Sorted images:', sorted_images)"]}
{"id": 198, "github_id": 7, "output": ["import torch\nfrom PIL import Image\nimport open_clip\nimport os\n\n# initialize the model and transformation\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nmodel, _, preprocess = open_clip.create_model_and_transforms('ViT-B-32', pretrained='commonpool_s_laion_s13m_b4k', device=device)\n\n# a simple function for image encoding\ndef encode_image(image_path, encoder, preprocess):\n  image = Image.open(image_path)\n  image = preprocess(image).unsqueeze(0).to(device)\n  with torch.no_grad():\n    image_features = encoder(image)\n  return image_features\n\n# a list to hold the image paths and their corresponding encodings\nimage_encodings = []\n\n# the directory containing the images\nimage_dir = './image'\n\n# iterate over all the images in the directory and add their encodings to the list\nfor image_name in os.listdir(image_dir):\n  image_path = os.path.join(image_dir, image_name)\n  image_encodings.append((image_path, encode_image(image_path, model.encode_image, preprocess)))\n\n# sort the images based on their encodings\nsorted_images = sorted(image_encodings, key=lambda x: torch.sum(x[1]))\n\n# print the sorted list of image paths\nfor image_path, _ in sorted_images:\n  print(image_path)", "import torch\nfrom PIL import Image\nimport open_clip\nimport os\nfrom tqdm import tqdm\nimport operator\n\n# Create the model and transforms\nmodel, _, preprocess = open_clip.create_model_and_transforms('ViT-B-32', pretrained='commonpool_s_laion_s13m_b4k')\ntokenizer = open_clip.get_tokenizer('ViT-B-32')\n\n# Path to directory with images\nimage_dir = './image'\n\n# Prepare image features\nimage_features_dict = {}\n\nfor image_name in tqdm(os.listdir(image_dir)):\n    image = preprocess(Image.open(os.path.join(image_dir, image_name))).unsqueeze(0)\n    with torch.no_grad(), torch.cuda.amp.autocast():\n        image_features = model.encode_image(image)\n    \n    image_features_dict[image_name] = image_features[0].cpu().numpy()\n\n# Sort the images based on features\nsorted_images = sorted(image_features_dict.items(), key=operator.itemgetter(1))", "import torch\nfrom PIL import Image\nimport open_clip\nimport os\nimport glob\n\n# Load the model and the tokenizer\nmodel, _, preprocess = open_clip.create_model_and_transforms('ViT-B-32', pretrained='commonpool_s_laion_s13m_b4k')\n\ntokenizer = open_clip.get_tokenizer('ViT-B-32')\n\n# Define a directory where images are stored\nimage_dir = './image'\n\n# Get a list of all image files in the directory\nimage_files = glob.glob(f'{image_dir}/*')\n\n# Create a dict to store image features\nimage_features_dict = {}\n\n# Loop over each image file\ncount = 0\nfor image_file in image_files:\n    # Open and preprocess the image\n    image = preprocess(Image.open(image_file)).unsqueeze(0)\n    \n    # Compute features for the image\n    with torch.no_grad(), torch.cuda.amp.autocast():\n        image_features = model.encode_image(image)\n        \n    # Normalize the features\n    image_features /= image_features.norm(dim=-1, keepdim=True)\n    \n    # Store the features in the dict\n    image_features_dict[image_file] = image_features.cpu().numpy()\n    \n    count += 1\n    if count % 100 == 0:\n        print(f'Processed {count} images')\n\n# At this point image_features_dict contains the features for all images\n# You can use these features to sort the images as needed", "import torch\nfrom PIL import Image\nimport open_clip\nimport os\n\n# Load the model\nmodel, preprocess = open_clip.create_model_and_transforms('ViT-B-32', pretrained='commonpool_s_laion_s13m_b4k')\n\n# Directory with the images\ndir_path = './image'\n\n# Iterate over all images in the directory\nfor img_path in os.listdir(dir_path):\n    # Open and preprocess the image\n    image = preprocess(Image.open(os.path.join(dir_path, img_path))).unsqueeze(0)\n\n    # Preprocess the image and compute the image features\n    with torch.no_grad(), torch.cuda.amp.autocast():\n        img_features = model.encode_image(image)\n\n    # Here we are just printing the features, but you can use these features to sort the images\n    print(f'Features for image {img_path}: {img_features}')", "import torch\nfrom PIL import Image\nimport open_clip\nimport os\nfrom tqdm import tqdm\n\n# Create model and transforms \nmodel, _, preprocess = open_clip.create_model_and_transforms('ViT-B-32', pretrained='commonpool_s_laion_s13m_b4k')\n\n# Load images and sort\nimage_dir = './image'\nimage_filenames = os.listdir(image_dir)\nimage_features = []\n\n# Process each image\nfor image_filename in tqdm(image_filenames):\n    # Load and preprocess image\n    image = Image.open(os.path.join(image_dir, image_filename))\n    image = preprocess(image).unsqueeze(0)\n\n    # Generate image features\n    with torch.no_grad():\n        image_feature = model.encode_image(image).cpu().numpy()\n\n    image_features.append(image_feature)\n\n# Sort images by features\nsorted_images = [x for _, x in sorted(zip(image_features, image_filenames), key=lambda pair: pair[0])]\n\n# Print out the sorted images\nprint(sorted_images)\n"]}
{"id": 202, "github_id": 7, "output": ["import torch\nfrom PIL import Image\nimport os\nimport open_clip\n\n# Create the model\nmodel, _, preprocess = open_clip.create_model_and_transforms('convnext_xxlargemodel', pretrained='laion2b_s34b_b82k_augreg_rewind')\n\ntokenizer = open_clip.get_tokenizer('convnext_xxlargemodel')\n\n# Load the images\nimage_dir = './usr/image'\nimage_files = [os.path.join(image_dir, f) for f in os.listdir(image_dir) if os.path.isfile(os.path.join(image_dir, f))]\n\nfor image_file in image_files:\n    image = preprocess(Image.open(image_file)).unsqueeze(0)\n\n    # Perform inference with the model\n    with torch.no_grad(), torch.cuda.amp.autocast():\n        image_features = model.encode_image(image)\n        image_features /= image_features.norm(dim=-1, keepdim=True)\n\n    print('Image features for:', image_file, image_features)", "import torch\nfrom PIL import Image\nimport open_clip\n\n# create model and transforms\nmodel, _, preprocess = open_clip.create_model_and_transforms('convnext_xxlarge', pretrained='laion2b_s34b_b82k_augreg_rewind')\ntokenizer = open_clip.get_tokenizer('convnext_xxlarge')\n\n# load images\nimage_paths = ['./usr/image/image1.jpg', './usr/image/image2.jpg', ...]  # list all image paths\n\nimages = [preprocess(Image.open(path)).unsqueeze(0) for path in image_paths]\n\n# compute image features\nwith torch.no_grad(), torch.cuda.amp.autocast():\n    image_features = [model.encode_image(image) for image in images]\n    # image_features holds features for each image which can be used for organization\n", "import torch\nfrom PIL import Image\nimport open_clip\nimport os\n\nmodel, _, preprocess = open_clip.create_model_and_transforms('convnext_xxlarge', pretrained='laion2b_s34b_b82k_augreg_rewind')\n\nimage_folder = './usr/image'\n\nfor image_path in os.listdir(image_folder):\n    image = preprocess(Image.open(os.path.join(image_folder, image_path))).unsqueeze(0)\n\n    with torch.no_grad(), torch.cuda.amp.autocast():\n        image_features = model.encode_image(image)\n        image_features /= image_features.norm(dim=-1, keepdim=True)\n        \n        print(f'Embeding for {image_path}:', image_features)", "import torch\nfrom PIL import Image\nimport open_clip\n\n# Prepare model and tokenizer\nmodel, _, preprocess = open_clip.create_model_and_transforms('convnext_xxlarge', pretrained='laion2b_s34b_b82k_augreg_rewind')\n\n# The path to your images folder\nimage_dir = './usr/image'\n\n# Organise images\nimage_dict = {}\nfor image_file in os.listdir(image_dir):\n    image_path = os.path.join(image_dir, image_file)\n    \n    if image_path.endswith(('.png', '.jpg', '.jpeg')):\n        image = preprocess(Image.open(image_path))\n        image = image.unsqueeze(0)\n        if torch.cuda.is_available():\n            image = image.to('cuda')\n\n        with torch.no_grad(), torch.cuda.amp.autocast():\n            image_features = model.encode_image(image)\n\n        # Store the image features for every image\n        image_dict[image_file] = image_features.tolist()\n\n# You can save image_dict to a file for future use or perform further processing directly.", "import torch\nfrom PIL import Image\nimport open_clip\nimport glob\nimport os\n\n# set the path to the directory containing the images\ndir_path = './usr/image'\n\n# Create OpenAI's CLIP model and processing tools\nmodel, _, preprocess = open_clip.create_model_and_transforms('convnext_xxlargemodel', pretrained='laion2b_s34b_b82k_augreg_rewind')\n\n# Load and preprocess the images\nimage_paths = glob.glob(os.path.join(dir_path, '*'))\nimages = [preprocess(Image.open(image_path)).unsqueeze(0) for image_path in image_paths]\n\n# Compute the image embeddings\nwith torch.no_grad(), torch.cuda.amp.autocast():\n    image_features = [model.encode_image(image) for image in images]"]}
{"id": 206, "github_id": 7, "output": ["import torch\nfrom PIL import Image\nimport open_clip\nimport os\n\n# Create Model and Preprocess\nmodel, preprocess = open_clip.create_model_and_transforms('ViT-B-16', pretrained='commonpool_l_text_s1b_b8k')\ntext = open_clip.tokenize(['The dog is playing in the park.', 'A cat was driving on the road'])\n\n# Iterate over Images in the directory './usr/image/'\npicture_dict = {}\nfor picture in os.listdir('./usr/image'):\n    image_path = os.path.join('./usr/image', picture)\n    image = preprocess(Image.open(image_path)).unsqueeze(0)\n\n    with torch.no_grad(), torch.cuda.amp.autocast():\n        image_features = model.encode_image(image)\n        text_features = model.encode_text(text)\n        logits = image_features @ text_features.T\n        probs = logits.softmax(dim=-1)\n        picture_dict[picture] = probs[0].cpu().numpy()\n\n# Sort the images\nsorted_images = sorted(picture_dict.items(), key=lambda x: x[1])\n\n# Print Sorted Images\nfor image, _ in sorted_images:\n    print(image)", "import torch\nfrom PIL import Image\nimport open_clip\nimport os\n\n# Preparing the model and tokenizer\nmodel, _, preprocess = open_clip.create_model_and_transforms('ViT-B-16', pretrained='commonpool_l_text_s1b_b8k')\ntokenizer = open_clip.get_tokenizer('ViT-B-16')\n\n# Loading and preprocessing images\nimage_dir = './usr/image'\nimages = {}\nfor filename in os.listdir(image_dir):\n    if filename in ['cat.jpg', 'dog.jpg']:\n        image = preprocess(Image.open(os.path.join(image_dir, filename))).unsqueeze(0)\n        images[filename] = image\n\n# Defining the descriptions\ndescriptions = {\n    'cat.jpg': 'A cat was driving on the road',\n    'dog.jpg': 'A brown dog was lying on the ground'\n}\n\n# Encoding images and texts\nimage_features = {}\ntext_features = {}\nfor filename, image in images.items():\n    with torch.no_grad(), torch.cuda.amp.autocast():\n        image_features[filename] = model.encode_image(image)\n        text = tokenizer([descriptions[filename]])\n        text_features[filename] = model.encode_text(text)\n    image_features[filename] /= image_features[filename].norm(dim=-1, keepdim=True)\n    text_features[filename] /= text_features[filename].norm(dim=-1, keepdim=True)\n\n# Calculating the similarity scores\nsimilarities = {}\nfor filename in images.keys():\n    similarities[filename] = (100.0 * image_features[filename] @ text_features[filename].T).softmax(dim=-1)\n\n# Sorting the images\nsorted_images = sorted(similarities.items(), key=lambda item: item[1], reverse=True)\nprint('Sorted images:', sorted_images)", "import torch\nfrom PIL import Image\nimport open_clip\nimport os\nimport numpy as np\n\n# Initialize the model and tokenizer\ntokenizer = open_clip.get_tokenizer('ViT-B-16')\nmodel, _, preprocess = open_clip.create_model_and_transforms('ViT-B-16', pretrained='commonpool_l_text_s1b_b8k')\n\n# Image directory\nimage_dir = './usr/image'\n\n# Image descriptions\nimage_descriptions = {\n    'cat': 'A cat was driving on the road',\n    'dog': 'A brown dog was lying on the ground'\n}\n\n# Initialize a dictionary to store image features\nimage_features = {}\n\n# Preprocess images, pass through the model and store features\nfor img_file in os.listdir(image_dir):\n    image_path = os.path.join(image_dir, img_file)\n    image = preprocess(Image.open(image_path)).unsqueeze(0)\n    with torch.no_grad(), torch.cuda.amp.autocast():\n        image_features[img_file] = model.encode_image(image)\n\n# Create a list to store tuples of (image, text) features and sort it\nfeatures_list = []\n# Iterate through each image-text pair\nfor img_file, description in image_descriptions.items():\n    text = tokenizer([description])\n    with torch.no_grad(), torch.cuda.amp.autocast():\n        text_features = model.encode_text(text)\n        text_features /= text_features.norm(dim=-1, keepdim=True)\n\n    # Calculate similarity as dot product between image and text features\n    similarity = (image_features[img_file] @ text_features.T).item()\n    features_list.append((img_file, similarity))\n\n# Sort images based on similarity to the text description\nsorted_images = sorted(features_list, key=lambda x: x[1], reverse=True)\n\nprint('Sorted images:', [img[0] for img in sorted_images])", "import torch\nimport open_clip\nfrom PIL import Image\nimport os\n\n# Set up the model and tokenizer\nmodel, _, preprocess = open_clip.create_model_and_transforms('ViT-B-16', pretrained='commonpool_l_text_s1b_b8k')\ntokenizer = open_clip.get_tokenizer('ViT-B-16')\n\n# Set the path to images and initialize a dictionary to store image features\nimage_dir = './usr/image'\nimage_features_dict = {}\n\n# Loop over each image in the directory\nfor image_name in os.listdir(image_dir):\n    # Open the image and preprocess it\n    image = preprocess(Image.open(os.path.join(image_dir, image_name))).unsqueeze(0)\n\n    # Encode the image to get image features\n    with torch.no_grad(), torch.cuda.amp.autocast():\n        image_features = model.encode_image(image)\n    \n    # Normalize the image features and store them in the dictionary\n    image_features /= image_features.norm(dim=-1, keepdim=True)\n    image_features_dict[image_name] = image_features\n\n# Now we have image features for each image, we can sort them based on a given text prompt\nprompt = 'A cat was driving on the road'\n\n# Tokenize the prompt and get text features\nprompt_text = tokenizer([prompt])\nwith torch.no_grad(), torch.cuda.amp.autocast():\n    text_features = model.encode_text(prompt_text)\ntext_features /= text_features.norm(dim=-1, keepdim=True)\n\n# Calculate the similarity of each image to the prompt and sort the images by similarity\nsimilarity_scores = {}\nfor name, img_feat in image_features_dict.items():\n    similarity_scores[name] = (img_feat @ text_features.T).item()\n\nsorted_images = sorted(similarity_scores, key=similarity_scores.get, reverse=True)\n\n# Print the sorted images\necho(sorted_images)", "import torch\nfrom PIL import Image\nimport open_clip\nimport os\n\n# Define the model and pretraining\nmodel, _, preprocess = open_clip.create_model_and_transforms('ViT-B-16', pretrained='commonpool_l_text_s1b_b8k')\ntokenizer = open_clip.get_tokenizer('ViT-B-16')\n\n# Prepare image descriptions\nimage_descriptions = {\n    'cat': 'A cat was driving on the road',\n    'dog': 'A brown dog was lying on the ground'\n}\n\n# Define the directory with images\nimage_dir = './usr/image'\n\nimage_features = {}\n\n# Iterate over all images and compute features\nfor image_name, description in image_descriptions.items():\n    # Load and preprocess the image\n    image_path = os.path.join(image_dir, f'{image_name}.jpg')\n    image = preprocess(Image.open(image_path)).unsqueeze(0)\n\n    # Tokenize the description\n    text = tokenizer([description])\n\n    # Compute image and text features\n    with torch.no_grad(), torch.cuda.amp.autocast():\n        image_features[image_name] = model.encode_image(image)\n        text_features = model.encode_text(text)\n\n    # Normalize the features\n    image_features[image_name] /= image_features[image_name].norm(dim=-1, keepdim=True)\n    text_features /= text_features.norm(dim=-1, keepdim=True)\n\n# Sort images based on the cosine similarity between image and its description\nsorted_images = sorted(image_features.keys(), key=lambda x: torch.nn.CosineSimilarity()(image_features[x], text_features).item())\n\nprint('Sorted images:', sorted_images)"]}
{"id": 210, "github_id": 7, "output": ["import torch\nfrom PIL import Image\nimport open_clip\n\n# create model and transforms\nmodel, _, preprocess = open_clip.create_model_and_transforms('ViT-B-16', pretrained='laion2b_s34b_b88k')\ntokenizer = open_clip.get_tokenizer('ViT-B-16')\n\n# prepare an image\nimage = preprocess(Image.open(\"your-image.jpg\")).unsqueeze(0)\n\ntext = tokenizer([\"Description of the image\"])\n\n# performing model inference\ndone torch.cuda.amp.autocast():\n    image_features = model.encode_image(image)\n    text_features = model.encode_text(text)\n    \n    # normalize the features\n    image_features /= image_features.norm(dim=-1, keepdim=True)\n    text_features /= text_features.norm(dim=-1, keepdim=True)\n\n    # calculating text probabilities\n    text_probs = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n\nprint(\"Label probs:\", text_probs)\n", "import torch\nfrom PIL import Image\nimport open_clip\n\n# create a model (ViT-B-16) and its corresponding tokenizer\nmodel, _, preprocessor = open_clip.create_model_and_transforms('ViT-B-16', pretrained='laion2b_s34b_b88k')\ntokenizer = open_clip.get_tokenizer('ViT-B-16')\n\n# let's say you have a list of images (path to the images)\nimg_paths = ['img1.jpg', 'img2.jpg', 'img3.jpg']  # replace this with your actual image paths\n\n# now we will process each image, generate features and print the zero-shot label probabilities\nfor img_path in img_paths:\n    image = Image.open(img_path)\n    image = preprocessor(image).unsqueeze(0)  # preprocess the image and make it a batch of 1\n\n    # provide some classes/labels to choose from for your images\n    text = tokenizer(['a dog', 'a cat', 'a car', 'a chair'])\n\n    with torch.no_grad(), torch.cuda.amp.autocast():\n        image_features = model.encode_image(image)\n        text_features = model.encode_text(text)\n\n        # normalize features\n        image_features /= image_features.norm(dim=-1, keepdim=True)\n        text_features /= text_features.norm(dim=-1, keepdim=True)\n\n        # calculate the similarity between the images and the text labels\n        text_probs = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n\n    print(f'Image: {img_path}')\n    print(f'Label probabilities: {text_probs}')\n", "import torch\nfrom PIL import Image\nimport open_clip\n\nmodel, _, preprocess = open_clip.create_model_and_transforms('ViT-B-16', pretrained='laion2b_s34b_b88k')\n\n# Let's assume you have an image named 'image.jpg'\nimage = preprocess(Image.open('image.jpg')).unsqueeze(0)\n\n# Now let's create a list of target text descriptions\n# For example,\ntext_descriptions = [\n    'A cat',\n    'A dog',\n    'A car',\n    # Add more descriptions here\n]\n\n# Create the tokenizer\ntokenizer = open_clip.get_tokenizer('ViT-B-16')\n\ntext = tokenizer(text_descriptions)\n\n# Now let's get the image and text features\nwith torch.no_grad():\n    image_features = model.encode_image(image)\n    text_features = model.encode_text(text)\n    \n# Normalize the features\nimage_features /= image_features.norm(dim=-1, keepdim=True)\ntext_features /= text_features.norm(dim=-1, keepdim=True)\n\n# Calculate and print the text probabilities for the image\ntext_probs = (100.0 * image_features @ text_features.T).softmax(dim=-1)\nprint('Text probabilities:', text_probs)", "import torch\nfrom PIL import Image\nimport open_clip\n\n# Create model and transforms\nmodel, _, preprocess = open_clip.create_model_and_transforms('ViT-B-16', pretrained='laion2b_s34b_b88k')\n\ntokenizer = open_clip.get_tokenizer('ViT-B-16')\n\n# Load the images, preprocess it and unsqueeze it\nimage = preprocess(Image.open(\"YOUR_IMAGE.png\")).unsqueeze(0)\n\n# Specify the text labels for your images:\ntext = tokenizer(\"YOUR_TEXT\")\n\nwith torch.no_grad(), torch.cuda.amp.autocast():\n    image_features = model.encode_image(image)\n    text_features = model.encode_text(text)\n\n    # Normalize the features\n    image_features /= image_features.norm(dim=-1, keepdim=True)\n    text_features /= text_features.norm(dim=-1, keepdim=True)\n\n    # Compute probabilities \n    text_probs = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n\nprint(\"Label probs:\", text_probs)  # This prints the text labels probabilities for the given image", "import torch\nfrom PIL import Image\nimport open_clip\n\n# Initialize model and preprocess function\nmodel, _, preprocess = open_clip.create_model_and_transforms('ViT-B-16', pretrained='laion2b_s34b_b88k')\ntokenizer = open_clip.get_tokenizer('ViT-B-16')\n\n# Load an image (replace with your images)\nimage_path = 'path_to_your_image.jpg' # replace with your image path\nimage = preprocess(Image.open(image_path)).unsqueeze(0)\n\ntexts = ['a cat', 'a dog', 'a bird'] # replace with your object categories\n\n# Encode image and texts\ntext = tokenizer(texts)\nwith torch.no_grad(), torch.cuda.amp.autocast():\n    image_features = model.encode_image(image)\n    text_features = model.encode_text(text)\n    image_features /= image_features.norm(dim=-1, keepdim=True)\n    text_features /= text_features.norm(dim=-1, keepdim=True)\n\n    # Compute the similarity between image and each text\n    text_probs = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n\n# Print the similarity scores\nprint('Similarity scores:', text_probs.tolist()[0])"]}
{"id": 214, "github_id": 8, "output": ["import os\n\n# Set up parameters\nDATASET = 'EthanolConcentration'\nMODEL = 'MICN'\nPATIENCE = 10\nEPOCHS = 16\nBATCH_SIZE = 100\n\n# Define the script to run\nscript = f'bash ./scripts/classification/{MODEL}.sh'\n\n# Set up environment variables\nos.environ['DATASET'] = DATASET\nos.environ['PATIENCE'] = str(PATIENCE)\nos.environ['EPOCHS'] = str(EPOCHS)\nos.environ['BATCH_SIZE'] = str(BATCH_SIZE)\n\n# Run the script\nos.system(script)", "python main.py --task_name classification --is_training 1 --root_path ./dataset/EthanolConcentration/ --model_id EthanolConcentration --model MICN --data UEA --e_layers 3 --batch_size 100 --d_model 128 --d_ff 256 --top_k 3 --des 'Exp' --itr 1 --learning_rate 0.001 --train_epochs 16 --c_out 3 --patience 10", "python main.py --task_name classification --is_training 1 --root_path ./dataset/EthanolConcentration/ --model_id EthanolConcentration --model MICN --data UEA --e_layers 3 --batch_size 100 --d_model 128 --d_ff 256 --top_k 3 --des 'Exp' --itr 1 --learning_rate 0.001 --train_epochs 16 --patience 10", "python -u run.py \\\n  --task_name classification \\\n  --is_training 1 \\\n  --root_path ./dataset/EthanolConcentration/ \\\n  --model_id EthanolConcentration \\\n  --model MICN \\\n  --data UEA \\\n  --e_layers 3 \\\n  --batch_size 16 \\\n  --d_model 128 \\\n  --d_ff 256 \\\n  --top_k 3 \\\n  --des 'Exp' \\\n  --itr 1 \\\n  --learning_rate 0.001 \\\n  --train_epochs 100 \\\n  --c_out 3 \\\n  --patience 10", "import torch.nn as nn\n\n# Define MIC class\nclass MIC(nn.Module):\n    def __init__(self, feature_size=512, n_heads=8, dropout=0.05, decomp_kernel=[32], conv_kernel=[24],isometric_kernel=[18, 6], device='cuda'):\n        super(MIC, self).__init__()\n        # Initialize your layers here\n\n# Define SeasonalPrediction class\nclass SeasonalPrediction(nn.Module):\n    def __init__(self, embedding_size=512, n_heads=8, dropout=0.05, d_layers=1, decomp_kernel=[32], c_out=1, conv_kernel=[2, 4], isometric_kernel=[18, 6], device='cuda'):\n        super(SeasonalPrediction, self).__init__()\n        # Initialize your layers here\n\n# Define Model class\nclass Model(nn.Module):\n    def __init__(self, configs, conv_kernel=[12, 16]):\n        super(Model, self).__init__()\n        # Initialize your layers here\n\n# Define your patience, epochs, and batch_size\npatience = 10\nepochs = 16\nbatch_size = 100\n\n# Define the EthanolConcentration subset from UEA datasets (Assuming it's loaded somewhere)\nEthanolConcentration_subset = ''\n\n# Initialize your models\nmic_model = MIC()\nseasonal_model = SeasonalPrediction()\nmodel = Model()\n\n# Use your models\n# ...\n"]}
{"id": 218, "github_id": 8, "output": ["from Time_Series_Library import Informer2020\n\n# Define model\nmodel_name = 'informer'  # Cross-former model in the library is named Informer2020\nmodel = Informer2020(enc_in=21, dec_in=21, c_out=21, seq_len=96, label_len=48, pred_len=96,\n                  factor=3, d_model=32, n_heads=8, e_layers=2,\n                  d_layers=1, d_ff=32, dropout=0.05, attn='prob',\n                  embed='timeF', freq='h', activation='gelu',\n                  output_attention=False, distil=True, mix=True,\n                  device=torch.device('cuda:0'),\n                  top_k=5, des='Exp')\n\n# Dataset parameters\nis_training = True\nroot_path = './dataset/weather/'\ndata_path = 'weather.csv'\n\n# Training parameters\ntrain_epochs = 1\nitr=1\ndes='Exp'\ntask='long_term_forecast'\nfeatures='M'\nmodel_id='weather_96_96'\n\ndata = 'custom'\n\n# Training\ndef long_term_forecast(is_training, data, root_path, data_path, task, features, seq_len, label_len, pred_len, model_id, model):\n    if is_training:\n        model.train()\n    else:\n        model.eval()\n\n    # Load and process data\n    data.get_data(data_path)\n    data.seq_len = seq_len\n    data.label_len = label_len\n    data.pred_len = pred_len\n    data.data_scale()\n\n    # Train or forecast\n    if is_training:\n        model.train_model()  \n    else:\n        forecast_values = model.forecast()\n        return forecast_values\n\n# Call the function\nlong_term_forecast(is_training, data, root_path, data_path, task, features, seq_len, label_len, pred_len, model_id, model)", "python -u run.py --task_name long_term_forecast --is_training 1 --root_path ./dataset/weather/ --data_path weather.csv --model_id weather_96_96 --model Crossformer --data custom --features M --seq_len 96 --label_len 48 --pred_len 96 --e_layers 2 --d_layers 1 --factor 3 --enc_in 21 --dec_in 21 --c_out 21 --d_model 32 --d_ff 32 --top_k 5 --des 'Exp' --itr 1", "#!/usr/bin/env python\n\nimport os\nimport time_series_library as tsl\n\n# Config parameters\nparams = {\n'task_name': 'long_term_forecast',\n'is_training': 1,\n'root_path': './dataset/weather/',\n'data_path': 'weather.csv',\n'model_id': 'weather_96_96',\n'model': 'Crossformer',\n'data': 'custom',\n'features': 'M',\n'seq_len': 96,\n'label_len': 48,\n'pred_len': 96,\n'e_layers': 2,\n'd_layers': 1,\n'factor': 3,\n'enc_in': 21,\n'dec_in': 21,\n'c_out': 21,\n'd_model': 32,\n'd_ff': 32,\n'top_k': 5,\n'des': 'Exp',\n'itr': 1,\n'train_epochs': 1\n}\n\n# Define model\nmodel = tsl.Crossformer(params)\n\n# Load data\ndata = tsl.data_load(params['root_path']+params['data_path'])\n\n# Train\nmodel.train(data, params['train_epochs'])\n\n# Save model\nmodel.save(params['model_id'])", "python -u run.py \\\n  --task_name long_term_forecast \\\n  --is_training 1 \\\n  --root_path ./dataset/weather/ \\\n  --data_path weather.csv \\\n  --model_id weather_96_96 \\\n  --model Crossformer \\\n  --data custom \\\n  --features M \\\n  --seq_len 96 \\\n  --label_len 48 \\\n  --pred_len 96 \\\n  --e_layers 2 \\\n  --d_layers 1 \\\n  --factor 3 \\\n  --enc_in 21 \\\n  --dec_in 21 \\\n  --c_out 21 \\\n  --d_model 32 \\\n  --d_ff 32 \\\n  --top_k 5 \\\n  --des 'Exp' \\\n  --itr 1", "# Here is an example code based on your provided parameters\n\nfrom Crossformer import Crossformer\n\nif __name__ == '__main__':\n\n    # Initialize the Crossformer\n    model = Crossformer(\n        task_name = 'long_term_forecast', \n        is_training = 1, \n        root_path = './dataset/weather/',\n        data_path = 'weather.csv',\n        model_id = 'weather_96_96',\n        model = 'Crossformer',\n        data = 'custom',\n        features = 'M',\n        seq_len = 96,\n        label_len = 48,\n        pred_len = 96,\n        e_layers = 2,\n        d_layers = 1,\n        factor = 3,\n        enc_in = 21,\n        dec_in = 21,\n        c_out = 21,\n        d_model = 32,\n        d_ff = 32,\n        top_k = 5,\n        des = 'Exp',\n        itr = 1,\n        train_epochs = 1\n    )\n\n    # Train the model\n    model.train()\n\n    # Make predictions\n    model.predict()\n\n    # Evaluate the model performance\n    model.evaluate()\n}"]}
{"id": 222, "github_id": 8, "output": ["import os\nfrom exp.exp_long_term_forecasting import Exp_Basic\n\n# Set the path to the dataset\nos.environ['TSlib_DIR'] = './dataset/ETT-small/'\n\n# Set the specific dataset\nos.environ['data_id'] = 'ETTh1'\n\n# Set the model to 'DLinear'\nos.environ['model_id'] = 'DLinear'\n\n# Set the prediction length (steps)\nos.environ['pred_len'] = '720'\n\n# Perform the experiment\nExp_Basic(exp_id=os.environ['model_id'], task_id='long_term_forecasting')", "import os\nfrom time_series_forecast import TimeSeriesForecast\n\n# Initialize the object with the desirable parameters\nts = TimeSeriesForecast(model_id='ETTh1_96_96', model_name='DLinear', if_training=1, root_path='./dataset/ETT-small/',\n                       data_path='ETTh1.csv', data_name='ETTh1', features='M', E_layers = 2, D_layers = 1,\n                       d_ff = 2048, d_model = 512, N = 2, seq_len = 96, label_len = 48, pred_len = 720,\n                       e_layers=2, d_layers=1, factor=3, enc_in=7, dec_in=7, cst_in=7, d_model=512,\n                       ff_hidnen=2048, n_head=8)\n\n# Use the initialized object to train the model and forecast\nts.task_long_term_forecast()", "export CUDA_VISIBLE_DEVICES=2\n\nmodel_name=DLinear\n\npython -u run.py \\\n  --task_name long_term_forecast \\\n  --is_training 1 \\\n  --root_path ./dataset/ETT-small/ \\\n  --data_path ETTh1.csv \\\n  --model $model_name \\\n  --data ETTh1 \\\n  --features M \\\n  --seq_len 96 \\\n  --label_len 48 \\\n  --pred_len 720 \\\n  --e_layers 2 \\\n  --d_layers 1 \\\n  --factor 3 \\\n  --enc_in 7 \\\n  --dec_in 7 \\\n  --c_out 7 \\\n  --des 'Exp' \\\n  --itr 1", "import os\nimport sys\nsys.path.append(os.path.abspath(os.path.join('./')))\n\nfrom exp.exp_informer import Exp_Informer\n\n# set the task arguments\ntask = 'long_term_forecast'\nis_training = 1\nroot_path = './dataset/ETT-small/'\ndata_path = 'ETTh1.csv'\nmodel_id = 'ETTh1_96_96'\nmodel = 'DLinear'\ndata = 'ETTh1'\nfeatures = 'M'\nseq_len = 720\nlabel_len = 48\npred_len = 720\ne_layers = 2\nd_layers = 1\nfactor = 3\nenc_in = 7\ndec_in = 7\nc_out = 7\ndes = 'Exp'\nitr = 1\n\nexp = Exp_Informer(task, model, data, features, seq_len, label_len, pred_len,\n                  e_layers, d_layers, factor, enc_in, dec_in, c_out, model_id, root_path, \n                  is_training, des)\n\nexp.run(itrs=[itr], setting={})\n", "import os\nfrom pytorch_forecasting import TemporalFusionTransformer, TimeSeriesDataSet\nfrom pytorch_forecasting.data import groupby_apply\nfrom pytorch_forecasting.metrics import QuantileLoss\nfrom pytorch_forecasting.models.temporal_fusion_transformer.tuning import optimize_hyperparameters\nimport torch\nfrom torch.utils.data import DataLoader\n\n# set the parameters\ntask_name = 'long_term_forecast'\nis_training = True\nroot_path = './dataset/ETT-small/'\ndata_path = 'ETTh1.csv'\nmodel_id = 'ETTh1_96_96'\nmodel = 'DLinear'\ndata = 'ETTh1'\nfeatures = 'M'\nseq_len = 96\nlabel_len = 48\npred_len = 96  # set the prediction length to 720\ne_layers = 2\nd_layers = 1\nfactor = 3\nenc_in = 7\ndec_in = 7\nc_out = 7\ndes = 'Exp'\nitr = 1\n\n# get the dataset\ndata_loader = TimeSeriesDataSet.load_from_checkpoint(os.path.join(root_path, data_path))\n\n# generate model\nmodel = TemporalFusionTransformer.from_dataset(\n    data_loader,\n    learning_rate=0.03,\n    hidden_size=16,\n    attention_head_size=1,\n    dropout=0.1,\n    hidden_continuous_size=8,\n    output_size=7,  # 7 quantiles by default\n    loss=QuantileLoss()\n)\n\n# train the model\ntrainer = torch.optim.Adam(model.parameters(), lr=0.01)\nnum_epochs = 10  # arbitrary large number\nearly_stop_callback = EarlyStopping(monitor=\"val_loss\", min_delta=1e-4, patience=10, verbose=False, mode=\"min\")\ntrainer.fit(model, train_dataloader=data_loader, val_dataloaders=val_dl, epochs=num_epochs, callbacks=[early_stop_callback])\n\n# save the model\ntorch.save(model.state_dict(), model_id + '.model')\n"]}
{"id": 226, "github_id": 8, "output": ["import sys\n\nsys.argv = ['--task_name long_term_forecast',\n '--is_training 1',\n '--root_path ./dataset/weather/',\n '--data_path weather.csv',\n '--model_id weather_96_96',\n '--model PatchTST',\n '--data custom',\n '--features M',\n '--seq_len 96',\n '--label_len 48',\n '--pred_len 720',\n '--e_layers 2',\n '--d_layers 1',\n '--factor 3',\n '--enc_in 21',\n '--dec_in 21',\n '--c_out 21',\n '--des Exp',\n '--itr 1',\n '--n_heads 4',\n '--train_epochs 3']\n\n# Including the Time-Series-Library path\nsys.path.insert(0, 'path_to_Time-Series-Library_repository')\n\nfrom TimeSeries import TimeSeriesTrain\n\ntrain = TimeSeriesTrain()\ntrain.run()", "import os\nfrom pts import PatchTST\n\n# Define parameters\nparams = {\n    'task': 'long_term_forecast',\n    'model_id': 'weather_96_96',\n    'model': '$model_name',\n    'data': 'custom',\n    'features': 'M',\n    'seq_len': 96,\n    'label_len': 48,\n    'pred_len': 720,\n    'e_layers': 2,\n    'd_layers': 1,\n    'factor': 3,\n    'enc_in': 21,\n    'dec_in': 21,\n    'c_out': 21,\n    'des': 'Exp',\n    'itr': 1,\n    'n_heads': 4,\n    'train_epochs': 3\n}\n\n# Initialize model\nmodel = PatchTST(**params)\n\n# Load data\ndata_path = os.path.join('./dataset/weather/', 'weather.csv')\nmodel.load_data(data_path)\n\n# Train the model\nif is_training:\n    model.train()\n\n# Save the model\nmodel.save('./saved_models/')\n\n# Long term forecast\nforecast = model.long_term_forecast()\nprint(forecast)", "from TimeSeriesLibrary import PatchTST\n\n# Initialize model\nmodel = PatchTST(input_size=33, output_size=1, seq_len=96, label_len=48, pred_len=720,\ne_layers=2, d_layers=1, factor=3, enc_in=21, dec_in=21, c_out=21, des='Exp', itr=1, n_heads=8)\n\n# Path to data\ndata_path = './dataset/weather/weather.csv'\n\n# Load data\ndata = model.load_data(data_path)\n\n# Training\nmodel.train(data, epochs=3)", "python3 Time_Series_Library/main.py --task_name long_term_forecast --is_training 1 --root_path ./dataset/weather/ --data_path weather.csv --model_id weather_96_96 --model $model_name --data custom --features M --seq_len 96 --label_len 48 --pred_len 720 --e_layers 2 --d_layers 1 --factor 3 --enc_in 21 --dec_in 21 --c_out 21 --des 'Exp' --itr 1 --n_heads 4 --train_epochs 3", "--task_name long_term_forecast --is_training 1 --root_path ./dataset/weather/ --data_path weather.csv --model_id weather_96_96 --model PatchTST --data custom --features M --seq_len 96 --label_len 48 --pred_len 720 --e_layers 2 --d_layers 1 --factor 3 --enc_in 21 --dec_in 21 --c_out 21 --des 'Exp' --itr 1 --n_heads 4 --train_epochs 3"]}
{"id": 230, "github_id": 8, "output": ["import anomaly_detection_task\n\n# Assigning parameters\nparams = {\n'task_name': 'anomaly_detection',\n'is_training': 1,\n'root_path': './dataset/MSL',\n'model_id': 'MSL',\n'model': 'Crossformer',\n'data': 'MSL',\n'features': 'M',\n'seq_len': 100,\n'pred_len': 0,\n'd_model': 128,\n'd_ff': 128,\n'e_layers': 3,\n'enc_in': 55,\n'c_out': 55,\n'anomaly_ratio': 1,\n'batch_size': 128,\n'train_epochs': 10\n}\n\n# Running the task\nanomaly_detection_task.run(params)", "import os \n\nos.system(f'python main.py --task_name anomaly_detection --is_training 1 --root_path ./dataset/MSL --model_id MSL --model Crossformer --data MSL --features M --seq_len 100 --pred_len 0 --d_model 128 --d_ff 128 --e_layers 3 --enc_in 55 --c_out 55 --anomaly_ratio 1 --batch_size 128 --train_epochs 10')", "export CUDA_VISIBLE_DEVICES=0\n\npython -u run.py \\\n  --task_name anomaly_detection \\\n  --is_training 1 \\\n  --root_path ./dataset/MSL \\\n  --model_id MSL \\\n  --model Crossformer \\\n  --data MSL \\\n  --features M \\\n  --seq_len 100 \\\n  --pred_len 0 \\\n  --d_model 128 \\\n  --d_ff 128 \\\n  --e_layers 3 \\\n  --enc_in 55 \\\n  --c_out 55 \\\n  --anomaly_ratio 1 \\\n  --batch_size 128 \\\n  --train_epochs 10", "python train.py --task_name anomaly_detection --is_training True --root_path ./dataset/MSL --model_id MSL --model Crossformer --data MSL --features M --seq_len 100 --pred_len 0 --d_model 128 --d_ff 128 --e_layers 3 --enc_in 55 --c_out 55 --anomaly_ratio 1 --batch_size 128 --train_epochs 10", "\n# import the necessary libraries\nimport os\nimport pandas as pd\nimport torch\nfrom torch import nn, optim\nfrom torch.utils.data import DataLoader\nfrom Crossformer import Crossformer\nfrom dataset import MSLDataset\nfrom utils import evaluate_model\n\n# defining the necessary parameters\nparams = {\n    'task_name': 'anomaly_detection',\n    'is_training': 1,\n    'root_path': './dataset/MSL',\n    'model_id': 'MSL',\n    'model': 'Crossformer',\n    'data': 'MSL',\n    'features': 'M',\n    'seq_len': 100,\n    'pred_len': 0,\n    'd_model': 128,\n    'd_ff': 128,\n    'e_layers': 3,\n    'enc_in': 55,\n    'c_out': 55,\n    'anomaly_ratio': 1,\n    'batch_size': 128,\n    'train_epochs': 10\n}\n\n# Initialize Crossformer model\nmodel = Crossformer(params)\n\n# Load and preprocess the dataset\ndataset = MSLDataset(params)\n\n# Define dataloader\ndataloader = DataLoader(dataset, batch_size=params['batch_size'], shuffle=True)\n\n# Define loss function and optimizer\nloss_func = nn.MSELoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\n# Start training loop\nfor epoch in range(params['train_epochs']):\n    for i, batch in enumerate(dataloader):\n        inputs, labels = batch\n        optimizer.zero_grad()\n        outputs = model(inputs)\n        loss = loss_func(outputs, labels)\n        loss.backward()\n        optimizer.step()\n    print(f'Epoch {epoch+1}/{params['train_epochs']} Loss: {loss.item()}')\n\n# Evaluate the model\nevaluate_model(model, dataloader)"]}
{"id": 234, "github_id": 8, "output": ["from TimeSeriesLibrary import ShortTermForecastTask, CrossformerModel, EvaluatePerformance\n\n# define the task\ntask = ShortTermForecastTask(\n    task_name='My Short-term Forecast Task',\n    is_training=True,\n    root_path='/path/to/dataset',\n    seasonal_patterns='Daily',\n    model_id='model_1',\n    model=CrossformerModel,\n    data='M4',\n    features=['feature1', 'feature2'],\n    e_layers=2,\n    d_layers=2,\n    factor=2,\n    enc_in=300,\n    dec_in=300,\n    c_out=300,\n    batch_size=32,\n    d_model=512,\n    d_ff=2048,\n    top_k=1,\n    des='This is my Crossformer model for the M4 dataset.\n'\n    itr=100,\n    learning_rate=0.001,\n    loss='MSE'\n)\n\n# train the model\ntask.train()\n\n# evaluate the model\nperformance = EvaluatePerformance(task)\nperformance.compute()\n\n# print the performance metrics\nprint(performance.get_results())", "--task_name short_term_forecast --is_training 1 --root_path ./dataset/m4 --seasonal_patterns 'Daily' --model_id m4_Daily --model Crossformer --data m4 --features M --e_layers 2 --d_layers 1 --factor 3 --enc_in 1 --dec_in 1 --c_out 1 --batch_size 16 --d_model 32 --d_ff 32 --top_k 5 --des 'Exp' --itr 1 --learning_rate 0.001 --loss 'SMAPE'", "--task_name short_term_forecast --is_training 1 --root_path ./dataset/m4 --seasonal_patterns 'Daily' --model_id m4_Daily --model Crossformer --data m4 --features M --e_layers 2 --d_layers 1 --factor 3 --enc_in 1 --dec_in 1 --c_out 1 --batch_size 16 --d_model 32 --d_ff 32 --top_k 5 --des 'Exp' --itr 1 --learning_rate 0.001 --loss 'SMAPE'", "import argparse\nimport torch\n\nparser = argparse.ArgumentParser(description='TimesNet')\n\n# basic config\nparser.add_argument('--task_name', type=str, default='short_term_forecast')\nparser.add_argument('--is_training', type=int, default=1)\nparser.add_argument('--model_id', type=str, default='test')\nparser.add_argument('--model', type=str, default='Crossformer')\n\n# data loader\nparser.add_argument('--data', type=str, default='M4')\nparser.add_argument('--root_path', type=str, default='./data/M4/')\nparser.add_argument('--data_path', type=str, default='Daily.csv')\nparser.add_argument('--features', type=str, default='M')\nparser.add_argument('--target', type=str, default='OT')\nparser.add_argument('--freq', type=str, default='d')\nparser.add_argument('--checkpoints', type=str, default='./checkpoints/')\n\n# forecasting task\nparser.add_argument('--seq_len', type=int, default=96)\nparser.add_argument('--label_len', type=int, default=48)\nparser.add_argument('--pred_len', type=int, default=96)\nparser.add_argument('--seasonal_patterns', type=str, default='Daily')\nparser.add_argument('--inverse', action='store_true', default=False)\n\n# inputation task\nparser.add_argument('--mask_rate', type=float, default=0.25)\n\n# anomaly detection task\nparser.add_argument('--anomaly_ratio', type=float, default=0.25)\n\n# Append other arguments as in your original code, they were omitted here for brevity\n\nargs = parser.parse_args()\nargs.use_gpu = True if torch.cuda.is_available() and args.use_gpu else False\n\nif args.use_gpu and args.use_multi_gpu:\n    args.devices = args.devices.replace(' ', '')\n    device_ids = args.devices.split(',')\n    args.device_ids = [int(id_) for id_ in device_ids]\n    args.gpu = args.device_ids[0]\n", "Here is your Python code for the task:\n\nimport os\n\n# Define model name\nmodel_name = 'Crossformer'\n\n# Directory of dataset\nroot_path = './dataset/m4'\n\n# Function to execute the code\ndef execute_code():\n    cmd = f'python main.py --task_name short_term_forecast --is_training 1 --root_path {root_path} --seasonal_patterns 'Daily' --model_id m4_Daily --model {model_name} --data m4 --features M --e_layers 2 --d_layers 1 --factor 3 --enc_in 1 --dec_in 1 --c_out 1 --batch_size 16 --d_model 32 --d_ff 32 --top_k 5 --des 'Exp' --itr 1 --learning_rate 0.001 --loss 'SMAPE''\n    os.system(cmd)\n\n# Execute the functionexecute_code()\n"]}
{"id": 238, "github_id": 8, "output": ["import os\n\n# set the necessary parameters\nos.environ['MODEL'] = 'TimesNet'\nos.environ['DATASET'] = 'ETTh1'\nos.environ['IN_LEN'] = '0.375'\nos.environ['OUT_LEN'] = '0.375'\nos.environ['MASK_RATE'] = '0.375'\nos.environ['ENC_IN'] = 'M'\nos.environ['DEC_IN'] = 'M'\n\n# execute the script\ncmd = \"bash ./scripts/imputation/ETT_script/TimesNet_ETTh1.sh\"\nos.system(cmd)", "python main.py --content 'impute' --model_name 'TimesNet' --task_name 'imputation' --is_training 1 --root_path './dataset/ETT-small/' --data_path 'ETTh1.csv' --model_id 'ETTh1_mask_0.375' --mask_rate 0.375 --data 'ETTh1' --features 'M' --seq_len 96 --label_len 48 --pred_len 24 --e_layers 2 --d_layers 1 --d_head 8 --d_emb 32 --factor 3 --enc_in 7 --dec_in 7 --c_out 7 --batch_size 16 --d_model 512 --d_ff 2048 --des 'Exp' --itr 1 --top_k 3 --learning_rate 0.001", "python -u run.py \\\n  --task_name imputation \\\n  --is_training 1 \\\n  --root_path ./dataset/ETT-small/ \\\n  --data_path ETTh1.csv \\\n  --model_id ETTh1_mask_0.375 \\\n  --mask_rate 0.375 \\\n  --model TimesNet \\\n  --data ETTh1 \\\n  --features M \\\n  --seq_len 96 \\\n  --label_len 0 \\\n  --pred_len 0 \\\n  --e_layers 2 \\\n  --d_layers 1 \\\n  --factor 3 \\\n  --enc_in 7 \\\n  --dec_in 7 \\\n  --c_out 7 \\\n  --batch_size 16 \\\n  --d_model 16 \\\n  --d_ff 32 \\\n  --des 'Exp' \\\n  --itr 1 \\\n  --top_k 3 \\\n  --learning_rate 0.001", "from Time_Series_Library.timesnet import TimesNet\n\n# Establish parameters\nparams = {\n'task': 'imputation',\n'is_training': 1,\n'root_path': './dataset/ETT-small/',\n'data_path': 'ETTh1.csv',\n'model_id': 'ETTh1_mask_0.375',\n'mask_rate': 0.375,\n'model': 'TimesNet',\n'data': 'ETTh1',\n'features':'M',\n'seq_len':96,\n'label_len':0,\n'pred_len':0,\n'e_layers':2,\n'd_layers':1,\n'factor':3,\n'enc_in':7,\n'dec_in':7,\n'c_out':7,\n'batch_size':16,\n'd_model': 16,\n'd_ff':32,\n'des':'Exp',\n'itr':1,\n'top_k':3,\n'learning_rate':0.001\n}\n\n# Initialize model\nModel = TimesNet(params)\n\n# Train model\nModel.train()\n\n# Evaluate model\nModel.eval()", "python -u run.py \\\n  --task_name imputation \\\n  --is_training 1 \\\n  --root_path ./dataset/ETT-small/ \\\n  --data_path ETTh1.csv \\\n  --model_id ETTh1_mask_0.375 \\\n  --mask_rate 0.375 \\\n  --model TimesNet \\\n  --data ETTh1 \\\n  --features M \\\n  --seq_len 96 \\\n  --label_len 0 \\\n  --pred_len 0 \\\n  --e_layers 2 \\\n  --d_layers 1 \\\n  --factor 3 \\\n  --enc_in 7 \\\n  --dec_in 7 \\\n  --c_out 7 \\\n  --batch_size 16 \\\n  --d_model 16 \\\n  --d_ff 32 \\\n  --des 'Exp' \\\n  --itr 1 \\\n  --top_k 3 \\\n  --learning_rate 0.001"]}
{"id": 242, "github_id": 9, "output": ["from model.attention.ViP import WeightedPermuteMLP\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\n# Using pip mode\n\ninput = torch.randn(50, 49, 512)\nn_sa = WeightedPermuteMLP()\noutput = n_sa(input)\nprint(output.shape)", "import torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\n# Using pip method\nfrom fightingcv_attention.attention.ViP import *\n\nif __name__ == '__main__':\n    input=torch.randn(50,49,512)\n    sa = Mlp_ViT(dim=512)\n    output=sa(input)\n    print(output.shape)", "class AttentionLayers(nn.Module):\n    def __init__(self,in_features,hidden_features,out_features,act_layer=nn.GELU,drop=0.1):\n        super().__init__()\n        self.fc1=nn.Linear(in_features,hidden_features)\n        self.act=act_layer()\n        self.fc2=nn.Linear(hidden_features,out_features)\n        self.drop=nn.Dropout(drop)\n\n    def __init__(self,dim,seg_dim=8, qkv_bias=False, proj_drop=0.):\n        super().__init__()\n        self.seg_dim=seg_dim\n        self.mlp_c=nn.Linear(dim,dim,bias=qkv_bias)\n        self.mlp_h=nn.Linear(dim,dim,bias=qkv_bias)\n        self.mlp_w=nn.Linear(dim,dim,bias=qkv_bias)\n        self.reweighting=MLP(dim,dim//4,dim*3)\n        self.proj=nn.Linear(dim,dim)\n        self.proj_drop=nn.Dropout(proj_drop)\n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.act(x)\n        x = self.fc2(x)\n        x = self.drop(x)\n        return x\n\n    def forward(self, x):\n        x = self.mlp_c(x)\n        x = self.mlp_h(x)\n        x = self.mlp_w(x)\n        x = self.reweighting(x)\n        x = self.proj(x)\n        x = self.proj_drop(x)\n        return x", "class ViPAttention(nn.Module):\n    def __init__(self, in_features, hidden_features, out_features, act_layer=nn.GELU, drop=0.1 ,dim = 8, seg_dim=0, qkv_bias=False, proj_drop=0.1):\n        super(ViPAttention, self).__init__()\n\n        self.fc1=nn.Linear(in_features,hidden_features)\n        self.act=act_layer()\n        self.fc2=nn.Linear(hidden_features,out_features)\n        self.drop=nn.Dropout(drop)\n\n        self.seg_dim=seg_dim\n\n        self.mlp_c=nn.Linear(dim,dim,bias=qkv_bias)\n        self.mlp_h=nn.Linear(dim,dim,bias=qkv_bias)\n        self.mlp_w=nn.Linear(dim,dim,bias=qkv_bias)\n\n        self.reweighting=MLP(dim,dim//4,dim*3)\n\n        self.proj=nn.Linear(dim,dim)\n        self.proj_drop=nn.Dropout(proj_drop)\n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.act(x)\n        x = self.fc2(x)\n        x = self.drop(x)\n\n        #Add your own forward methods based on your own requirments\n\n        return x", "import torch\nfrom torch.nn import functional as F\n\n# Using pip method \n\nfrom fightingcv_attention.attention.ViP import WeightedPermuteMLP\n\nif __name__ == '__main__':\n    input=torch.randn(50,49,512)\n    sa = WeightedPermuteMLP(d_model=512)\n    output=sa(input)\n    print(output.shape)"]}
{"id": 246, "github_id": 9, "output": ["from model.attention.ParNetAttention import ParNetAttention\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\nif __name__ == '__main__':\n    input=torch.randn(50,512,7,7)\n    pna = ParNetAttention(channel=512)\n    output=pna(input)\n    print(output.shape) #50,512,7,7\n", "import torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\n# The necessary code to build and apply ParNetAttention is:\n\nclass ParNetAttention(nn.Module):\n    def __init__(self, d_model: int):\n        super().__init__()\n        self.inner_linear=nn.Conv2d(d_model//2,d_model,kernel_size=1)\n        self.outer_linear=nn.Conv1d(d_model,d_model,1)\n\n    def forward(self, x: torch.Tensor):\n        B,C,H,W=x.shape\n        x=x.view(B,C,H*W)\n        x=self.inner_linear(x)\n        x=self.outer_linear(x)\n        return x.view(B,C,H,W)\n\n# You can now use ParNetAttention in your model like any other pytorch layer\n# Remember to provide `d_model` argument when creating the layer (number of input features)\nattention = ParNetAttention(d_model=512)\n\n# An attention layer takes as input the feature map from previous layer\nprev_feature_map = torch.rand((1, 512, 49, 49))\n# It returns a feature map of the same size\nfeature_map = attention(prev_feature_map)\n# It should have the same size as the input\nassert feature_map.shape == (1, 512, 49, 49)", "from model.attention.ParNetAttention import ParNetAttention\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\ninput = torch.randn(1, 512, 7, 7)\nparnet = ParNetAttention(channel=512)\noutput=parnet(input)\nprint(output.shape)", "import torch\nfrom torch import nn\nfrom torch.nn import functional as F\nfrom model.attention.ParNetAttention import ParNetAttention\n\nif __name__ == '__main__':\n    input = torch.randn(50, 512, 7, 7)\n    pna = ParNetAttention(channel=512)\n    output = pna(input)\n    print(output.shape)  # output: (50, 512, 7, 7)", "import torch\nfrom torch import nn\nfrom torch.nn import functional as F\n# You can just write the code that is returned when the code is written.\n\nfrom model.attention.ParNetAttention import ParNetAttention\n\nif __name__ == '__main__':\n    input=torch.randn(50,49,512)\n    parnet = ParNetAttention(d_model=512)\n    output=parnet(input)\n    print(output.shape)  #[50, 49, 512]\n"]}
{"id": 250, "github_id": 9, "output": ["import torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\n# Use Polarized Self-Attention\n\ninput = torch.randn(50,49,512)\n\n# Use Polarized Self-Attention Module from fightingcv_attention library\nfrom fightingcv_attention.attention.PolarizedSelfAttention import PolarizedSelfAttention\n\n# Initialize Polarized Self Attention with model dimensions\nd_model = 512\npolarized_self_attention = PolarizedSelfAttention(d_model)\n\n# Compute the output\noutput = polarized_self_attention(input)\n\nprint(output.shape)\n\n# The output shape should be same as input", "from model.attention.PolarizedSelfAttention import PolarizedSelfAttention\nimport torch\nfrom torch import nn\n\ninp = torch.randn(50, 512, 7, 7)\npolarizedSelfAttention = PolarizedSelfAttention(in_channel=512)\noutput = polarizedSelfAttention(inp)\n\n# You can print the output shape using print(output.shape)", "import torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\nfrom model.attention.PolarizedSelfAttention import PolarizedSelfAttention\n\nif __name__ == '__main__':\n    input=torch.randn(50,49,512)\n    sa = PolarizedSelfAttention(d_model=512, d_k=512, d_v=512, h=8, num_patches=49)\n    output=sa(input, input, input)\n    print(output.shape)", "from model.attention.PolarizedSelfAttention import PolarizedSelfAttention\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\ninput=torch.randn(50,49,512)\npsa = PolarizedSelfAttention(d_model=512)\noutput=psa(input)\nprint(output.shape)", "from model.attention.PolarizedSelfAttention import PolarizedSelfAttention\nimport torch\n\ninput = torch.randn(50, 49, 512)\npolarized_self_attention = PolarizedSelfAttention(dim=512)\noutput = polarized_self_attention(input)\nprint(output.shape)"]}
{"id": 254, "github_id": 9, "output": ["from model.backbone.HATNet import HATNet\nimport torch\nfrom torch import nn\n\nif __name__ == '__main__':\n    input=torch.randn(1,3,224,224)\n    hat = HATNet(dims=[64, 128, 320, 512],image_size=224,patch_size=7,class_num=1000)\n    output=hat(input)\n    print(output.shape)", "import torch\nfrom model.backbone.HATNet import HATNet\nimport torch\nfrom torch import nn\n\nif __name__ == '__main__':\n    input=torch.randn(1,3,224,224)\n    hat = HATNet()\n    output=hat(input)\n    print(output.shape)", "from model.backbone.HATNet import HATNet\nimport torch\nfrom torch import nn\n\nif __name__ == '__main__':\n    input=torch.randn(1,3,224,224)\n    hat = HATNet(dims=[48, 96, 240, 384],image_size=14,patch_size=7,class_num=1000)\n    output=hat(input)\n    print(output.shape)", "class HATNet(nn.Module):\n    def __init__(self, img_size=224, in_chans=3, num_classes=1000, dims=[64, 128, 256, 512],\n                 head_dim=64, expansions=[4, 4, 6, 6], grid_sizes=[1, 1, 1, 1],\n                 ds_ratios=[8, 4, 2, 1], depths=[3, 4, 8, 3], drop_rate=0.,\n                 drop_path_rate=0., act_layer=nn.SiLU, kernel_sizes=[3, 3, 3, 3]):\n        super().__init__()\n        self.depths = depths\n        self.patch_embed = nn.Sequential(\n            nn.Conv2d(3, 16, 3, padding=1, stride=2),\n            nn.GroupNorm(1, 16, eps=1e-6),\n            act_layer(inplace=True),\n            nn.Conv2d(16, dims[0], 3, padding=1, stride=2),\n        )\n        \n        self.blocks = nn.ModuleList([])\n        for i in range(len(depths)):\n            for _ in range(depths[i]):\n                self.blocks.append(Block(dim=dims[i], head_dim=head_dim, grid_size=grid_sizes[i],\n                                         ds_ratio=ds_ratios[i], expansion=expansions[i],\n                                         drop=drop_rate, drop_path=drop_path_rate,\n                                         kernel_size=kernel_sizes[i], act_layer=act_layer))\n            if i < len(depths) - 1:\n                self.blocks.append(Downsample(dims[i], dims[i+1], kernel_size=kernel_sizes[i+1]))\n        \n    def forward(self, x):\n        x = self.patch_embed(x)\n        for block in self.blocks:\n            x = block(x)\n        return x", "from model.backbone.HATNet import HATNet\nimport torch\nfrom torch import nn\n\nif __name__ == '__main__':\n    input=torch.randn(1,3,224,224)\n    hat = HATNet(dims=[48, 96, 240, 384], num_heads=8, depths=[2, 2, 6, 3])\n    output=hat(input)\n    print(output.shape)"]}
{"id": 260, "github_id": 9, "output": ["import torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\n# Using the AFT model from the fighting cv library\nfrom fightingcv_attention.attention.AFT import *\n\ninput=torch.randn(50,49,512)\naft = AFT(d_model=512, n=49)\noutput=aft(input)\nprint(output.shape)", "from model.attention.AFT import AFT\n\ninput = torch.randn(50, 49, 512)\naft_block = AFT(dim=512, d_wf=1024)\noutput = aft_block(input)\nprint(output.shape)", "import torch\nfrom fightingcv_attention.attention.AFT import *\n\ninput = torch.randn(50,49,512)\nAFT_model = AFT(d_model=512,d_ffn=2048,s=10)\noutput = AFT_model(input)\nprint(output.shape)", "import torch\nfrom torch import nn\nfrom torch.nn import functional as F\nfrom fightingcv_attention.attention.AFT import AFT\n\nif __name__ == '__main__':\n    input=torch.randn(50,49,512)\n    aft = AFT(d_model=512, d_k=512, d_v=512)\n    output=aft(input)\n    print(output.shape)\n", "import torch\nfrom torch import nn\nfrom torch.nn import functional as F\nfrom fightingcv_attention.attention.AFT import *\n\ninput = torch.randn(50,49,512)\nmodel = AFT(d_model=512)\noutput = model(input)\nprint(output.shape)"]}
{"id": 264, "github_id": 9, "output": ["import torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\n#  Using pip Way\n\nfrom fightingcv_attention.attention.SimplifiedSelfAttention import *\n\nif __name__ == '__main__':\n    input=torch.randn(50,49,512)\n    ssa = SimplifiedScaledDotProductAttention(d_model=512, h=8)\n    output=ssa(input,input,input)\n    print(output.shape)", "import torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\n# Assuming that we have the FightingCV installed\nfrom fightingcv_attention.attention.SimplifiedSelfAttention import SimplifiedScaledDotProductAttention\n\nif __name__ == '__main__':\n    input=torch.randn(50,49,512)\n    ssa = SimplifiedScaledDotProductAttention(d_model=512, h=8)\n    output=ssa(input,input,input)\n    print(output.shape)", "import torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\n#  Using pip method\n\nfrom fightingcv_attention.attention.SimplifiedSelfAttention import *\n\nif __name__ == '__main__':\n    input=torch.randn(50,49,512)\n    ssa = SimplifiedScaledDotProductAttention(d_model=512, h=8)\n    output=ssa(input,input,input)\n    print(output.shape)", "from fightingcv_attention.attention.SimplifiedSelfAttention import SimplifiedScaledDotProductAttention\nimport torch\n\ninput = torch.randn(50,49,512)\nssa = SimplifiedScaledDotProductAttention(d_model=512, h=8)\noutput = ssa(input,input,input)\nprint(output.shape)", "from fightingcv_attention.attention.SimplifiedSelfAttention import SimplifiedScaledDotProductAttention\nimport torch\n\ninput=torch.randn(50,49,512)\n\n# create instance of the SimplifiedScaledDotProductAttention model\nssa = SimplifiedScaledDotProductAttention(d_model=512, h=8)\n\n# Apply model to the dataset\noutput=ssa(input,input,input)\nprint(output.shape)"]}
{"id": 268, "github_id": 9, "output": ["import torch\nfrom model.attention.ExternalAttention import ExternalAttention\n\ninput=torch.randn(50,49,512)\nea = ExternalAttention(d_model=512)\noutput=ea(input)\nprint(output.shape)", "from model.attention.ExternalAttention import ExternalAttention\n\ninput=torch.randn(50,49,512)\nea = ExternalAttention(d_model=512, S=8)\noutput=ea(input)\n\nprint(output.shape)\n", "import torch\nfrom torch import nn\nfrom torch.nn import functional as F\nfrom fightingcv_attention.attention.ExternalAttention import *\n\nif __name__ == '__main__':\n    input=torch.randn(50,49,512)\n    ea = ExternalAttention(d_model=512,S=8)\n    output=ea(input)\n    print(output.shape)", "from model.attention.ExternalAttention import ExternalAttention\n\nimport torch\n\nif __name__ == '__main__':\n    input=torch.randn(50,49,512)\n    ea = ExternalAttention(d_model=512,S=8)\n    output=ea(input)\n    print(output.shape)", "import torch\nfrom torch import nn\n\n# Your data input\ninput = torch.randn(50, 49, 512)\n\neat = ExternalAttention(d_model=512, S=8)\noutput = eat(input)\n\n# Display the output shape\nprint(f'The output shape is {output.shape}')"]}
{"id": 3, "github_id": 10, "output": ["python3 esrgan.py --epoch 0 --n_epochs 100 --dataset_name \"<DATASET_NAME>\" --batch_size 16 --lr 0.0001 --hr_height 128 --hr_width 128", "from keras.optimizers import Adam\nfrom keras.models import Sequential\nfrom keras.layers import BatchNormalization, Dense, Flatten, Conv2D, Conv2DTranspose, LeakyReLU, Reshape\n\n# Build Generator\nmodel = Sequential()\nmodel.add(Dense(128 * 32 * 32, activation='relu', input_dim=100))\nmodel.add(Reshape((32, 32, 128)))\nmodel.add(BatchNormalization())\nmodel.add(Conv2DTranspose(64, kernel_size=4, strides=2, padding='same'))\nmodel.add(LeakyReLU(alpha=0.2))\nmodel.add(BatchNormalization())\nmodel.add(Conv2DTranspose(1, kernel_size=4, strides=2, padding='same', activation='tanh'))\n\n# Compile model\nopt = Adam(lr=0.0001, beta_1=0.5)\nmodel.compile(loss='binary_crossentropy', optimizer=opt)\n\n# Prepare for training\niterations = 100\nbatch_size = 16\n\n# Train model\nfor i in range(iterations):\n    x_real, y_real = generate_real_samples(batch_size)\n    x_fake, y_fake = generate_fake_samples(model, batch_size)\n    model.train_on_batch(x_real, y_real)\n    model.train_on_batch(x_fake, y_fake)\n", "import argparse\nimport os\nfrom torchvision.utils import save_image\n\nimport torch\n\nfrom implementations.esrgan.models import Generator, Discriminator\nfrom implementations.esrgan.datasets import SRDataset\n\n\nparser = argparse.ArgumentParser()\nparser.add_argument('--train_hr_img_dir', type=str, required=True, help='High resolution image directory for training')\nparser.add_argument('--train_lr_img_dir', type=str, required=True, help='Low resolution image directory for training')\nparser.add_argument('--valid_hr_img_dir', type=str, required=True, help='High resolution image directory for validation')\nparser.add_argument('--valid_lr_img_dir', type=str, required=True, help='Low resolution image directory for validation')\n\nos.makedirs('images/training', exist_ok=True)\nos.makedirs('images/validation', exist_ok=True)\nos.makedirs('saved_models', exist_ok=True)\n\n# Hyperparameters\nbatch_size = 16\nimage_size = 128\nlr = 0.0001\nb1 = 0.9\nb2 = 0.999\nn_channels = 3\ndepth = 5 # Depth of residual blocks\nnum_epochs = 100\nnum_res_blocks = 23\nlambda_adv = 0.001\nlambda_pixel = 1.0\n\n\n# Initialize generator, discriminator and losses\n\n# Generator\nmodel_gen = Generator(depth)\n\nif torch.cuda.is_available():\n    model_gen = model_gen.cuda()\n\n# Discriminator\nmodel_disc = Discriminator(n_channels, depth)\n\nif torch.cuda.is_available():\n    model_disc = model_disc.cuda()\n\n# Losses\nloss_fn_vgg = torch.nn.MSELoss()\nloss_fn_vgg = loss_fn_vgg.cuda() if torch.cuda.is_available() else loss_fn_vgg\n\nloss_fn_adv = torch.nn.BCEWithLogitsLoss()\nloss_fn_adv = loss_fn_adv.cuda() if torch.cuda.is_available() else loss_fn_adv\n\nloss_fn_pixel = torch.nn.L1Loss()\nloss_fn_pixel = loss_fn_pixel.cuda() if torch.cuda.is_available() else loss_fn_pixel\n\n\n# Optimizers\nopt_gen = torch.optim.Adam(model_gen.parameters(), lr=lr, betas=(b1, b2))\nopt_disc = torch.optim.Adam(model_disc.parameters(), lr=lr, betas=(b1, b2))\n\n# Initialize dataloader for Super Resolution Dataset\ndataloader = torch.utils.data.DataLoader(\n    SRDataset(args.train_hr_img_dir, args.train_lr_img_dir),\n    batch_size=batch_size,\n    shuffle=True,\n)\n\nvalid_dataloader = torch.utils.data.DataLoader(\n    SRDataset(args.valid_hr_img_dir, args.valid_lr_img_dir),\n    batch_size=batch_size,\n)", "import torch\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets\nfrom torchvision import transforms\n\ntransform = transforms.Compose([\n    transforms.ToPILImage(),\n    transforms.Resize((128, 128)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.5], std=[0.5])\n])\n\ndataset_path = './data' # specify your dataset path\n\ntrain_dataset = datasets.ImageFolder(root=dataset_path, transform=transform)\ntool = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nbatch_size = 16\n\ntrain_loader = DataLoader(dataset=train_dataset,\n                          batch_size=batch_size,\n                          shuffle=True,\n                          num_workers=4)\n\nfrom models import Generator, Discriminator\nfrom utils import *\n\ngenerator = Generator()\ndiscriminator = Discriminator()\n\n# set your prefered learning rate\nlr = 0.0001\n\n# set the number of training epochs\nn_epochs = 100\n\nloss_func = torch.nn.MSELoss()\n\ng_optim = torch.optim.Adam(generator.parameters(), lr=lr)\nd_optim = torch.optim.Adam(discriminator.parameters(), lr=lr)\n\nfor epoch in range(1, n_epochs+1):\n    g_loss_sum = 0\n    d_loss_sum = 0\n    for real_img, _ in train_loader:\n        real_img = real_img.to(device)\n        z = torch.randn(batch_size, latent_dim).to(device)\n\n        fake_img = generator(z)\n\n        real_out = discriminator(real_img)\n        fake_out = discriminator(fake_img)\n\n        g_loss = loss_func(real_out, fake_out)\n        d_loss = loss_func(fake_out, real_out.detach())\n\n        g_loss_sum += g_loss.item()\n        d_loss_sum += d_loss.item()\n\n        g_optim.zero_grad()\n        g_loss.backward()\n        g_optim.step()\n\n        d_optim.zero_grad()\n        d_loss.backward()\n        d_optim.step()\n\n    print(f'Epoch [{epoch}/{n_epochs}], g_loss: {g_loss_sum/len(train_loader):.4f}, d_loss: {d_loss_sum/len(train_loader):.4f}')\n", "import argparse\nimport os\nimport numpy as np\nimport math\nimport itertools\nimport time\nimport datetime\nimport sys\n\nimport torchvision.transforms as transforms\nfrom torchvision.utils import save_image, make_grid\n\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets\nfrom torch.autograd import Variable\n\nfrom models import *\nfrom datasets import *\n\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch\n\n# Set the parameters\nos.makedirs('images', exist_ok=True)\n\nparser = argparse.ArgumentParser()\nparser.add_argument('--epoch', type=int, default=0, help='Starting epoch number')\nparser.add_argument('--n_epochs', type=int, default=100, help='Number of epochs of training')\nparser.add_argument('--dataset_name', type=str, default='img_align_celeba', help='Name of the dataset')\nparser.add_argument('--batch_size', type=int, default=16, help='Size of the batches')\nparser.add_argument('--lr', type=float, default=0.0001, help='Adam Learning rate')\nparser.add_argument('--b1', type=float, default=0.9, help='Adam first beta value')\nparser.add_argument('--b2', type=float, default=0.999, help='Adam second beta value')\nparser.add_argument('--decay_epoch', type=int, default=100, help='Epoch from which to start lr decay')\nparser.add_argument('--n_cpu', type=int, default=8, help='Number of cpu threads to use during batch generation')\nparser.add_argument('--hr_height', type=int, default=128, help='High res. image height')\nparser.add_argument('--hr_width', type=int, default=128, help='High res. image width')\nparser.add_argument('--channels', type=int, default=3, help='Number of image channels')\nparser.add_argument('--sample_interval', type=int, default=100, help='Interval between saving image samples')\nopt = parser.parse_args()\n"]}
{"id": 32, "github_id": 10, "output": ["import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.autograd import Variable\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets, transforms\n\nfrom context_encoder import ContextEncoder\nfrom context_encoder import ContextEncoderDataset\n\n# Parameters\nlr = 0.0001\nepochs = 100\nbatch_size = 16\nimage_size = 128\n\n# Image processing\ntransform = transforms.Compose([\n    transforms.Scale(image_size),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)),\n])\n\n# Data loader\ntrain_data = ContextEncoderDataset(datasets.MNIST('data', train=True, download=True, transform=transform),\n                                   image_size=image_size)\ntrain_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n\ntest_data = ContextEncoderDataset(datasets.MNIST('data', train=False, download=True, transform=transform),\n                                  image_size=image_size)\ntest_loader = DataLoader(test_data, batch_size=batch_size, shuffle=True)\n\n# Models\nG = ContextEncoder(in_channels=3, out_channels=3)\nD = ContextEncoder(in_channels=3, out_channels=1)\n\n# Optimizers\nG_optimizer = optim.Adam(G.parameters(), lr=lr)\nD_optimizer = optim.Adam(D.parameters(), lr=lr)\n\n# Training\nfor epoch in range(epochs):\n    for real_images, _ in train_loader:\n        # Train D\n        D.zero_grad()\n        outputs = D(real_images)\n        real_loss = nn.MSELoss()(outputs, real_images)\n        real_loss.backward()\n        D_optimizer.step()\n\n        # Train G\n        G.zero_grad()\n        fake_images = G(real_images)\n        outputs = D(fake_images.detach())\n        fake_loss = nn.MSELoss()(outputs, real_images)\n        fake_loss.backward()\n        G_optimizer.step()\n\n    print('Epoch [{}/{}], d_loss: {:.6f}, g_loss: {:.6f}'.format(epoch, epochs, real_loss.data[0], fake_loss.data[0]))", "import torch\nfrom torch.utils.data import DataLoader\nfrom torchvision import transforms\nfrom torchvision import datasets\nfrom torch.autograd import Variable\nimport torch.nn as nn\nfrom models import Generator, Discriminator\n\n\n# Hyper-parameters\nimage_size = 128\nlearning_rate = 0.0001\nnum_epochs = 100\nbatch_size = 16\nlatent_dim = 100\n\n# Device configuration\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# Image processing\ntransform = transforms.Compose([\n    transforms.Resize(image_size),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))])\n\n# MNIST dataset (images and labels)\ndataset = datasets.MNIST(root='../../data',\n                          transform=transform,\n                          download=True)\n\n# Data loader\ndata_loader = DataLoader(dataset=dataset,\n                         batch_size=batch_size,\n                         shuffle=True)\n\n# Model initialization\nD = Discriminator().to(device)\nG = Generator(latent_dim).to(device)\n\n# Optimizers\nD_optimizer = torch.optim.Adam(D.parameters(), lr=learning_rate)\nG_optimizer = torch.optim.Adam(G.parameters(), lr=learning_rate)\n\n# Loss function\ncriterion = nn.BCELoss()\n\n# Training\nfor epoch in range(num_epochs):\n    for i, (images, _) in enumerate(data_loader):\n        batch_size = images.size(0)\n        images = images.to(device)\n\n        # Create the labels which are later used as input for the BCE loss\n        real_labels = torch.ones(batch_size).to(device)\n        fake_labels = torch.zeros(batch_size).to(device)\n\n        # ================================================================== #\n        #                      Train the discriminator                       #\n        # ================================================================== #\n\n        # Compute BCE loss with real images\n        outputs = D(images)\n        d_loss_real = criterion(outputs, real_labels)\n        real_score = outputs\n\n        # Compute BCE loss with fake images\n        z = torch.randn(batch_size, latent_dim).to(device)\n        fake_images = G(z)\n        outputs = D(fake_images)\n        d_loss_fake = criterion(outputs, fake_labels)\n        fake_score = outputs\n\n        # Backprop and optimize\n        d_loss = d_loss_real + d_loss_fake\n        D.zero_grad()\n        d_loss.backward()\n        D_optimizer.step()\n\n        # ================================================================== #\n        #                        Train the generator                         #\n        # ================================================================== #\n\n        # Compute loss with fake images\n        z = torch.randn(batch_size, latent_dim).to(device)\n        fake_images = G(z)\n        outputs = D(fake_images)\n\n        # We train G to maximize log(D(G(z)) instead of minimizing log(1-D(G(z)))\n        # For the reason, see the last paragraph of section 3. https://arxiv.org/pdf/1406.2661.pdf\n        g_loss = criterion(outputs, real_labels)\n\n        # Backprop and optimize\n        D.zero_grad()\n        G.zero_grad()\n        g_loss.backward()\n        G_optimizer.step()\n\n    print(f'Epoch [{epoch}/{num_epochs}], d_loss: {d_loss.item():.4f}, g_loss: {g_loss.item():.4f}')", "```python\n# import necessary libraries\nimport torch\nimport torch.nn as nn\nimport torchvision\nimport torchvision.transforms as transforms\nfrom torch.utils.data import DataLoader\n\n# set parameters\nlr = 0.0001\nnum_epochs = 100\nbatch_size = 16\nimage_size = 128\n\n# define the generator and discriminator models\n\n# for the sake of this example, we'll use simple feed-forward neural networks\n# in reality you would want to use a more sophisticated architecture\n\nclass Generator(nn.Module):\n    def __init__(self):\n        super(Generator, self).__init__()\n        # define layers\n\n    def forward(self, x):\n        # define forward pass\n        return x\n\nclass Discriminator(nn.Module):\n    def __init__(self):\n        super(Discriminator, self).__init__()\n        # define layers\n\n    def forward(self, x):\n        # define forward pass\n        return x\n\n# initiate the generator and discriminator\nG = Generator()\nD = Discriminator()\n\n# set loss criterion and optimizers\ncriterion = nn.BCELoss()\noptimizer_G = torch.optim.Adam(G.parameters(), lr=lr)\noptimizer_D = torch.optim.Adam(D.parameters(), lr=lr)\n\n# prepare data\n# we'll use the MNIST dataset for this example\ntransform = transforms.Compose([transforms.Resize(image_size), transforms.ToTensor()])\ndataset = torchvision.datasets.MNIST(root='.', train=True, download=True, transform=transform)\ndataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n\n# start training\nfor epoch in range(num_epochs):\n    for i, (real_images, _) in enumerate(dataloader):\n        # train generator and discriminator\n        # make sure to use the correct loss criterion and update the models' parameters\n```", "import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets, transforms\n\n# Hyperparameters\ng_channels = 64\nd_channels = 64\nnum_epochs = 100\nlearning_rate = 0.0001\nbatch_size = 16\nsample_interval = 500\nimg_size = 128\nlatent_dim = 100\nimg_shape = (3, img_size, img_size)\n\n# Image transformations\ntransform = transforms.Compose([\n    transforms.Resize(img_size),\n    transforms.ToTensor(),\n    transforms.Normalize([0.5], [0.5])\n])\n\n# Data loader\ntrain_data = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\ntrain_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n\n# Device configuration\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# Implement your Context-Conditional GAN model here using nn.Module.\n\n# Model creation\nmodel = ContextConditionalGAN()\nmodel = model.to(device)\n\n# Loss and optimizer\ncriterion = nn.BCELoss()\noptimizer = optim.Adam(model.parameters(), lr=learning_rate)\n\n# Training loop\nfor epoch in range(num_epochs):\n    for i, (images, _) in enumerate(train_loader):\n        images = images.to(device)\n        \n        # Forward pass\n        outputs = model(images)\n        loss = criterion(outputs, images)\n        \n        # Backward and optimize\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        \n        # Printing information\n        if (i+1) % 100 == 0:\n            print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'\n                  .format(epoch+1, num_epochs, i+1, len(train_loader), loss.item()))", "import torch\nimport torchvision\nfrom torchvision.transforms import transforms\nfrom models import NLayerDiscriminator, Pix2PixHDGenerator\n\n# Hyperparameters\nlearning_rate = 0.0001\nnum_epochs = 100\nimage_size = 128\nbatch_size = 16\n\n# Device setup\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# Load and preprocess dataset\ntransform = transforms.Compose([transforms.Resize(image_size), \n                                transforms.ToTensor(), \n                                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\ndataset = torchvision.datasets.ImageFolder(root='/path/to/your/dataset/', transform=transform)\ndataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n\n# Initialize models\nG = Pix2PixHDGenerator().to(device)\nD = NLayerDiscriminator().to(device)\n\n# Initialize loss function (BCE Loss for binary classification) and optimizers\ncriterion = torch.nn.BCELoss()\noptimizer_G = torch.optim.Adam(G.parameters(), lr=learning_rate, betas=(0.5, 0.999))\noptimizer_D = torch.optim.Adam(D.parameters(), lr=learning_rate, betas=(0.5, 0.999))\n\n# Training loop\nfor epoch in range(num_epochs):\n    for i, (real_images, _) in enumerate(dataloader):\n        real_images = real_images.to(device)\n\n        # Train Discriminator\n        D.zero_grad()\n        output = D(real_images)\n        real_loss = criterion(output, torch.ones_like(output))\n        fake_images = G(real_images)\n        output = D(fake_images.detach())\n        fake_loss = criterion(output, torch.zeros_like(output))\n        d_loss = real_loss + fake_loss\n        d_loss.backward()\n        optimizer_D.step()\n\n        # Train Generator\n        G.zero_grad()\n        output = D(fake_images)\n        g_loss = criterion(output, torch.ones_like(output))\n        g_loss.backward()\n        optimizer_G.step()\n\n        if (i+1) % 500 == 0:\n            print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(dataloader)}], D_Loss: {d_loss.item()}, G_Loss: {g_loss.item()}')\n\n    # Save generated images every epoch\n    torchvision.utils.save_image(fake_images, f'generated_{epoch+1}.png')"]}
{"id": 6, "github_id": 10, "output": ["import torch\nimport torch.nn as nn\nimport torchvision.transforms as transforms\nimport torchvision.datasets as dsets\nimport torch.optim as optim\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Hyperparameters\nlearning_rate = 0.0001\nnum_epochs = 100\nbatch_size = 16\nimage_size = 128\n\n# Image preprocessing modules\ntransform = transforms.Compose([\n    transforms.Resize(image_size),\n    transforms.CenterCrop(image_size),\n    transforms.ToTensor(),\n    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n\n# Load Data (Insert appropriate directory)\ndata = dsets.ImageFolder(root='Your Data Directory', transform=transform)\n\ndataloader = torch.utils.data.DataLoader(dataset=data, batch_size=batch_size, shuffle=True)\n\nclass Generator(nn.Module):\n    # Generator Desgin\n\nclass Discriminator(nn.Module):\n    # Discriminator Desgin\n\nG = Generator()\nD = Discriminator()\n\n# Adam Optimizer\nG_optimizer = optim.Adam(G.parameters(), lr= learning_rate)\nD_optimizer = optim.Adam(D.parameters(), lr= learning_rate)\n\n# Binary Cross Entropy Loss\ncriterion = nn.BCELoss()\n\n# Training Loop\nfor epoch in range(num_epochs):\n    for i, (images, _) in enumerate(dataloader):\n        # Discriminator Training\n        # Generator Training\n\n        if (i+1) % 100 == 0:\n            print('Epoch [%d/%d], Step[%d/%d], d_loss: %.4f, g_loss: %.4f' %\n                  (epoch, num_epochs, i+1, len(data)//batch_size, d_loss.data, g_loss.data))", "import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torchvision.transforms import Compose, Resize, Normalize, ToTensor\nfrom torchvision.datasets import ImageFolder\nfrom torchvision.utils import make_grid\nfrom torch.utils.data import DataLoader\n\nfrom implementations.dcgan.dcgan import Generator, Discriminator\n\n# Parameters\nlatent_dim = 100\nlearning_rate = 0.0001\nbatch_size = 16\nimage_size = 128\nchannels = 3\nepochs = 100\n\n# Create the generator and discriminator\nnetG = Generator(latent_dim, channels).to(device)\nnetD = Discriminator(channels).to(device)\n\n# Initialize BCELoss function\ncriterion = nn.BCELoss()\n\n# Create batch of latent vectors that we will use to visualize\n# the progression of the generator\nfixed_noise = torch.randn(64, latent_dim, 1, 1, device=device)\n\n# Setup Adam optimizers for G and D\noptimizerD = optim.Adam(netD.parameters(), lr=learning_rate, betas=(0.5, 0.999))\noptimizerG = optim.Adam(netG.parameters(), lr=learning_rate, betas=(0.5, 0.999))\n\n# Training Loop\n\n# Lists to keep track of progress\nimg_list = []\nG_losses = []\nD_losses = []\niters = 0\n\nprint(\"Starting Training Loop...\")\n# For each epoch\nfor epoch in range(epochs):\n    # For each batch in the dataloader\n    for i, data in enumerate(dataloader, 0):\n        # Format batch\n        real_cpu = data[0].to(device)\n        b_size = real_cpu.size(0)\n        label = torch.full((b_size,), real_label, device=device)\n        # ---------------------\n        # Train Discriminator: maximize log(D(x)) + log(1 - D(G(z)))\n        # ---------------------\n        ## Train with all-real batch\n        # Forward pass real batch through D\n        output = netD(real_cpu).view(-1)\n        # Calculate loss on all-real batch\n        errD_real = criterion(output, label)\n        # Calculate gradients for D in backward pass\n        errD_real.backward()\n        D_x = output.mean().item()\n\n        ## Train with all-fake batch\n        # Generate batch of latent vectors\n        noise = torch.randn(b_size, latent_dim, 1, 1, device=device)\n        # Generate fake image batch with G\n        fake = netG(noise)\n        label.fill_(fake_label)\n        # Classify all fake batch with D\n        output = netD(fake.detach()).view(-1)\n        # Calculate D's loss on the all-fake batch\n        errD_fake = criterion(output, label)\n        # Calculate the gradients for this batch\n        errD_fake.backward()\n        D_G_z1 = output.mean().item()\n        # Add the gradients from the all-real and all-fake batches\n        errD = errD_real + errD_fake\n        # Update D\n        optimizerD.step()\n\n        # -----------------\n        # Train Generator: maximize log(D(G(z)))\n        # -----------------\n        netG.zero_grad()\n        label.fill_(real_label)  # fake labels are real for generator cost\n        # Since we just updated D, perform another forward pass of all-fake batch through D\n        output = netD(fake).view(-1)\n        # Calculate G's loss based on this output\n        errG = criterion(output, label)\n        # Calculate gradients for G\n        errG.backward()\n        D_G_z2 = output.mean().item()\n        # Update G\n        optimizerG.step()\n\n        # Output training stats\n        if i % 50 == 0:\n            print('[%d/%d][%d/%d]\\tLoss_D: %.4f\\tLoss_G: %.4f\\tD(x): %.4f\\tD(G(z)): %.4f / %.4f'\n                  % (epoch, epochs, i, len(dataloader),\n                     errD.item(), errG.item(), D_x, D_G_z1, D_G_z2))\n\n        # Save Losses for plotting later\n        G_losses.append(errG.item())\n        D_losses.append(errD.item())\n\n        # Check how the generator is doing by saving G's output on fixed_noise\n        if (iters % 500 == 0) or ((epoch == epochs-1) and (i == len(dataloader)-1)):\n            with torch.no_grad():\n                fake = netG(fixed_noise).detach().cpu()\n            img_list.append(make_grid(fake, padding=2, normalize=True))\n\n        iters += 1", "from implementations.dcgan.dcgan import DCGAN \n dcgan = DCGAN(img_size=128, lr=0.0001, b1=0.5, b2=0.999, batch_size=16, n_cpu=8, latent_dim=100, n_epochs=100, sample_interval=400)", "import torch\nimport torchvision\nimport torchvision.transforms as transforms\nfrom torch.autograd import Variable\nfrom torch import nn, optim\n\n# Hyperparameters\nlr = 0.0001\nbatch_size = 16\nimage_size = 128\nepochs = 100\n\n# Image preprocessing\ntransform = transforms.Compose([\n    transforms.Resize(image_size),\n    transforms.ToTensor(),\n    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n])\n\n# Load the dataset\ntrainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\ntrainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True)\n\n# Define the model\nclass Generator(nn.Module):\n    # ... (Implement here)\n    pass\n\nclass Discriminator(nn.Module):\n    # ... (Implement here)\n    pass\n\n# Initialize the model\nG = Generator()\nD = Discriminator()\n\n# Set the optimizer\noptimizer_G = optim.Adam(G.parameters(), lr=lr)\noptimizer_D = optim.Adam(D.parameters(), lr=lr)\n\n# Training\nfor epoch in range(epochs):\n    for i, data in enumerate(trainloader, 0):\n        # Implement here...", "import torch\nimport torchvision\nimport torchvision.transforms as transforms\nfrom torch.autograd import Variable\nimport torch.nn as nn\nimport torch.optim as optim\n\n# Hyperparameters\nbatch_size = 16\nimage_size = 128\nlr = 0.0001\nepochs = 100\n\n# Image preprocessing\ntransform = transforms.Compose([\n    transforms.Resize(image_size),\n    transforms.ToTensor(),\n    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n])\n\n# Load the dataset\n# Assuming you are using MNIST, please replace with your own dataset if not.\ndataset = torchvision.datasets.MNIST(root='../../data/',\n                                      train=True,\n                                      transform=transform,\n                                      download=True)\ndataloader = torch.utils.data.DataLoader(dataset=dataset,\n                                         batch_size=batch_size,\n                                         shuffle=True)\n\nclass Generator(nn.Module):\n    def __init__(self):\n        # Define your generator architecture here\n\n    def forward(self, x):\n        # Define the forward pass here\n\n\n# You would need a similar class definition for your discriminator\n\nclass Discriminator(nn.Module):\n    def __init__(self):\n        # Define your discriminator architecture here\n\n    def forward(self, x):\n        # Define the forward pass here\n\n\n# Initialize models\nG = Generator()\nD = Discriminator()\n\n# Convert the model to cuda\nG.cuda()\nD.cuda()\n\n# Define your loss function\ncriterion = nn.BCELoss()\n\noptimizerG = optim.Adam(G.parameters(), lr=lr)\noptimizerD = optim.Adam(D.parameters(), lr=lr)\n\nfor epoch in range(epochs):\n    for i, (images, _) in enumerate(dataloader):\n        # One training step goes here"]}
{"id": 11, "github_id": 10, "output": ["import torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\nfrom torchvision.utils import save_image\n\n# Initialize generator and discriminator\nG = generator()\nD = discriminator()\n\n# Binary cross entropy loss and optimizer\ncriterion = nn.BCELoss()\ng_optimizer = torch.optim.Adam(G.parameters(), lr=0.0001)\nd_optimizer = torch.optim.Adam(D.parameters(), lr=0.0001)\n\n# Training\nfor epoch in range(100):\n    for i, (imgs, _) in enumerate(dataloader):\n\n        # Ground truths\n        real = Variable(torch.ones(imgs.size(0), 1)).to(device)\n        fake = Variable(torch.zeros(imgs.size(0), 1)).to(device)\n\n        # Configure input\n        real_imgs = Variable(imgs.type(Tensor)).to(device)\n\n        # -----------------\n        #  Train Generator\n        # -----------------\n\n        g_optimizer.zero_grad()\n\n        # Generate a batch of images\n        gen_imgs = G(z)\n\n        # Loss measures generator's ability to fool the discriminator\n        g_loss = criterion(D(gen_imgs), real)\n\n        g_loss.backward()\n        g_optimizer.step()\n\n        # ---------------------\n        #  Train Discriminator\n        # ---------------------\n\n        d_optimizer.zero_grad()\n\n        # Measure discriminator's ability to classify real from generated samples\n        real_loss = criterion(D(real_imgs), real)\n        fake_loss = criterion(D(gen_imgs.detach()), fake)\n        d_loss = (real_loss + fake_loss) / 2\n\n        d_loss.backward()\n        d_optimizer.step()\n\n    print(f'Epoch: {epoch}/{100}, Discriminator Loss: {d_loss}, Generator Loss: {g_loss}')", "import torch\nfrom torchvision import transforms\nfrom torchvision.utils import save_image\n\nfrom datasets import ImageDataset\nfrom models import Generator\n\n# Training configurations\nlr = 0.0001\nbatch_size = 16\nimage_size = 128\nn_epochs = 100\nlatent_dim = 100\nchannels = 3\n\n# Create dataloader\ntransforms_ = [transforms.Resize((image_size, image_size)), transforms.ToTensor(), transforms.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5))]\ndataloader = torch.utils.data.DataLoader(ImageDataset('', transforms_=transforms_), batch_size=batch_size, shuffle=True)\n\n# Create generator\nnetG = Generator(input_dim=latent_dim, output_channels=channels)\n\noptimizer_G = torch.optim.Adam(netG.parameters(), lr=lr)\n\nTensor = torch.cuda.FloatTensor if torch.cuda.is_available() else torch.Tensor\n\nfor epoch in range(n_epochs):\n    for i, imgs in enumerate(dataloader):\n\n        # Adversarial ground truths\n        valid = Variable(Tensor(imgs.size(0), 1).fill_(1.0), requires_grad=False)\n        fake = Variable(Tensor(imgs.size(0), 1).fill_(0.0), requires_grad=False)\n\n        # Configure input\n        real_imgs = Variable(imgs.type(Tensor))\n\n        # -----------------\n        #  Train Generator\n        # -----------------\n\n        optimizer_G.zero_grad()\n\n        # Sample noise and labels as generator input\n        z = Variable(Tensor(np.random.normal(0, 1, (imgs.shape[0], latent_dim))))\n\n        # Generate a batch of images\n        gen_imgs = netG(z)\n\n        # Loss measures generator's ability to trick the discriminator\n        g_loss = adversarial_loss(netD(gen_imgs), valid)\n\n        g_loss.backward()\n        optimizer_G.step()\n\n        print('[Epoch %d/%d] [Batch %d/%d] [G loss: %f]' % (epoch, n_epochs, i, len(dataloader), g_loss.item()))\n\n        batches_done = epoch * len(dataloader) + i\n        if batches_done % sample_interval == 0:\n            save_image(gen_imgs.data[:25], 'images/%d.png' % batches_done, nrow=5, normalize=True)", "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.autograd import Variable\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets, transforms, utils\n\n# Variables\nlr = 0.0001\nbatch_size = 16\nimage_size = 128\nn_epochs = 100\n\ntransform = transforms.Compose([\n    transforms.Resize(image_size),\n    transforms.ToTensor(),\n    transforms.Normalize([0.5], [0.5])\n])\n\n# dataset & dataloader\ndataset = datasets.MNIST('path_to_your_dataset', download=True, transform=transform) # Note: replace 'path_to_your_dataset' with the real path to your dataset\nloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n\n# define your InfoGAN model here \n# ...\n\n# optimizer\noptimizer = optim.Adam(model.parameters(), lr=lr)\n\n# training loop\nfor epoch in range(n_epochs):\n    for i, (x, _) in enumerate(loader):\n        # train your InfoGAN model here\n        # ...\n        \npass", "from implementations.infogan.infogan import InfoGAN\n\ninfoGAN = InfoGAN()\ninfoGAN.lr = 0.0001\ninfoGAN.n_epochs = 100\ninfoGAN.batch_size = 16\ninfoGAN.img_size = 128\ninfoGAN.train()", "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.autograd as autograd\nfrom torchvision.models import vgg19\n\nfrom datasets import *\n\n# Image processing\ntransform = transforms.Compose([transforms.Resize((128, 128)),\ntransforms.ToTensor(),\ntransforms.Normalize(mean=(0.5,), std=(0.5,))])\n\n# InfoGAN model class\nclass InfoGAN(nn.Module):\n    def __init__(self, image_size=128, latent_dim=16, kl_coeff=1.0):\n        super(InfoGAN, self).__init__()\n        self.image_size = image_size\n        self.latent_dim = latent_dim\n\n        self.generator = Generator(latent_dim, image_size)\n        self.discriminator = Discriminator(image_size)\n        self.Q = Q(image_size)\n\n        self.kl_coeff = kl_coeff\n\n    def forward(self, z):\n        gen_images = self.generator(z)\n        disc_logits = self.discriminator(gen_images)\n        q_logits = self.Q(gen_images)\n        return gen_images, disc_logits, q_logits\n\n# Loss function\ndef loss_function(gen_images, real_images, disc_logits, disc_real_logits, q_logits, target):\n    G_loss = F.binary_cross_entropy_with_logits(disc_logits, torch.ones_like(disc_logits))\n    D_loss = (F.binary_cross_entropy_with_logits(disc_real_logits, torch.ones_like(disc_real_logits)) +\n              F.binary_cross_entropy_with_logits(disc_logits, torch.zeros_like(disc_logits))) / 2\n    info_loss = F.binary_cross_entropy_with_logits(q_logits, target)\n    return D_loss, G_loss + kl_coeff * info_loss\n\n# Training function\ndef train(model, dataloader, num_epochs=100, lr=0.0001):\n    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n\n    real_images = torch.ones(batch_size, 1, 128, 128)\n    for epoch in range(num_epochs):\n        for i, (images, _) in enumerate(dataloader):\n            real_images.copy_(images)\n            z = torch.randn(batch_size, model.latent_dim)\n\n            optimizer.zero_grad()\n\n            gen_images, disc_logits, q_logits = model(z)\n            D_loss, G_loss = loss_function(gen_images, real_images, disc_logits, disc_real_logits, q_logits, target)\n\n            D_loss.backward()\n            optimizer.step()\n\n        if epoch % 10 == 0:\n            print ('Epoch [{}/{}], Loss D: {:.4f}, Loss G: {:.4f}'\n                   .format(epoch, num_epochs, D_loss.item(), G_loss.item()))\n\n\n# load datasets\n# Please replace with your own dataset path\ndataset_path = \"YOUR_DATASET_PATH\"\ndataloader = torch.utils.data.DataLoader(datasets.ImageFolder(dataset_path, transform=transform), batch_size=16, shuffle=True)\n\n# Initialize model\nmodel = InfoGAN()\n\n# Train model\ntrain(model, dataloader, num_epochs=100, lr=0.0001)"]}
{"id": 23, "github_id": 10, "output": ["import torch\nimport torchvision.transforms as transforms\nfrom torch.autograd import Variable\nimport numpy as np\nfrom implementations.wgan_div import *\nimport glob\n\n# Image processing\ntransform = transforms.Compose([\n    transforms.Resize((128, 128)),\n    transforms.ToTensor(),\n]\n# Load data\nimage_list = glob.glob('path_to_images/*.jpg')  # replace path_to_images with your path\n\n# Convert to tensor and normalize\nimage_data = []\nfor img in image_list:\n    image = Image.open(img)\n    image = transform(image)\n    image_data.append(image)\n\nimages = torch.stack(image_data)\n\n# Hyperparameters\nlr = 0.0001\nepochs = 100\nbatch_size = 16\nlatent_dim = 100\nimg_size = 128\nchannels = 3\nsample_interval = 400\n\n# Initialize generator and discriminator\ngenerator = Generator(latent_dim, channels)\ndiscriminator = Discriminator(channels)\n\n# Loss functions\nadversarial_loss = torch.nn.BCELoss()\npixelwise_loss = torch.nn.L1Loss()\n\nif torch.cuda.is_available():\n    generator.cuda()\n    discriminator.cuda()\n    adversarial_loss.cuda()\n    pixelwise_loss.cuda()\n    images = images.cuda()\n\n# Configure data loader\nTensor = torch.cuda.FloatTensor if torch.cuda.is_available() else torch.FloatTensor\ndataloader = torch.utils.data.DataLoader(\n    images,\n    batch_size=batch_size,\n    shuffle=True,\n)\n\n# Optimizers\noptimizer_G = torch.optim.Adam(generator.parameters(), lr=lr, betas=(0.5, 0.999))\noptimizer_D = torch.optim.Adam(discriminator.parameters(), lr=lr, betas=(0.5, 0.999))\n\nfor epoch in range(epochs):\n    for i, imgs in enumerate(dataloader):\n\n        # Adversarial ground truths\n        valid = Variable(Tensor(imgs.size(0), 1).fill_(1.0), requires_grad=False)\n        fake = Variable(Tensor(imgs.size(0), 1).fill_(0.0), requires_grad=False)\n\n        # Configure input\n        real_imgs = Variable(imgs.type(Tensor))\n\n        # -----------------\n        #  Train Generator\n        # -----------------\n\n        optimizer_G.zero_grad()\n\n        # Sample noise as generator input\n        z = Variable(Tensor(np.random.normal(0, 1, (imgs.shape[0], latent_dim))))\n\n        # Generate a batch of images\n        gen_imgs = generator(z)\n\n        # Loss measures generator's ability to fool the discriminator\n        g_loss = adversarial_loss(discriminator(gen_imgs), valid)\n\n        g_loss.backward()\n        optimizer_G.step()\n\n        # ---------------------\n        #  Train Discriminator\n        # ---------------------\n\n        optimizer_D.zero_grad()\n\n        # Measure discriminator's ability to classify real from generated samples\n        real_loss = adversarial_loss(discriminator(real_imgs), valid)\n        fake_loss = adversarial_loss(discriminator(gen_imgs.detach()), fake)\n        d_loss = (real_loss + fake_loss) / 2\n\n        d_loss.backward()\n        optimizer_D.step()\n\n        print('[Epoch %d/%d] [Batch %d/%d] [D loss: %f] [G loss: %f]' % (epoch + 1, epochs, i, len(dataloader), d_loss.item(), g_loss.item()))\n\n        if i % sample_interval == 0:\n            # save image grid with generated images\n            save_image(gen_imgs.data[:25], 'images/%d.png' % batches_done, nrow=5, normalize=True)\n", "import os\nimport numpy as np\nimport math\nimport itertools\n\nimport torchvision.transforms as transforms\nfrom torchvision.utils import save_image\n\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets\nfrom torch.autograd import Variable\n\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch\nimport torch.optim as optim\n\ncuda = True if torch.cuda.is_available() else False\nTensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor\nclass Generator(nn.Module):\n    def __init__(self):\n        super(Generator, self).__init__()\n\n        self.init_size = im_size // 4\n        self.l1 = nn.Sequential(nn.Linear(latent_dim, 128*self.init_size**2))\n\n        self.conv_blocks = nn.Sequential(\n            nn.BatchNorm2d(128),\n            nn.Upsample(scale_factor=2),\n            nn.Conv2d(128, 128, 3, stride=1, padding=1),\n            nn.BatchNorm2d(128, 0.8),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Upsample(scale_factor=2),\n            nn.Conv2d(128, 64, 3, stride=1, padding=1),\n            nn.BatchNorm2d(64, 0.8),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Conv2d(64, channels, 3, stride=1, padding=1),\n            nn.Tanh(),\n        )\n\n    def forward(self, z):\n        out = self.l1(z)\n        out = out.view(out.shape[0], 128, self.init_size, self.init_size)\n        img = self.conv_blocks(out)\n        return img\n\nclass Discriminator(nn.Module):\n    def __init__(self):\n        super(Discriminator, self).__init__()\n\n        def discriminator_block(in_filters, out_filters, bn=True):\n            block = [nn.Conv2d(in_filters, out_filters, 3, 2, 1), nn.LeakyReLU(0.2, inplace=True), nn.Dropout2d(0.25)]\n            if bn:\n                block.append(nn.BatchNorm2d(out_filters, 0.8))\n            return block\n\n        self.model = nn.Sequential(\n            *discriminator_block(channels, 16, bn=False),\n            *discriminator_block(16, 32),\n            *discriminator_block(32, 64),\n            *discriminator_block(64, 128),\n        )\n\n        # The height and width of downsampled image\n        ds_size = im_size // 2**4\n        self.adv_layer = nn.Linear(128*ds_size**2, 1)\n\n    def forward(self, img):\n        out = self.model(img)\n        out = out.view(out.shape[0], -1)\n        validity = self.adv_layer(out)\n\n        return validity\n\n# Loss weight for gradient penalty\nlambda_gp = 10\n\n# Initialize generator and discriminator\ngenerator = Generator()\ndiscriminator = Discriminator()\n\nif cuda:\n    generator.cuda()\n    discriminator.cuda()\n\n# Configure data loader\nos.makedirs('../../data/mnist', exist_ok=True)\ndataloader = torch.utils.data.DataLoader(\n    datasets.MNIST(\n        '../../data/mnist', train=True, download=True,\n        transform=transforms.Compose([\n            transforms.Resize(im_size),\n            transforms.ToTensor(),\n            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n        ])),\n    batch_size=batch_size, shuffle=True)\n\n# Optimizers\noptimizer_G = torch.optim.Adam(generator.parameters(), lr=lr, betas=(b1, b2))\noptimizer_D = torch.optim.Adam(discriminator.parameters(), lr=lr, betas=(b1, b2))\n\ndef compute_gradient_penalty(D, real_samples, fake_samples):\n    \"\"\"Calculates the gradient penalty loss for WGAN GP\"\"\"\n    # Random weight term for interpolation between real and fake samples\n    alpha = Tensor(np.random.random((real_samples.size(0), 1, 1, 1)))\n    # Get random interpolation between real and fake samples\n    interpolates = (alpha * real_samples + ((1 - alpha) * fake_samples)).requires_grad_(True)\n    d_interpolates = D(interpolates)\n    fake = Variable(Tensor(real_samples.shape[0], 1).fill_(1.0), requires_grad=False)\n    # Get gradient w.r.t. interpolates\n    gradients = autograd.grad(\n        outputs=d_interpolates,\n        inputs=interpolates,\n        grad_outputs=fake,\n        create_graph=True,\n        retain_graph=True,\n        only_inputs=True,\n    )[0]\n    gradients = gradients.view(gradients.size(0), -1)\n    gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n    return gradient_penalty\n# Training\nfor epoch in range(n_epochs):\n    for i, (imgs, _) in enumerate(dataloader):\n\n        # Configure input\n        real_imgs = Variable(imgs.type(Tensor))\n\n        # ---------------------\n        #  Train Discriminator\n        # ---------------------\n\n        optimizer_D.zero_grad()\n\n        # Sample noise as generator input\n        z = Variable(Tensor(np.random.normal(0, 1, (imgs.shape[0], latent_dim))))\n\n        # Generate a batch of images\n        fake_imgs = generator(z)\n\n        # Real images\n        real_validity = discriminator(real_imgs)\n        # Fake images\n        fake_validity = discriminator(fake_imgs)\n        # Gradient penalty\n        gradient_penalty = compute_gradient_penalty(discriminator, real_imgs.data, fake_imgs.data)\n        # Adversarial loss\n        d_loss = -torch.mean(real_validity) + torch.mean(fake_validity) + lambda_gp * gradient_penalty\n\n        d_loss.backward()\n        optimizer_D.step()\n\n        optimizer_G.zero_grad()\n\n        # Train the generator every n_critic steps\n        if i % n_critic == 0:\n\n            # -----------------\n            #  Train Generator\n            # -----------------\n\n            # Generate a batch of images\n            fake_imgs = generator(z)\n            # Loss measures generator's ability to fool the discriminator\n            # Train on fake images\n            fake_validity = discriminator(fake_imgs)\n            g_loss = -torch.mean(fake_validity)\n\n            g_loss.backward()\n            optimizer_G.step()\n\n            print (\n                \"[Epoch %d/%d] [Batch %d/%d] [D loss: %f] [G loss: %f]\"\n                % (epoch, n_epochs, i, len(dataloader), d_loss.item(), g_loss.item())\n            )\n\n        batches_done = epoch * len(dataloader) + i\n        if batches_done % sample_interval == 0:\n            save_image(fake_imgs.data[:25], 'images/%d.png' % batches_done, nrow=5, normalize=True)", "from implementations.wgan_div import WGANGP\n\n# Initialize model\nmodel = WGANGP(lr=0.0001, epochs=100, batch_size=16, img_size=128)\n\n# Training model\nmodel.train()\n\n# Save the model weights\nmodel.save_model()\n\n# Generate and save some samples\nmodel.sample_images(epoch=100)", "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.autograd as autograd\nfrom torch.autograd import Variable\nimport torch.optim as optim\nfrom torchvision import transforms\nfrom torchvision.utils import save_image\nfrom torchvision import datasets\n\n# Hyper-parameters\nlr = 0.0001\nbatch_size = 64\nb1 = 0.5\nb2 = 0.999\nimg_size = 128\nchannels = 1\nlatent_dim = 100\nn_epochs = 100\nselected_attrs = ['Black_Hair', 'Blond_Hair', 'Brown_Hair', 'Male', 'Young']\nimgs_dir = 'images'\nsave_model_interval = 10000 # Iteration interval for saving the model.\nsample_interval = 1000 # Iteration interval for sampling images.\n\nimg_shape = (channels, img_size, img_size)\n\n# Configure data loader\ntransform = transforms.Compose([transforms.Resize((img_size, img_size)), transforms.ToTensor(), transforms.Normalize([0.5], [0.5])])\ndataloader = torch.utils.data.DataLoader(datasets.MNIST('data/mnist', train=True, download=True, transform=transform), batch_size=batch_size, shuffle=True)\n\n# Implement here the WGAN-DIV model\n\n# Your WGAN-DIV model code here", "from data_loader import DataLoader\nfrom models import Generator, Discriminator\nfrom utils import *\n\n# Set parameters\nlearning_rate = 0.0001\nepochs = 100\nbatch_size = 16\nimg_size = 128\nchannels = 3\nlaten_dim = 100\n\n# Initialize generator and discriminator\nG = Generator(laten_dim, channels)\nD = Discriminator(img_size, channels)\n\n# Configure data loader\ndataloader = DataLoader(img_size, batch_size)\n\n# Loss function\nadversarial_loss = torch.nn.BCELoss()\n\n# Configure optimizers\noptimizer_G = torch.optim.Adam(G.parameters(), lr=learning_rate, betas=(0.5, 0.999))\noptimizer_D = torch.optim.Adam(D.parameters(), lr=learning_rate, betas=(0.5, 0.999))\n\ndef sample_image(nrow, batches_done):\n    \"\"\"Saves a grid of generated digits.\"\"\"\n    # Sample noise\n    z = Variable(FloatTensor(np.random.normal(0, 1, (nrow*nrow, laten_dim))))\n    gen_imgs = G(z)\n    save_image(gen_imgs.data, 'images/%d.png' % batches_done, nrow=nrow, normalize=True)\n\n# Training\nfor epoch in range(epochs):\n    for i, (imgs, _) in enumerate(dataloader):\n        # Adversarial ground truths\n        valid = Variable(FloatTensor(imgs.shape[0], 1).fill_(1.0), requires_grad=False)\n        fake = Variable(FloatTensor(imgs.shape[0], 1).fill_(0.0), requires_grad=False)\n        real_imgs = Variable(imgs.type(FloatTensor))\n\n        # Training Generator\n        optimizer_G.zero_grad()\n\n        z = Variable(FloatTensor(np.random.normal(0, 1, (imgs.shape[0], laten_dim))))\n        gen_imgs = G(z)\n        D_fake = D(gen_imgs)\n        Wasserstein_G = -torch.mean(D_fake)\n        Wasserstein_G.backward()\n        optimizer_G.step()\n\n        # Training Discriminator\n        optimizer_D.zero_grad()\n        Dx = D(real_imgs)\n        Dg = D(gen_imgs.detach())\n        gradient_penalty = compute_gradient_penalty(D, real_imgs.data, gen_imgs.data)\n\n        # Adversarial loss with Wasserstein Divergence and GP\n        Wasserstein_D = torch.mean(Dg) - torch.mean(Dx)\n        D_loss = Wasserstein_D + lambda_gp * gradient_penalty\n        D_loss.backward()\n        optimizer_D.step()\n\n    if batches_done % sample_interval == 0:\n        save_image(gen_imgs.data[:25], 'images/%d.png' % batches_done, nrow=5, normalize=True)\n    batches_done += n\nprint('Training finished.')"]}
{"id": 16, "github_id": 10, "output": ["import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets, transforms\n\nfrom models import Generator, Discriminator\nfrom utils import weights_init_normal\n\n# Set device\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# Set parameters\nlr = 0.0001\nb1 = 0.5\nb2 = 0.999\nn_epochs = 100\nbatch_size = 16\nn_classes = 10\nimg_size = 128\nchannels = 1\nsample_interval = 1000\n\n# Configure data loader\nos.makedirs('data/mnist', exist_ok=True)\ndataloader = torch.utils.data.DataLoader(\n    datasets.MNIST('data/mnist', train=True, download=True,\n                   transform=transforms.Compose([transforms.Resize(img_size), transforms.ToTensor(), transforms.Normalize([0.5], [0.5])])),\n    batch_size=batch_size, shuffle=True)\n\n# Loss functions\ncriterion_G = torch.nn.MSELoss()\ncriterion_D = torch.nn.BCELoss()\ncriterion_auxiliary = torch.nn.CrossEntropyLoss()\n\n# Initialize generator and discriminator\ngenerator = Generator(n_classes=n_classes)\ndiscriminator = Discriminator(n_classes=n_classes)\n\nif cuda:\n    generator.cuda()\n    discriminator.cuda()\n    criterion_G.cuda()\n    criterion_D.cuda()\n    criterion_auxiliary.cuda()\n\n# Initialize weights\n# generator.apply(weights_init_normal)\ndiscriminator.apply(weights_init_normal)\n\n# Optimizers\noptimizer_G = torch.optim.Adam(generator.parameters(), lr=lr, betas=(b1, b2))\noptimizer_D = torch.optim.Adam(discriminator.parameters(), lr=lr, betas=(b1, b2))\n\nfor epoch in range(n_epochs):\n    for i, (imgs, labels) in enumerate(dataloader):\n\n        batch_size = imgs.shape[0]\n\n        # Adversarial ground truths\n        valid = Variable(FloatTensor(batch_size, 1).fill_(1.0), requires_grad=False)\n        fake = Variable(FloatTensor(batch_size, 1).fill_(0.0), requires_grad=False)\n\n        # Configure input\n        real_imgs = Variable(imgs.type(FloatTensor))\n        labels = Variable(labels.type(LongTensor))\n\n        optimizer_G.zero_grad()\n\n        z = Variable(FloatTensor(np.random.normal(0, 1, (batch_size, generator.latent_dim))))\ngen_labels = Variable(LongTensor(np.random.randint(0, n_classes, batch_size)))\n\n        # Generate a batch of images\n        gen_imgs = generator(z)\n\n        validity, pred_label = discriminator(gen_imgs)\n\n        g_loss = criterion_G(validity, valid) + criterion_auxiliary(pred_label, gen_labels)\n\n        g_loss.backward()\n        optimizer_G.step()\n\n        optimizer_D.zero_grad()\n\n        real_pred, real_aux = discriminator(real_imgs)\n        d_real_loss = (criterion_D(real_pred, valid) + criterion_auxiliary(real_aux, labels)) / 2\n\n        fake_pred, fake_aux = discriminator(gen_imgs.detach())\n        d_fake_loss = (criterion_D(fake_pred, fake) + criterion_auxiliary(fake_aux, gen_labels)) / 2\n\n        d_loss = (d_real_loss + d_fake_loss) / 2\n\n        d_loss.backward()\n        optimizer_D.step()", "import torch\nimport torch.nn as nn\nimport torchvision.transforms as transforms\nfrom torchvision.utils import save_image\nfrom torchvision.datasets import ImageFolder\nfrom torch.utils.data import DataLoader\nfrom torch.autograd import Variable\nimport os\nimport numpy as np\n\n# Generator and Discriminator classes here...\n\n# Training parameters\nepochs = 100\nlearning_rate = 0.0001\nbatch_size = 16\nimage_size = 128\n\nos.makedirs('images', exist_ok=True)\n\n# Dataset and Dataloader setup (Modify the 'path_to_your_data' with the actual directory of your dataset)\ndataset = ImageFolder(root='path_to_your_data',\n                      transform=transforms.Compose([\n                                transforms.Resize((image_size, image_size)),\n                                transforms.ToTensor(),\n                                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),]))\ndataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\nmodel_G = Generator().to(device)\nmodel_D = Discriminator().to(device)\n\n# Loss function and optimizers\ncriterion = nn.BCELoss()\noptimizer_G = torch.optim.Adam(model_G.parameters(), lr=learning_rate)\noptimizer_D = torch.optim.Adam(model_D.parameters(), lr=learning_rate)\n\n# Training Loop\nfor epoch in range(epochs):\n    for i, (imgs, _) in enumerate(dataloader):\n\n        valid = Variable(torch.ones(batch_size, 1).to(device))\n        fake  = Variable(torch.zeros(batch_size, 1).to(device))\n\n        real_imgs = Variable(imgs.type(torch.FloatTensor)).to(device)\n        \n        # Training Generator\n        optimizer_G.zero_grad()\n        \n        z = Variable(torch.randn(batch_size, 100, 1, 1)).to(device)\n        gen_imgs = model_G(z)\n        \n        loss_G = criterion(model_D(gen_imgs, gen_labels), valid)\n        loss_G.backward()\n        optimizer_G.step()\n        \n        # Training Discriminator\n        optimizer_D.zero_grad()\n        real_loss = criterion(model_D(real_imgs), valid)\n        fake_loss = criterion(model_D(gen_imgs.detach()), fake)\n        \n        loss_D = (real_loss + fake_loss) / 2\n        loss_D.backward()\n        optimizer_D.step()\n        \n    if epoch % 10 == 0:\n        save_image(gen_imgs.data[:25], 'images/{}.png'.format(epoch), nrow=5, normalize=True)", "import torch\nimport torchvision.transforms as transforms\nfrom torchvision.utils import save_image\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets\nfrom torch.autograd import Variable\n\nimport os\nimport numpy as np\n\n# Model parameters\nimage_size = 128\nbatch_size = 16\nlr = 0.0001\nnum_epochs = 100\n\n# Configure data loader\nos.makedirs('data', exist_ok=True)\ndataloader = torch.utils.data.DataLoader(\n    datasets.MNIST('data', train=True, download=True,\n                   transform=transforms.Compose([\n                       transforms.Resize(image_size),\n                       transforms.ToTensor(),\n                       transforms.Normalize((0.5,), (0.5,))\n                   ])),\n    batch_size=batch_size, shuffle=True)\n\n\n# Initialize generator and discriminator\nG = Generator()\nD = Discriminator()\n\n# Loss function and optimizers\noptim_G = torch.optim.Adam(G.parameters(), lr=lr)\noptim_D = torch.optim.Adam(D.parameters(), lr=lr)\n\n# Initialize weights\nG.apply(weights_init)\nD.apply(weights_init)\n\n# Train the GAN\nfor epoch in range(num_epochs):\n    for i, (imgs, _) in enumerate(dataloader):\n\n        # Configure input\n        real_imgs = Variable(imgs.type(Tensor))\n\n        # Train Generator\n        # ---------------\n\n        optim_G.zero_grad()\n\n        # Sample noise as generator input\n        noise = Variable(Tensor(np.random.normal(0, 1, (imgs.shape[0], latent_dim))))\n\n        # Generate batch of images\n        gen_imgs = G(noise)\n\n        # Loss measures Generator's ability to trick the discriminator\n        g_loss = -torch.mean(D(gen_imgs))\n\n        g_loss.backward()\n        optim_G.step()\n\n        # Train Discriminator\n        # -------------------\n\n        optim_D.zero_grad()\n\n        # Calculate Discriminator loss\n        real_loss = D(real_imgs)\n        fake_loss = D(gen_imgs.detach())\n        d_loss = -torch.mean(real_loss) + torch.mean(fake_loss)\n\n        d_loss.backward()\n        optim_D.step()\n\n        # Normalize weights of Discriminator\n        for p in D.parameters():\n            p.data.clamp_(-clip_value, clip_value)\n\n        # Output training stats\n        print(f'Epoch: {epoch}/{num_epochs}, Batch: {i}/{len(dataloader)}, D_Loss: {d_loss.item()}, G_Loss: {g_loss.item()}')\n\n        if i % sample_interval == 0:\n            save_image(gen_imgs.data[:25], f'images/{epoch}_{i}.png', nrow=5, normalize=True)\n", "import torch\nfrom torchvision.transforms import Resize\nfrom torch.utils.data import DataLoader\nfrom torchvision.datasets import ImageFolder\nfrom torchvision.utils import make_grid\nfrom torch.autograd import Variable\nfrom torchvision import transforms\n\nimport matplotlib.pyplot as plt\n\nfrom implementations.softmax_gan.softmax_gan import Generator, Discriminator\n\n# Hyperparameters\nimage_size = 128\nbatch_size = 16\nlr = 0.0001\nepochs = 100\nlatent_dim = 100\n\n# Setup the device\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# Create the transform\ntransform = transforms.Compose([Resize(image_size),\n                                transforms.ToTensor(),\n                                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n\n# Initialize the datasets and data loader\ndataset = ImageFolder(root='[path_to_dataset]', transform=transform)\ndataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n\n# Initialize the generator and discriminator\ngenerator = Generator(latent_dim).to(device)\ndiscriminator = Discriminator().to(device)\n\n# Initialize the optimizers\noptimizer_G = torch.optim.Adam(generator.parameters(), lr=lr)\noptimizer_D = torch.optim.Adam(discriminator.parameters(), lr=lr)\n\n# Define tensors\nTensor = torch.cuda.FloatTensor if torch.cuda.is_available() else torch.FloatTensor\n\n# Train the model\nfor epoch in range(epochs):\n    for i, (imgs, _) in enumerate(dataloader):\n        # Adversarial ground truths\n        valid = Variable(Tensor(imgs.size(0), 1).fill_(1.0), requires_grad=False)\n        fake = Variable(Tensor(imgs.size(0), 1).fill_(0.0), requires_grad=False)\n\n        # Configure input\n        real_imgs = Variable(imgs.type(Tensor))\n\n        # Generate a batch of images\n        z = Variable(Tensor(np.random.normal(0, 1, (imgs.shape[0], latent_dim))))\n        gen_imgs = generator(z)\n\n        # ---------------------\n        #  Train Discriminator\n        # ---------------------\n\n        optimizer_D.zero_grad()\n\n        real_loss = torch.nn.functional.binary_cross_entropy_with_logits(discriminator(real_imgs), valid)\n        fake_loss = torch.nn.functional.binary_cross_entropy_with_logits(discriminator(gen_imgs.detach()), fake)\n        d_loss = (real_loss + fake_loss) / 2\n\n        d_loss.backward()\n        optimizer_D.step()\n\n        # ---------------------\n        #  Train Generator\n        # ---------------------\n\n        optimizer_G.zero_grad()\n\n        g_loss = torch.nn.functional.binary_cross_entropy_with_logits(discriminator(gen_imgs), valid)\n\n        g_loss.backward()\n        optimizer_G.step()\n\n    print(f'[Epoch {epoch}/{epochs}] [D loss: {d_loss.item()}] [G loss: {g_loss.item()}]')\n\n# Save the model\ntorch.save(generator.state_dict(), 'generator.pth')\ntorch.save(discriminator.state_dict(), 'discriminator.pth')\n\n# Preview generated images\nz = Variable(Tensor(np.random.normal(0, 1, (64, latent_dim))))\ngen_imgs = generator(z).detach().cpu()\ngrid = make_grid(gen_imgs, nrow=8, normalize=True)\nplt.imshow(grid.permute(1, 2, 0))\nplt.show()", "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets, transforms\n\n# Set device\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# Define model\nmodel = SoftmaxGAN().to(device)\n\n# Define optimizer\noptimizer = optim.Adam(model.parameters(), lr=0.0001)\n\n# Define transformation\ntransform = transforms.Compose([transforms.Resize((128, 128)),\ntransforms.ToTensor()])\n\n# Load dataset. Update 'path_to_data' to your dataset path\ndataset = datasets.ImageFolder(root='path_to_data', transform=transform)\n\n# Define dataloader\ndata_loader = DataLoader(dataset=dataset, batch_size=16, shuffle=True)\n\n# Run over 100 epochs\ndef train():\n    model.train()\n    for epoch in range(100):\n        for idx, (data, _) in enumerate(data_loader):\n            data = data.to(device)\n            optimizer.zero_grad()\n            output = model(data)\n            loss = F.cross_entropy(output)\n            loss.backward()\n            optimizer.step()\n    print('Training finished')\n\ntrain()"]}
{"id": 17, "github_id": 10, "output": ["import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision\nimport torchvision.transforms as transforms\nfrom torch.utils.data import DataLoader\n\n# Hyperparameters\nlearning_rate = 0.0001\nnum_epochs = 100\nimg_size = 128\nbatch_size = 16\n\n# Pix2Pix model\n# You will need to define the Generator and Discriminator architecture of Pix2Pix\n\n\n# Pix2Pix's generator and discriminator architecture will be here\n\nclass Generator(nn.Module):\n    \n    # Generator architecture will be here\n    ...\n\nclass Discriminator(nn.Module):\n    \n    # Discriminator architecture will be here\n    ...\n\n\ngen = Generator()\ndisc = Discriminator()\n\n# Define Optimizers\noptimizer_gen = torch.optim.Adam(gen.parameters(), lr=learning_rate)\noptimizer_disc = torch.optim.Adam(disc.parameters(), lr=learning_rate)\n\n# Loss Function\nloss_function = nn.BCEWithLogitsLoss()\n\n# Training loop\nfor epoch in range(num_epochs):\n    for x_real in DataLoader:\n        ...\n        # Training procedure will be here\n        ...\n", "import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torchvision.transforms import Resize\nfrom torchvision.utils import save_image\n\n# Define the generator and discriminator\nnetG = Generator()\nnetD = Discriminator()\n\n# Initialize BCELoss function\ncriterion = nn.BCELoss()\n\n# Establish convention for real and fake labels\nreal_label = 1.\nfake_label = 0.\n\n# Setup Adam optimizers for both generator and discriminator\noptimizerD = optim.Adam(netD.parameters(), lr=0.0001, betas=(0.5, 0.999))\noptimizerG = optim.Adam(netG.parameters(), lr=0.0001, betas=(0.5, 0.999))\n\n# Set the number of training epochs\nnum_epochs = 100\n\n# Set the image size\nimage_size = 128\ntransform = Resize((image_size,image_size))\n\n# Set the batch size\nbatch_size = 16\n\nfor epoch in range(num_epochs):\n    for i, data in enumerate(dataloader, 0):\n        ############################\n        # (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))\n        ###########################\n\n        # Train with all-real batch\n        netD.zero_grad()\n        real = data[0].to(device)\n        batch_size = real.size(0)\n\n        output = netD(real)\n        errD_real = criterion(output, real_label)\n        errD_real.backward()\n        D_x = output.mean().item()\n\n        # Train with all-fake batch\n        noise = torch.randn(batch_size, nz, 1, 1, device=device)\n        fake = netG(noise)\n\n        output = netD(fake.detach())\n        errD_fake = criterion(output, fake_label)\n        errD_fake.backward()\n        D_G_z1 = output.mean().item()\n\n        errD = errD_real + errD_fake\n        optimizerD.step()\n\n        ############################\n        # (2) Update G network: maximize log(D(G(z)))\n        ###########################\n        netG.zero_grad()\n        output = netD(fake)\n        errG = criterion(output, real_label)\n        errG.backward()\n        D_G_z2 = output.mean().item()\n        optimizerG.step()\n\n        if i % 100 == 0:\n            print('[%d/%d][%d/%d] Loss_D: %.4f Loss_G: %.4f D(x): %.4f D(G(z)): %.4f / %.4f'\n                    % (epoch, num_epochs, i, len(dataloader), errD.item(), errG.item(), D_x, D_G_z1, D_G_z2))\n\n        if (epoch % 5 == 0) or ((epoch == num_epochs-1) and (i == len(dataloader)-1)):\n            with torch.no_grad():\n                fake = netG(fixed_noise).detach().cpu()\n            img_list.append(vutils.make_grid(fake, padding=2, normalize=True))\n\n", "python3 pix2pix.py --dataset_name YOUR_DATASET_NAME --epoch 100 --img_height 128 --img_width 128 --batch_size 16 --lr 0.0001", "import argparse\nimport os\nimport numpy as np\nimport math\nimport itertools\nimport datetime\nimport time\n\nimport torchvision.transforms as transforms\nfrom torchvision.utils import save_image\n\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets\nfrom torch.autograd import Variable\n\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch\n\nos.makedirs('images', exist_ok=True)\n\nclass Generator(nn.Module):\n    def __init__(self):\n        super(Generator, self).__init__()\n\n        self.model = nn.Sequential(\n            nn.Linear(100, 256),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Linear(256, 512),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Linear(512, 1024),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Linear(1024, 128*128*3),\n            nn.Tanh()\n        )\n\n    def forward(self, z):\n        img = self.model(z)\n        img = img.view(img.size(0), 3, 128, 128)\n        return img\n\nclass Discriminator(nn.Module):\n    def __init__(self):\n        super(Discriminator, self).__init__()\n\n        self.model = nn.Sequential(\n            nn.Linear(128*128*3, 1024),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Dropout(0.3),\n            nn.Linear(1024, 512),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Dropout(0.3),\n            nn.Linear(512, 256),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Dropout(0.3),\n            nn.Linear(256, 1),\n        )\n\n    def forward(self, img):\n        img_flat = img.view(img.size(0), -1)\n        validity = self.model(img_flat)\n\n        return validity\n\n\n# Loss function\nadversarial_loss = torch.nn.MSELoss()\n\n# Initialize generator and discriminator\ngenerator = Generator()\ndiscriminator = Discriminator()\n\nif cuda:\n    generator.cuda()\n    discriminator.cuda()\n    adversarial_loss.cuda()\n\n# Configure data loader\ndataloader = torch.utils.data.DataLoader(\n    datasets.MNIST(\n        '../../data/mnist/',\n        train=True,\n        download=True,\n        transform=transforms.Compose(\n            [transforms.Resize(128), transforms.ToTensor(), transforms.Normalize([0.5], [0.5])]\n        ),\n    ),\n    batch_size=16,\n    shuffle=True,\n)\n\n# Optimizers\noptimizer_G = torch.optim.Adam(generator.parameters(), lr=0.0001, betas=(0.5, 0.999))\noptimizer_D = torch.optim.Adam(discriminator.parameters(), lr=0.0001, betas=(0.5, 0.999))\n\nTensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor\n\n# Training\nfor epoch in range(100):\n    for i, (imgs, _) in enumerate(dataloader):\n\n        # Adversarial ground truths\n        valid = Variable(Tensor(imgs.size(0), 1).fill_(1.0), requires_grad=False)\n        fake = Variable(Tensor(imgs.size(0), 1).fill_(0.0), requires_grad=False)\n\n        # Configure input\n        real_imgs = Variable(imgs.type(Tensor))\n\n        # Train Generator\n        optimizer_G.zero_grad()\n\n        # Sample noise as generator input\n        z = Variable(Tensor(np.random.normal(0, 1, (imgs.shape[0], 100))))\n\n        # Generate a batch of images\n        gen_imgs = generator(z)\n\n        # Adversarial loss\n        g_loss = adversarial_loss(discriminator(gen_imgs), valid)\n\ng_loss.backward()\noptimizer_G.step()\n # Train Discriminator\n\n        optimizer_D.zero_grad()\n\n        # Measure discriminator's ability to classify real from generated samples\n\n        real_loss = adversarial_loss(discriminator(real_imgs), valid)\n        fake_loss = adversarial_loss(discriminator(gen_imgs.detach()), fake)\n        d_loss = (real_loss + fake_loss) / 2\n\n        d_loss.backward()\n        optimizer_D.step()\n\n    print( f'[Epoch {epoch}/{100}] [D loss: {d_loss.item()}] [G loss: {g_loss.item()}]')\n\n    save_image(gen_imgs.data[:25], f'images/{epoch}.png', nrow=5, normalize=True)", "import argparse\nimport os\nimport numpy as np\nimport math\nimport itertools\nimport datetime\nimport time\nimport torchvision.transforms as transforms\nfrom torchvision.utils import save_image, make_grid\nfrom torchvision import datasets\nfrom torch.utils.data import DataLoader\nfrom torch.autograd import Variable\nfrom models import *\nfrom datasets import *\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch\n\n# Configure settings\nparser = argparse.ArgumentParser()\nparser.add_argument('--epoch', type=int, default=0, help='epoch to start training from')\nparser.add_argument('--n_epochs', type=int, default=100, help='number of epochs of training')\nparser.add_argument('--dataset_name', type=str, default='img_align_celeba', help='name of the dataset')\nparser.add_argument('--batch_size', type=int, default=16, help='size of the batches')\nparser.add_argument('--lr', type=float, default=0.0001, help='adam: learning rate')\nparser.add_argument('--b1', type=float, default=0.5, help='adam: decay of first order momentum of gradient')\nparser.add_argument('--b2', type=float, default=0.999, help='adam: decay of first order momentum of gradient')\nparser.add_argument('--decay_epoch', type=int, default=100, help='epoch from which to start lr decay')\nparser.add_argument('--n_cpu', type=int, default=8, help='number of cpu threads to use during batch generation')\nparser.add_argument('--img_height', type=int, default=128, help='size of image height')\nparser.add_argument('--img_width', type=int, default=128, help='size of image width')\nparser.add_argument('--channels', type=int, default=3, help='number of image channels')\nparser.add_argument('--sample_interval', type=int, default=500, help='interval between saving generator outputs')\nparser.add_argument('--checkpoint_interval', type=int, default=-1, help='interval between model checkpoints')\nopt = parser.parse_args()\n\nos.makedirs('images/%s' % opt.dataset_name, exist_ok=True)\nos.makedirs('saved_models/%s' % opt.dataset_name, exist_ok=True)\n\n# Establish CUDA capacity\ncuda = torch.cuda.is_available()\n\n# Initialize generator and discriminator\ngenerator = GeneratorUNet()\ndiscriminator = Discriminator()\n\nif cuda:\n    generator = generator.cuda()\n    discriminator = discriminator.cuda()\n\n# Initialize weights\ngenerator.apply(weights_init_normal)\ndiscriminator.apply(weights_init_normal)\n\n# Configure data loader\ntransforms_ = [\n    transforms.Resize((opt.img_height, opt.img_width), Image.BICUBIC),\n    transforms.ToTensor(),\n    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]\ndataloader = DataLoader(ImageDataset('%s' % opt.dataset_name, transforms_=transforms_),\n                        batch_size=opt.batch_size, shuffle=True, num_workers=opt.n_cpu)\n\n# Optimizers\noptimizer_G = torch.optim.Adam(generator.parameters(), lr=opt.lr, betas=(opt.b1, opt.b2))\noptimizer_D = torch.optim.Adam(discriminator.parameters(), lr=opt.lr, betas=(opt.b1, opt.b2))\n\n# TODO: Fill with training and testing code"]}
{"id": 28, "github_id": 10, "output": ["import torch\nfrom torch.autograd import Variable\nimport numpy as np\nfrom torchvision.utils import save_image\n\nfrom implementations.cluster_gan.models import Generator, Discriminator\nfrom implementations.cluster_gan.utils import weights_init_normal\n\n\n# Set parameters\nlearning_rate = 0.0001  # Learning rate\nnum_epochs = 100        # Number of training epochs\nbatch_size = 16         # Batch size\nimg_size = 128          # Image size\n\n# Initialize generator and discriminator\ngenerator = Generator()\ndiscriminator = Discriminator()\n\n# Initialize weights\ngenerator.apply(weights_init_normal)\ndiscriminator.apply(weights_init_normal)\n\n# Loss functions and Optimizers\nadversarial_loss = torch.nn.MSELoss()\nauxiliary_loss = torch.nn.CrossEntropyLoss()\n\nTensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor\n\noptimizer_G = torch.optim.Adam(generator.parameters(), lr=learning_rate)\noptimizer_D = torch.optim.Adam(discriminator.parameters(), lr=learning_rate)\n\n# Training\nfor epoch in range(num_epochs):\n    for i, (imgs, _) in enumerate(dataloader):\n\n        # Adversarial ground truths\n        valid = Variable(Tensor(imgs.shape[0], 1).fill_(1.0), requires_grad=False)\n        fake = Variable(Tensor(imgs.shape[0], 1).fill_(0.0), requires_grad=False)\n\n        # Configure input\n        real_imgs = Variable(imgs.type(Tensor))\n        labels = Variable(Tensor(np.random.randint(0, 10, imgs.shape[0])))\n\n        # Train Generator\n\n        # Sample noise as generator input\n        z = Variable(Tensor(np.random.normal(0, 1, (batch_size, 10))))\n\n        # Generate a batch of images\n        gen_imgs = generator(z)\n\n        # Loss measures generator's ability to fool the discriminator\n        validity, _ = discriminator(gen_imgs)\n        g_loss = adversarial_loss(validity, valid)\n\n        # Train Discriminator\n\n        # Loss for real images\n        real_pred, real_aux = discriminator(real_imgs)\n        d_real_loss = (adversarial_loss(real_pred, valid) + auxiliary_loss(real_aux, labels)) / 2\n\n        # Loss for fake images\n        fake_pred, fake_aux = discriminator(gen_imgs.detach())\n        d_fake_loss = (adversarial_loss(fake_pred, fake) + auxiliary_loss(fake_aux, labels)) / 2\n\n        # Total discriminator loss\n        d_loss = (d_real_loss + d_fake_loss) / 2\n\n        # Optimize Discriminator\n        optimizer_D.zero_grad()\n        d_loss.backward()\n        optimizer_D.step()\n\n        # Optimize Generator\n        optimizer_G.zero_grad()\n        g_loss.backward()\n        optimizer_G.step()", "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision\nimport torchvision.transforms as transforms\nfrom torch.autograd import Variable\nimport itertools\nfrom implementations.clustergan.clustergan import Generator, Discriminator\n\n# Hyperparameters\nbatch_size = 16\nimg_size = 128\nlr = 0.0001\nepochs = 100\nlatent_dim = 100\n\n# Initialize Generator and Discriminator\ngenerator = Generator(latent_dim=latent_dim, img_size=img_size)\ndiscriminator = Discriminator(img_size=img_size)\n\n# Loss function\nadversarial_loss = torch.nn.BCELoss()\n\n# Optimizers\ngenerator_optimizer = torch.optim.Adam(generator.parameters(), lr=lr)\ndiscriminator_optimizer = torch.optim.Adam(discriminator.parameters(), lr=lr)\n\n# Prepare the dataloader (suppose the dataset is 'dataset')\ndataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n\n# Training Loop\nfor epoch in range(100):\n    for i, (imgs, _) in enumerate(dataloader):\n\n        # Adversarial ground truths\n        valid = Variable(torch.FloatTensor(imgs.size(0), 1).fill_(1.0), requires_grad=False)\n        fake = Variable(torch.FloatTensor(imgs.size(0), 1).fill_(0.0), requires_grad=False)\n\n        # Configure input\n        real_imgs = Variable(imgs)\n\n        # -----------------\n        #  Train Generator\n        # -----------------\n\n        generator_optimizer.zero_grad()\n\n        # Sample noise as generator input\n        z = Variable(torch.FloatTensor(np.random.normal(0, 1, (imgs.shape[0], latent_dim))))\n\n        # Generate a batch of images\n        gen_imgs = generator(z)\n\n        # Loss measures generator's ability to fool the discriminator\n        g_loss = adversarial_loss(discriminator(gen_imgs), valid)\n\n        g_loss.backward()\n        generator_optimizer.step()\n\n        # ---------------------\n        #  Train Discriminator\n        # ---------------------\n\n        discriminator_optimizer.zero_grad()\n\n        # Measure discriminator's ability to classify real from generated samples\n        real_loss = adversarial_loss(discriminator(real_imgs), valid)\n        fake_loss = adversarial_loss(discriminator(gen_imgs.detach()), fake)\n        d_loss = (real_loss + fake_loss) / 2\n\n        d_loss.backward()\n        discriminator_optimizer.step()\n\n    print(f'[Epoch {epoch}/{epochs}] [D loss: {d_loss.item()}] [G loss: {g_loss.item()}]')\n", "import torch\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\n\nfrom implementations.cluster_gan.models import Generator, Discriminator\nfrom utils import reparameterization\n\ndef train_cluster_gan(data_loader, lr=0.0001, b1=0.5, b2=0.999, n_epochs=100, batch_size=16, n_classes=10, latent_dim=100, img_size=128, channels=1):\n    cuda = True if torch.cuda.is_available() else False\n\n    # Loss function\n    adversarial_loss = torch.nn.BCELoss()\n    pixelwise_loss = torch.nn.L1Loss()\n\n    # Initialize generator and discriminator\n    generator = Generator()\n    discriminator = Discriminator()\n\n    if cuda:\n        generator.cuda()\n        discriminator.cuda()\n        adversarial_loss.cuda()\n        pixelwise_loss.cuda()\n\n    # Optimizers\n    optimizer_G = torch.optim.Adam(generator.parameters(), lr=lr, betas=(b1, b2))\n    optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=lr, betas=(b1, b2))\n\n    Tensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor\n\n    for epoch in range(n_epochs):\n        for i, (imgs, _) in enumerate(data_loader):\n\n            # Adversarial ground truths\n            valid = Variable(Tensor(imgs.shape[0], 1).fill_(1.0), requires_grad=False)\n            fake = Variable(Tensor(imgs.shape[0], 1).fill_(0.0), requires_grad=False)\n\n            # Configure input\n            real_imgs = Variable(imgs.type(Tensor))\n\n            # -----------------\n            #  Train Generator\n            # -----------------\n\n            optimizer_G.zero_grad()\n\n            # Sample noise and labels as generator input\n            z = Variable(Tensor(np.random.normal(0, 1, (imgs.shape[0], latent_dim))))\n\n            gen_imgs = generator(z)\n\n            # Loss measures generator's ability to fool the discriminator\n            validity, _ = discriminator(gen_imgs)\n            g_loss = adversarial_loss(validity, valid) + lambda_pixel * pixelwise_loss(gen_imgs, real_imgs)\n\n            g_loss.backward()\n            optimizer_G.step()\n\n            # ---------------------\n            #  Train Discriminator\n            # ---------------------\n\n            optimizer_D.zero_grad()\n\n            # Loss for real images\n            real_pred, _ = discriminator(real_imgs)\n            d_real_loss = adversarial_loss(real_pred, valid)\n\n            # Loss for fake images\n            fake_pred, _ = discriminator(gen_imgs.detach())\n            d_fake_loss = adversarial_loss(fake_pred, fake)\n\n            # Total discriminator loss\n            d_loss = (d_real_loss + d_fake_loss) / 2\n\n            d_loss.backward()\n            optimizer_D.step()\n\n            print (\n                \"[Epoch %d/%d] [Batch %d/%d] [D loss: %f] [G loss: %f]\"\n                % (epoch, n_epochs, i, len(data_loader), d_loss.item(), g_loss.item())\n            )\n\n            batches_done = epoch * len(data_loader) + i\n", "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torchvision import datasets, transforms\nfrom torch.autograd import Variable\n\n# Training settings\nbatch_size = 16\nepochs = 100\nlr = 0.0001\nimg_size = 128\n\n# ClusterGAN Model\nmodel = ClusterGAN(image_size=img_size)\nmodel = model.to(device)\n\n# Optimizer\noptimizer = optim.Adam(model.parameters(), lr=lr)\ncriterion = nn.MSELoss()\n\n# Data Loader\ndata_loader = torch.utils.data.DataLoader(datasets.MNIST('data', train=True, download=True, transform=transforms.ToTensor()), batch_size=batch_size, shuffle=True)\n\nfor epoch in range(1, epochs + 1):\n    for batch_idx, (data, _) in enumerate(data_loader):\n        data = data.to(device)\n        optimizer.zero_grad()\n        output = model(data)\n        loss = criterion(output, data)\n        loss.backward()\n        optimizer.step()\n        if batch_idx % 10 == 0:\n            print('Train Epoch: {} [{}/{} ({:.0f}%)]\tLoss: {:.6f}'.format(epoch, batch_idx * len(data), len(data_loader.dataset), 100. * batch_idx / len(data_loader), loss.item()))\n", "import torch\nfrom torch.autograd import Variable\n\n# Hyperparameters\nlearning_rate = 0.0001\nepochs = 100\nbatch_size = 16\nimage_size = 128\n\n# Cluster GAN specific parameters\nn_c = 10  # Number of clusters\nz_dim = 64  # Dimension of the latent vector (z)\n\n# Initialize the discriminator, generator and cluster_head for the Cluster GAN\nD = Discriminator(image_size=image_size).to(device)\nG = Generator(z_dim=z_dim, image_size=image_size).to(device)\nCluster_head = Cluster_head(n_c=n_c).to(device)\n\n# Loss functions\nloss = torch.nn.CrossEntropyLoss()\n\n# Create the optimizers\noptimizer_D = torch.optim.Adam(D.parameters(), lr=learning_rate)\noptimizer_G = torch.optim.Adam(G.parameters(), lr=learning_rate)\n\n# Training loop\nfor epoch in range(epochs):\n\n    for i, data in enumerate(dataloader, 0):\n        # Unpack the data\n        real_images, _ = data\n        real_images = Variable(real_images).to(device)\n\n        #=====================Train the discriminator=====================#\n        optimizer_D.zero_grad()\n        # Calculate discriminator loss for real images\n        real_pred, _ = D(real_images)\n        d_real_loss = loss(real_pred, real_labels)\n        \n        # Generate noise vectors\n        z = torch.randn(batch_size, z_dim, 1, 1)\n        z = z.to(device)\n\n        # Generate fake images\n        fake_images = G(z)\n        # Calculate discriminator loss for fake images\n        fake_pred, _ = D(fake_images)\n        d_fake_loss = loss(fake_pred, fake_labels)\n\n        # Backpropagation\n        d_loss = d_real_loss + d_fake_loss\n        d_loss.backward()\n        optimizer_D.step()\n\n        #=====================Train the generator=====================#\n        optimizer_G.zero_grad()\n        # Generate noise vectors\n        z = torch.randn(batch_size, z_dim, 1, 1)\n        z = z.to(device)\n\n        # Generate fake images\n        fake_images = G(z)\n\n        # Calculate generator loss\n        fake_pred, _ = D(fake_images)\n        g_loss = loss(fake_pred, real_labels)\n        \n        # Backpropagation\n        g_loss.backward()\n        optimizer_G.step()\n\n        #=====================Train the cluster head=====================#\n        optimizer_Cluster_head.zero_grad()\n        # Generate cluster labels\n        cluster_labels = torch.randint(n_c, size=(batch_size,))\n        cluster_labels = cluster_labels.to(device)\n\n        # Calculate cluster head loss\n        z = torch.randn(batch_size, z_dim, 1, 1)\n        z = z.to(device)\n        fake_images = G(z)\n\n        _, cluster_pred = D(fake_images)\n        cluster_pred = Cluster_head(cluster_pred) \n        cluster_loss = loss(cluster_pred, cluster_labels)\n\n        # Backpropagation\n        cluster_loss.backward()\n        optimizer_Cluster_head.step()\n\n    print(f'Epoch {epoch+1}/{epochs} || Loss D: {d_loss} || Loss G: {g_loss} || loss cluster: {cluster_loss}')"]}
